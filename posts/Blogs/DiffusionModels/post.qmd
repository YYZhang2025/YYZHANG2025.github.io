---
title: "Diffusion Models"
date: today
date-modified: last-modified
date-format: iso
description: "This blog introduces diffusion models, covering their core principles and perspectives. We begin with the basics of DDPM, then explore score-based, ODE, and SDE viewpoints, as well as flow and score matching. Finally, we highlight applications in image generation, text-to-image, video, and reinforcement learning."
categories: 
    - Generative AI
    - Diffusion Models
language: 
    title-block-modified: "Last modified"
---


---


Interested in the generative models.  For example [Nano Banana](https://nanobanana.ai/), DALL-E 2, Stable Diffusion, Midjourney, and so on.  I have been reading some papers and blogs about diffusion models.  Here is a summary of what I learned. 

First, let's prepare some math knowledge. 

# Preliminaries

## Kullback-Leibler Divergence(KL Divergence)
The Kullback-Leibler divergence, also known as relative entropy, is a measure of how one probability distribution diverges from a second expected probability distribution. For two probability distributions $P$ and $Q$ defined on the same probability space, the KL divergence from $P$ to $Q$ is defined as:

$$
D_{KL}(P || Q) = \mathbb{E}_{x \sim P}\left[\log\left(\frac{P(x)}{Q(x)}\right)\right]
$${#eq-kl-divergence}

there are some properties of KL divergence:

1. **Non-negativity**: $D_{KL}(P || Q) \geq 0$, with equality if and only if $P = Q$ almost everywhere.
2. **Asymmetry**: $D_{KL}(P || Q) \neq D_{KL}(Q || P)$ in general.


## Jensen's Inequality
Jensenâ€™s Inequality is a fundamental result in convex analysis. It states that for a [concave function](https://en.wikipedia.org/wiki/Concave_function) $f$ and a random variable $X$,

$$
f(\mathbb{E}[X]) \geq \mathbb{E}[f(X)] .
$${#eq-jensen-inequality}

For example, $f(x) = \log x$  is a **concave function**, so we have
$$
\log(\mathbb{E}[X]) \geq \mathbb{E}[\log(X)] .
$${#eq-jensen-inequality-log}


## Variational Lower Bound (ELBO)
In the context of probabilistic models, the **Evidence Lower Bound** (ELBO) is a *lower bound* on the log-likelihood of observed data. It is commonly used in variational inference to <u>approximate complex posterior distributions</u>. For example, in the variational autoencoder (VAE)[@AutoEncodingVariationalBayes2022kingma] framework, the ELBO is given by:

$$
\text{ELBO}(x) = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) || p(z))
$$ {#eq-elbo}
where:

- $x$ is the observed data,
- $z$ is the latent variable,
- $p(x|z)$ is the likelihood of the data given the latent variable,
- $p(z)$ is the **prior distribution** over the latent variable,
- $q(z|x)$ is the **variational posterior distribution**.
- $D_{KL}(q(z|x) || p(z))$ is the KL divergence between the variational posterior and the prior.

The ELBO serves as a **surrogate objective function** that can be maximized to learn the parameters of the model and the variational distribution. Maximizing the ELBO is equivalent to minimizing the KL divergence between the variational posterior and the true posterior distribution, thereby improving the approximation of the posterior.


## Langevin Dynamics
Langevin dynamics is a sampling method that combines gradient information with stochastic noise to sample from a target distribution. It is particularly useful for sampling from complex, high-dimensional distributions where traditional methods may struggle. The Langevin dynamics update rule is given by:
$$
x_{t+1} = x_t + \frac{\epsilon}{2} \nabla \log p(x_t) + \sqrt{\epsilon} \eta_t
$${#eq-langevin-dynamics}



# Diffusion Models
Diffusion models[@DenoisingDiffusionProbabilistic2020ho] are a class of generative models that have gained significant attention in recent years for their ability to generate high-quality data, particularly images. The base idea of diffusion models is: <u><tag style='color:red'>Iteratively converting noise into data</tag></u>.




# Case Study 

## Text-to-Image Generation


## Video Generation

Movie Gen 

## Diffusion Policy 

## Diffusion Language Models 


# Learning Resource 

There are many good learning resource available online, thanks those who are opening those content:

Lectures:

- [MIT 6.S183: A Practical Introduction to Diffusion Models](https://www.practical-diffusion.org/): 
- [MIT 6.S184: Generative AI with Stochastic Differential Equations](https://diffusion.csail.mit.edu/): focusing on diffusion models through the lens of SDEs.  The ODE/SDE is based on this lecture

- [KAIST: CS492(D): Diffusion Models and Their Applications](https://mhsung.github.io/kaist-cs492d-fall-2024/): More comprehensive introduction to the diffusion models 
- [Stanford CS236 Deep Generative Models](https://deepgenerativemodels.github.io/): Introduce different generative models from VAE to GAN and DDPM 

Blogs:

- [ Generative Modeling by Estimating Gradients of the Data Distribution By Yang Song](https://yang-song.net/blog/2021/score/)
- [What are Diffusion Models? By Lilian Wang](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#nice)