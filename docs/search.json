[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Yuyang",
    "section": "",
    "text": "I am a recent graduate from Singapore Management University in MITB (AI Track). With a strong foundation in computer science and big data, I have hands-on experience in machine learning and deep learning, and I’m passionate about building impactful AI-driven solutions. I am currently seeking full-time opportunities in the AI field where I can contribute, grow, and make a meaningful impact. Open to Work – feel free to reach out at zhangyuyang1211@gmail.com!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Yuyang",
    "section": "Education",
    "text": "Education\n\n  \n    \n    \n      2023 - 2025\n      Master of IT in Business\n      Singapore Management University\n    \n  \n  \n    \n    \n      2020 - 2023\n      Bachelor of Computer Science\n      University of Wollongong"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About Yuyang",
    "section": "Work Experience",
    "text": "Work Experience\n\n  \n    \n    \n      March 2025  - Present\n      Research Assistant Intern\n      SMART"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "About Yuyang",
    "section": "Projects",
    "text": "Projects\n\n  \n    \n    \n       File-Tuning Large Visual Language Model\n          [GitHub]\n          [Blog]\n      \n        Wrting the"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "About Yuyang",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n  \n    Python Development\n    Algorithm Design\n    Deep Learning\n    Machine Learning\n    Computer Vision\n    Natural Language Processing\n    Deep Reinforcement Learning\n    Graph Neural Networks\n    Large Language Model\n    Model Compression\n    Convex Optimization\n    Probabilistic Graph Model\n    Meta Learning\n    Deep Generative Model"
  },
  {
    "objectID": "posts/projects.html",
    "href": "posts/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Visual Language Model\n\n14 min\n\n\nLarge Language Model\n\nMulti-Modality\n\n\n\nIn this project, I will implement a Visual Language Model (VLM) using ‘pure’ PyTorch, which is a model that can understand and generate text based on visual inputs.\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaliGemma Inference and Fine Tuning\n\n1 min\n\n\nLarge Language Model\n\nMulti-Modality\n\nFine-Tuning\n\n\n\nBuilt a PaliGemma model from scratch in PyTorch, loaded the 3B (224x224) model, fine-tuned it with LoRA for specific tasks, and developed a Gradio app to showcase its…\n\n\n\nYuyang Zhang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Projects/VLM/vlm.html",
    "href": "posts/Projects/VLM/vlm.html",
    "title": "Visual Language Model",
    "section": "",
    "text": "0.1 Vision Transformer\n  \n  0.1.1 Patch Embedding\n  0.1.2 Multi Head Self-Attention\n  0.1.3 Feed Forward Network\n  0.1.4 Transformer Block\n  0.1.5 Vision Transformer\n  0.1.6 Modality Projection\n  \n  0.2 Language Model\n  1 Data Prepare"
  },
  {
    "objectID": "posts/Projects/VLM/vlm.html#vision-transformer",
    "href": "posts/Projects/VLM/vlm.html#vision-transformer",
    "title": "Visual Language Model",
    "section": "0.1 Vision Transformer",
    "text": "0.1 Vision Transformer\nThe vision encoder is the classic Vision Transformer (ViT) architecture as proposed in (Dosovitskiy et al. 2021).\n\n\n\n\n\n\nFigure 1: The illustration of the Vision Transformer.\n\n\n\n\n0.1.1 Patch Embedding\nThe first component of the ViT is the Patch Embedding layer, which converts the input image into a sequence of patches. Each patch is treated as a token, similar to how words are treated in NLP models. The patches are flattened and linearly projected into a higher-dimensional space. We can combine those two steps into a single convolutional layer. After patching the image into smaller patches, we can add a learnable class token and positional embeddings to the sequence of patches. The class token is used for classification tasks, while positional embeddings help the model understand the spatial relationships between patches.\nclass ViTPatchEmbedding(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.img_size = config.vit_img_size\n        self.patch_size = config.vit_patch_size\n\n        assert (\n            self.img_size % self.patch_size == 0\n        ), \"Image size must be divisible by patch size.\"\n        self.num_patches = (self.img_size // self.patch_size) ** 2\n        self.cls_flag = config.vit_cls_flag\n        self.embd_dim = config.vit_hidden_dim\n\n        self.conv = nn.Conv2d(\n            in_channels=3,\n            out_channels=self.embd_dim,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n            padding=\"valid\",\n        )\n\n        if self.cls_flag:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embd_dim))  # (B, 1, D)\n            self.position_embeddings = nn.Parameter(\n                torch.zeros(1, self.num_patches + 1, self.embd_dim)\n            )  # (B, P+1, D)\n        else:\n            self.position_embeddings = nn.Parameter(\n                torch.zeros(1, self.num_patches, self.embd_dim)\n            )\n\n    def forward(self, imgs: torch.Tensor):\n        # (B, C, H, W) -&gt; (B, D, H // P, W // P)\n        x = self.conv(imgs)\n        # (B, D, H // P, W // P) -&gt; (B, H // P * W // P, D)\n        x = x.flatten(2).transpose(1, 2)\n\n        if self.cls_flag:\n            cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n            x = torch.cat((cls_tokens, x), dim=1)  # (B, P+1, D)\n\n        x = x + self.position_embeddings  # (B, P+1, D) or (B, P, D)\n        return x\n\n\n0.1.2 Multi Head Self-Attention\nAfter we get the tokens from the Patch Embedding layer, we can feed those tokens into the transformer block. The transformer block consists of a Multi-Head Self-Attention (MHSA) layer and a Feed Forward Network (FFN). The MHSA layer allows the model to attend to different parts of the input sequence simultaneously, capturing complex relationships between patches. The FFN is a simple feed-forward neural network that processes the output of the MHSA layer.\ndef scale_dot_product_attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    dropout: float = 0.0,\n):\n\n    d_k = q.shape[-1]\n\n    # Compute the dot product attention scores\n    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (\n        d_k**0.5\n    )  # Scale by the square root of the dimension\n    if mask is not None:\n        attn_scores = attn_scores.masked_fill(mask == 0, float(\"-inf\"))\n    attn_weights = torch.softmax(attn_scores, dim=-1)\n    if dropout &gt; 0.0:\n        attn_weights = torch.nn.functional.dropout(\n            attn_weights, p=dropout, training=True\n        )\n    # Compute the attention output\n    attn_output = torch.matmul(attn_weights, v)  # Shape: (B, S_q, D)\n\n    return attn_output, attn_weights\n\n\n\nclass ViTMultiHeadAttention(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.n_heads = config.vit_n_heads\n        self.embd_dim = config.vit_hidden_dim\n\n        assert (\n            self.embd_dim % self.n_heads == 0\n        ), \"embd_dim must be divisible by num_heads\"\n        self.head_dim = self.embd_dim // self.n_heads\n\n        self.dropout = config.vit_dropout\n\n        self.qkv_proj = nn.Linear(self.embd_dim, 3 * self.embd_dim)\n        self.out_proj = nn.Linear(self.embd_dim, self.embd_dim)\n\n        # Dropout layer\n        self.attn_dropout = nn.Dropout(self.dropout)\n        self.resid_dropout = nn.Dropout(self.dropout)\n\n        # Use scaled dot product attention\n        self.sdpa = hasattr(F, \"scaled_dot_product_attention\")\n        if not self.sdpa:\n            print(\n                \"Warning: Scaled Dot Product Attention not available. Using custom implementation.\"\n            )\n\n    def forward(self, x: torch.Tensor):\n        B, T, C = x.size()\n\n        q, k, v = map(\n            lambda t: t.view(B, T, self.n_heads, self.head_dim).transpose(1, 2),\n            self.qkv_proj(x).chunk(3, dim=-1),\n        )\n\n        if self.sdpa:\n            y = F.scaled_dot_product_attention(\n                q, k, v, dropout_p=self.dropout if self.training else 0.0\n            )\n        else:\n            y, _ = scale_dot_product_attention(\n                q=q, k=k, v=v, dropout=self.dropout if self.training else 0.0\n            )\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.out_proj(y)\n\n        return self.resid_dropout(y)\n\n\n0.1.3 Feed Forward Network\nThe Feed Forward Network (FFN) is a simple two-layer fully connected network with a GeLU activation function in between. It processes the output of the MHSA layer and applies a residual connection to the input.\nclass MLP(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.activation_fn = nn.GELU(approximate=\"tanh\")\n        self.fc1 = nn.Linear(config.vit_hidden_dim, config.vit_inter_dim)\n        self.fc2 = nn.Linear(config.vit_inter_dim, config.vit_hidden_dim)\n        self.dropout = nn.Dropout(config.vit_dropout)\n\n    def forward(self, x: torch.Tensor):\n        x = self.fc1(x)\n        x = self.activation_fn(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\n0.1.4 Transformer Block\nAfter define the MHSA and FFN layers, we can combine them into a single transformer block. The transformer block applies layer normalization before the MHSA and FFN layers(pre-norm), and it also includes residual connections to help with training stability.\nclass ViTBlock(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.attn = ViTMultiHeadAttention(config)\n        self.mlp = MLP(config)\n        self.ln1 = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n        self.ln2 = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n\n    def forward(self, x: torch.Tensor):\n        # Layer normalization and multi-head attention\n        x = x + self.attn(self.ln1(x))\n        # Layer normalization and MLP\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n\n0.1.5 Vision Transformer\nFinally, we can combine the Patch Embedding layer and the transformer blocks to create the Vision Transformer. The Vision Transformer consists of a series of transformer blocks stacked on top of each other, with the output of the last block being used for classification or further processing.\nclass ViT(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.config = config\n\n        self.patch_embedding = ViTPatchEmbedding(config)\n\n        self.cls_flag = config.vit_cls_flag\n        self.dropout = nn.Dropout(config.vit_dropout)\n\n        self.blocks = nn.ModuleList(\n            [ViTBlock(config) for _ in range(config.vit_n_blocks)]\n        )\n\n        self.layer_norm = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n\n        self.apply(self._init_weights)\n    \n    def forward(self, imgs: torch.Tensor):\n        x = self.patch_embedding(imgs)\n        x = self.dropout(x)\n\n        for block in self.blocks:\n            x = block(x)\n\n        if self.cls_flag:\n            x = x[:, 0]\n        else:\n            x = self.layer_norm(x)\n\n        return x\nSo, that all we need for the Vision Encoder. After we get the output of the\n\n\n0.1.6 Modality Projection\nVision Encoder, we need to project the output into the semantic embedding space to match the text embedding space. This is done using a linear projection layer. One small trick used here is pixel shuffle(Shi et al. 2016), which is used to reduce the number of tokens.\n\n\n\n\n\n\nFigure 2: Illustration of the pixel shuffle operation and un-shuffling process. The pixel shuffle operation rearranges the elements of a tensor to increase the spatial resolution, while the un-shuffle process reverses this operation.\n\n\n\nclass ModalityProjector(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n        \n        self.config = config \n        \n        self.input_dim = config.vit_hidden_dim * (config.mp_pixel_shuffle_factor**2)    \n        self.output_dim = config.lm_hidden_dim\n        self.scale_factor = config.mp_pixel_shuffle_factor\n        \n        \n        self.proj = nn.Linear(self.input_dim, self.output_dim, bias=False)\n        self._init_weight()\n\n    def _init_weight(self):\n        nn.init.normal_(self.proj.weight, mean=0.0, std=0.02)\n    \n    def pixel_shuffle(self, x: torch.Tensor) -&gt; torch.Tensor:\n        B, S, D = x.size()\n        assert S % 2 == 0, \"Input sequence length must be even for pixel shuffle.\"\n        assert S ** 0.5 % self.scale_factor == 0, \"Input sequence length must be a perfect square for pixel shuffle.\"\n        \n        \n        H, W = S, S \n        x = x.view(B, H, W, D) # Convert the flattened sequence into a 2D grid\n        h_out = H // self.scale_factor\n        w_out = W // self.scale_factor\n        \n        x = einops.rearrange(x, \n                             \"b (h sf1) (w sf2) d -&gt; b (h w) (d sf1 sf2)\",\n                             sf1=self.scale_factor, \n                             sf2=self.scale_factor\n                             )\n        \n        return x\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.pixel_shuffle(x)\n        assert x.size(-1) == self.input_dim, f\"Input dimension mismatch: expected {self.input_dim}, got {x.size(-1)}\"\n        \n        x = self.proj(x)\n        return x\nAfter the pixel shuffle, the number of tokens is reduced by a factor of mp_pixel_shuffle_factor**2, which helps to reduce the computational cost while maintaining the semantic information of the visual input. The output of the Modality Projector is then ready to be fed into the Language Model (LM) for further processing."
  },
  {
    "objectID": "posts/Projects/VLM/vlm.html#language-model",
    "href": "posts/Projects/VLM/vlm.html#language-model",
    "title": "Visual Language Model",
    "section": "0.2 Language Model",
    "text": "0.2 Language Model"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html",
    "href": "posts/Blogs/speed-up-training.html",
    "title": "Speed Up Training for Neural Networks",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Data Representation\n  1.2 Calculate Memory Usage of Model\n  1.3 Collective operations\n  \n  2 Profiling\n  \n  2.1 Simple Benchmarking\n  2.2 PyTorch Profiler\n  2.3 NVIDIA Nsight Systems\n  \n  3 Single GPU Optimization\n  \n  3.1 Fusion\n  3.2 Tiling\n  3.3 Memory Coalescing\n  3.4 Mixed Precision Training\n  3.5 Gradient Accumulation\n  3.6 Case Study: Flash Attention\n  \n  4 Multi-GPU Optimization(Parallelism)\n  \n  4.1 Data Parallelism\n  4.2 Model Parallelism\n  4.3 Pipeline Parallelism\n  4.4 Tensor Parallelism\n  4.5 Context Parallelism\n  4.6 Case Study: DeepSpeed\n  4.7 Case Study: Megatron-LM\nTraining large neural networks(such as large language models) can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculations on a single GPU through different techniques such as fusion, tiling, memory coalescing, and parallelizing the training across multiple GPUs such as model parallelism and data parallelism. But before that, we need to understand the basic concepts of GPU, and data types to better understand why and when we need to use these techniques."
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#preliminary",
    "href": "posts/Blogs/speed-up-training.html#preliminary",
    "title": "Speed Up Training for Neural Networks",
    "section": "1 Preliminary",
    "text": "1 Preliminary\n\n1.1 Data Representation\nIn deep learning, we often use different data types to represent our data and model parameters. The most common data types are:\n\nFloat32: also known as single-precision floating-point format. This is the default floating-point representation  used in most deep learning frameworks. It provides a good balance between precision and performance. It use 32 bits (4 bytes) to represent a number.\nFloat16: This is a half-precision floating-point representation that uses 16 bits instead of 32 bits. It can significantly speed up training and reduce memory usage, but it may lead to numerical instability in some cases.\nBFloat16: This is a truncated version of Float32 that retains the exponent bits but reduces the mantissa bits. It is designed to provide a good trade-off between precision and performance, especially for training large models.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The representation of Float32\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The representation of Float16\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) The representation of BFloat16\n\n\n\n\n\n\n\nFigure 1: The representation of float32, float16, and bfloat16 data types. The figure shows how the bits are allocated for the sign, exponent, and mantissa in each data type.\n\n\n\n\n\n\n\n\n\nHow those bits represent the number?\n\n\n\n\\[\n\\text{value} = (-1)^s \\times (1.f) \\times 2^{e - 127}\n\\]\nwhere:\n\n\\(s\\) is the sign bit (0 for positive, 1 for negative)\n\\(f\\) is the mantissa (the fractional part): \\(1.f = 1 + \\sum_{i=1}^{23} b_i \\cdot 2^{-i}\\), where \\(b_i\\) are the bits of the mantissa either 0 or 1.\n\\(e\\) is the exponent (an 8-bit unsigned int) with a bias of 127:\n\nFor Float32, \\(e\\) is 8 bits, which range from [1, 254]\nFor Float16, \\(e\\) is 5 bits, which range from [1, 30]\nFor BFloat16, \\(e\\) is 8 bits, which range from [1, 254]\n\n\n\n\nTo check the data type of a tensor and its properties in PyTorch, you can use the .dtype attribute. For example:\nx = torch.zeros(4, 8)\nx.dtype # check the data type of x\nx.numel() # check the number of elements in x\nx.element_size() # check the size of each element in bytes\nx.numel() * x.element_size() # check the total size in bytes\n\n\n1.2 Calculate Memory Usage of Model\nAssume we have a model with \\(N\\) parameters, and each parameter is represented by float32 (4 bytes). \\(A\\) is the number of activation elements stored during forward (depends on input and model depth). How can we calculate the memory need for training this model? Notice that the memory usage of a model is not only determined by the parameters, but also by:\n\nactivations\ngradients\noptimizer states\n\nFor a single parameter, the memory usage for one forward pass and backward pass is:\n\nparameter: 4 bytes (float32)\nactivation: 4 bytes (float32)\ngradient: 4 bytes (float32)\noptimizer state, which can vary depending on the optimizer used. For example, Adam optimizer requires 2 additional states (momentum and variance), each of which is 4 bytes (float32).\n\nSo, the total memory usage for one parameter is: \\[\n\\text{Memory per parameter} = 4 + 4  + 2 \\times 4 = 16 \\text{ bytes}\n\\]\nThus, the total memory usage for the model is: \\[\n\\text{Total Memory} = N \\times 16 \\text{ bytes} + A \\times 4 \\text{ bytes}\n\\]\n\n\n\n\n\n\nWhy need activation for backward pass?\n\n\n\nSay a layer denotes the input to the layer as \\(\\mathbf{x}\\), the weights as \\(\\theta\\), and the output as \\(\\mathbf{y}\\). The loss function is denoted as \\(L\\), which is a function of the output \\(\\mathbf{y}\\) and the target \\(t\\): \\[\n\\mathbf{y} = f(\\mathbf{x}; \\theta)\n\\] To compute the gradient of the loss with respect to the weights, we need to use the chain rule: \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\theta}\n\\]\nwhere \\(\\frac{\\partial \\mathbf{y}}{\\partial \\theta}\\) is the gradient of the output with respect to the weights \\(\\theta\\), which usually are the function of the input \\(\\mathbf{x}\\) and the weights \\(\\theta\\). To compute this gradient, we need to know the input \\(\\mathbf{x}\\). For example, the linear layer computes: \\[\n\\mathbf{y} = \\mathbf{x} \\cdot \\theta + b\n\\] to compute the gradient, we need to know the input \\(\\mathbf{x}\\), \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\mathbf{x}\n\\] where \\(\\mathbf{x}\\) is the input to the layer, which is the activation of the previous layer. Thus, we need to store the activation for the backward pass.\n\n\n\n\nCalculate Memory Usage of Model\nimport torch\nimport torch.nn as nn\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define a simple model\nmodel = nn.Sequential(\n    nn.Linear(100, 200),\n    nn.ReLU(),\n    nn.Linear(200, 50),\n    nn.ReLU(),\n    nn.Linear(50, 10)\n).to(device)\n\n# Register hooks to track activation sizes\nactivation_sizes = []\n\ndef hook_fn(module, input, output):\n    if isinstance(output, torch.Tensor):\n        activation_sizes.append(output.numel())\n    elif isinstance(output, (list, tuple)):\n        activation_sizes.extend(o.numel() for o in output if isinstance(o, torch.Tensor))\n\nhooks = []\nfor layer in model:\n    hooks.append(layer.register_forward_hook(hook_fn))\n\n# Create input\nx = torch.randn(32, 100, device=device, requires_grad=True)\n\n# Forward pass\noutput = model(x)\nloss = output.mean()\n\n# Backward pass\nloss.backward()\n\n# Remove hooks\nfor h in hooks:\n    h.remove()\n\n# --------- Memory Estimation ---------\n# 1. Parameters\nnum_params = sum(p.numel() for p in model.parameters())\nparam_memory = num_params * 4\ngrad_memory = num_params * 4\noptimizer_memory = num_params * 8\n\n# 2. Activations (sum of all layer outputs + input)\nactivation_memory = (x.numel() + sum(activation_sizes)) * 4  # float32\n\n# 3. Total\ntotal_bytes = param_memory + grad_memory + optimizer_memory + activation_memory\ntotal_MB = total_bytes / (1024 ** 2)\n\n# Print\nprint(f\"Total parameters: {num_params}\")\nprint(f\"Activation elements: {sum(activation_sizes)}\")\nprint(f\"Total training memory (float32): {total_MB:.2f} MB\")\n\n\n\n\n1.3 Collective operations\nCollective operations are operations that involve multiple processes or devices, such as GPUs, to perform a computation. They are essential for parallelizing the training process across multiple GPUs. Some common collective operations include:\n\nBroadcast(Figure 2 (a)): This operation sends data from one process to all other processes. It is commonly used to share model parameters or hyperparameters across multiple GPUs.\nScatter(Figure 2 (b)): This operation distributes data from one process to multiple processes. It is often used to distribute input data across multiple GPUs.\nGather(Figure 2 (c)): This operation collects data from multiple processes and combines it into a single process. It is useful for aggregating results from multiple GPUs.\nReduce(Figure 2 (d)): This operation combines data from multiple processes into a single process. It is commonly used to compute the sum or maximum of gradients across multiple GPUs.\nAll-gather(Figure 2 (e)): This operation collects data from all processes and distributes it back to all processes. It is often used to gather gradients or model parameters from multiple GPUs without losing any information.\nAll-reduce(Figure 2 (g)): This operation combines data from all processes and distributes the result back to all processes. It is often used to average gradients across multiple GPUs during training.\nReduce-scatter(Figure 2 (f)): This operation combines data from multiple processes and distributes the result to each process. It is often used to reduce the amount of data that needs to be communicated between processes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Broadcast\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter\n\n\n\n\n\n\n\n\n\n\n\n(c) Gather\n\n\n\n\n\n\n\n\n\n\n\n(d) Reduce\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) All-gather\n\n\n\n\n\n\n\n\n\n\n\n(f) Reduce-scatter\n\n\n\n\n\n\n\n\n\n\n\n(g) All-reduce\n\n\n\n\n\n\n\nFigure 2: The illustration of different collective operations. The figure shows how data is communicated between processes in each operation.(Image take from: Stanford CS336)\n\n\n\n\nOne should take note is Reduce-Scatter combines two operations:\n\nReduce: Each process (or GPU) contributes its data, and a reduction operation (usually sum, mean, max, etc.) is applied across processes.\nScatter: The reduced result is partitioned and each process gets only a portion (its shard) of the reduced result.\n\n\n\n\n\n\n\nTip\n\n\n\nWay to remember the terminology:\n\nGather: collects data from multiple sources into one destination(not do any operation)\nReduce: performs some associative/commutative operation (sum, min, max)\nBroadcast/Scatter: is inverse of Gather\nAll: means destination is all devices"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#profiling",
    "href": "posts/Blogs/speed-up-training.html#profiling",
    "title": "Speed Up Training for Neural Networks",
    "section": "2 Profiling",
    "text": "2 Profiling\nTo optimize the training process, we need to first profile our model to identify the bottlenecks. In this section, we will introduce several tools to profile the model and understand where the time is spent during training. We will discuss several tools that can help us profile our model and identify the bottlenecks in the training process:\n\nSimple Benchmarking (Section 2.1): The simplest way to measure the time taken for each operation in your model. You can use the time module in Python to measure the time taken for each operation. For example, you can wrap your forward pass in a timer to measure the time taken for each layer.\nPyTorch Profiler (Section 2.2): PyTorch provides a built-in profiler that can help you analyze the performance of your model. You can use the torch.profiler module to profile your model and visualize the results. The profiler provides detailed information about the time spent on each operation, memory usage, and more.\nNVIDIA Nsight Systems (Section 2.3): This is a powerful profiling tool that can help you analyze the performance of your model on NVIDIA GPUs. It provides detailed information about the GPU utilization, memory usage, and more. You can use it to identify bottlenecks in your model and optimize the performance.\n\n\n2.1 Simple Benchmarking\n\n\n2.2 PyTorch Profiler\n\n\n2.3 NVIDIA Nsight Systems"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#single-gpu-optimization",
    "href": "posts/Blogs/speed-up-training.html#single-gpu-optimization",
    "title": "Speed Up Training for Neural Networks",
    "section": "3 Single GPU Optimization",
    "text": "3 Single GPU Optimization\nIn this section, we will discuss various techniques to optimize the training process on a single GPU. These techniques include: - Fusion(Section 3.1): This technique combines multiple operations into a single operation to reduce the number of kernel launches and improve performance. For example, you can fuse the forward and backward passes of a layer into a single operation. - Tiling(Section 3.2): This technique divides the input data into smaller tiles and processes them in parallel to improve memory access patterns and reduce memory usage. For example, you can tile the input data into smaller chunks and process them in parallel. - Memory Coalescing(Section 3.3): This technique optimizes memory access patterns to improve memory bandwidth utilization. For example, you can coalesce memory accesses to reduce the number of memory transactions and improve performance. - Mixed Precision Training(Section 3.4): This technique uses lower precision data types (such as float16 or bfloat16) to reduce memory usage and improve performance. It can significantly speed up training while maintaining model accuracy. PyTorch provides built-in support for mixed precision training through the torch.cuda.amp module, which allows you to automatically cast your model and inputs to lower precision during training. - Gradient Accumulation(Section 3.5): This technique accumulates gradients over multiple mini-batches before performing a weight update. It can help reduce the number of weight updates and improve training stability, especially when using large batch sizes. You can implement gradient accumulation by accumulating gradients in a buffer and updating the model parameters only after a certain number of mini-batches.\n\n3.1 Fusion\n\n\n3.2 Tiling\n\n\n3.3 Memory Coalescing\n\n\n3.4 Mixed Precision Training\n\n\n3.5 Gradient Accumulation\n\n\n3.6 Case Study: Flash Attention"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#multi-gpu-optimizationparallelism",
    "href": "posts/Blogs/speed-up-training.html#multi-gpu-optimizationparallelism",
    "title": "Speed Up Training for Neural Networks",
    "section": "4 Multi-GPU Optimization(Parallelism)",
    "text": "4 Multi-GPU Optimization(Parallelism)\nIn this section, we will discuss various techniques to optimize the training process across multiple GPUs. These techniques include: - Data Parallelism(Section 4.1): This technique splits the input data across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different subset of the input data, and the gradients are averaged across GPUs before updating the model parameters. - Model Parallelism(Section 4.2): This technique splits the model across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the model, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large models that do not fit into a single GPU’s memory. - Pipeline Parallelism(Section 4.3): This technique splits the model into multiple stages and processes each stage in parallel across multiple GPUs. Each GPU processes a different stage of the model, and the output of one stage is passed to the next stage. This can help improve throughput and reduce memory usage, especially for large models. - Tensor Parallelism(Section 4.4): This technique splits the tensors across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the tensor, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large tensors that do not fit into a single GPU’s memory. - Context Parallelism(Section 4.5): This technique splits the context across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the context, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large contexts that do not fit into a single GPU’s memory.\n\n4.1 Data Parallelism\n\n\n4.2 Model Parallelism\n\n\n4.3 Pipeline Parallelism\n\n\n4.4 Tensor Parallelism\n\n\n4.5 Context Parallelism\n\n\n4.6 Case Study: DeepSpeed\n\n\n4.7 Case Study: Megatron-LM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nSelf CPU %\nSelf CPU\nCPU total %\nCPU total\nCPU time avg\nSelf CUDA\nSelf CUDA %\nCUDA total\nCUDA time avg\n# of Calls\n\n\n\n\naten::gelu\n6.76%\n669.785us\n13.48%\n1.336ms\n1.336ms\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\nvoid at::native::vectorized_elementwise_kernel&lt;…&gt;\n0.00%\n0.000us\n0.00%\n0.000us\n0.000us\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\ncudaLaunchKernel\n6.72%\n665.807us\n6.72%\n665.807us\n665.807us\n0.000us\n0.00%\n0.000us\n0.000us\n1\n\n\ncudaDeviceSynchronize\n86.52%\n8.574ms\n86.52%\n8.574ms\n4.287ms\n0.000us\n0.00%\n0.000us\n0.000us\n2\n\n\n\nSelf CPU time total: 9.909 ms\nSelf CUDA time total: 8.642 ms"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Inference/post.html",
    "href": "posts/Blogs/LLM/LLM-Inference/post.html",
    "title": "LLM Part2: Inference",
    "section": "",
    "text": "In the part 1 of the LLM series, we covered the architecture of LLMs, including key components such as position encoding, attention mechanisms, and more. We also explored various normalization techniques and the training process for these models. In the part 2, we will explore different inference techniques, which is necessary for effectively utilizing LLMs in real-world applications. And we will also explore several practical examples and use cases to illustrate these techniques in action, such as: - vLLM"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Alignment/post.html",
    "href": "posts/Blogs/LLM/LLM-Alignment/post.html",
    "title": "LLM Part3: Alignment",
    "section": "",
    "text": "On this page\n   \n  \n  1 Supervised Fine-Tuning (SFT)\n  2 Review of Reinforcement Learning\n  3 RLHF\n  \n\nIn the part 1 and part 2 of the LLM series, we covered the architecture and inference techniques for LLMs. In this part 3, we will focus on alignment techniques, which are crucial for ensuring that LLMs behave in ways that are consistent with human values and intentions. We will explore various methods for aligning LLMs, including reinforcement learning from human feedback (RLHF), and discuss their implications for the development and deployment of these models. We will first explore the simple Supervised Fine-Tuning (SFT) approach, which involves fine-tuning LLMs on curated datasets that reflect human values and preferences. Than we will explore different RLHF techniques, which involve training LLMs using feedback from human evaluators to improve their alignment with human intentions. We will explore algorithms from PPO, DPO to GRPO and\n\n1 Supervised Fine-Tuning (SFT)\n\n\n2 Review of Reinforcement Learning\nIn this part, we will first review the basics of reinforcement learning, including key concepts such as rewards, policy, loss function, actor-critic methods, and more. This will provide a solid foundation for understanding the RLHF techniques we will explore later.\n\n\n3 RLHF"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html",
    "title": "LLM Part1: Architecture",
    "section": "",
    "text": "1 The overview of transformer model\n  2 Position Encoding\n  \n  2.1 Fixed Position Encoding\n  2.2 Relative Position Encoding\n  2.3 Learned Position Encoding\n  2.4 RoPE (Rotary Position Embedding)\n  \n  3 Normalization\n  \n  3.1 Layer Normalization vs. RMS Normalization\n  3.2 Pre-Layer Normalization vs. Post-Layer Normalization\n  3.3 QK Normalization\n  \n  4 Attention Mechanism\n  \n  4.1 Multi Headed Attention\n  4.2 Grouped Query Attention / Multi Query Attention\n  4.3 Multi Latent Attention\n  4.4 Flash Attention\n  4.5 Window Attention\n  4.6 Sliding window attention\n  \n  5 Activations\n  6 Feed Forward Network & Mixture of Experts\n  \n  6.1 Multi Layer Perceptron (MLP)\n  6.2 Gated Linear Unit (GLU)\n  6.3 Mixture of Experts (MoE)\n  \n  7 Model Initialization\n  \n  7.1 Weight Initialization\n  7.2 Layer Initialization\n  \n  8 Case Study\n  \n  8.1 LLaMA\n  8.2 Qwen\n  8.3 DeepSeek\n  8.4 GPT\n  \n  9 Conclusion\nThis is the part 1 of the LLM series, architecture. In this blog, we will going through different transformer architectures that power LLMs. Such as position encoding, attention mechanisms, and more. In the end, we will explore some SOTA LLM architectures, and study how to they combine those techniques to achieve better performance. We are going from bottom to up:\nBesides that, we will also explore different normalization techniques, such as Layer Normalization and RMS Normalization, and the different position of the normalization layers within the architecture. Then, we will going through the training process and how to effectively train these architectures."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#fixed-position-encoding",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#fixed-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.1 Fixed Position Encoding",
    "text": "2.1 Fixed Position Encoding"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#relative-position-encoding",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#relative-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.2 Relative Position Encoding",
    "text": "2.2 Relative Position Encoding"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#learned-position-encoding",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#learned-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.3 Learned Position Encoding",
    "text": "2.3 Learned Position Encoding"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#rope-rotary-position-embedding",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#rope-rotary-position-embedding",
    "title": "LLM Part1: Architecture",
    "section": "2.4 RoPE (Rotary Position Embedding)",
    "text": "2.4 RoPE (Rotary Position Embedding)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#layer-normalization-vs.-rms-normalization",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#layer-normalization-vs.-rms-normalization",
    "title": "LLM Part1: Architecture",
    "section": "3.1 Layer Normalization vs. RMS Normalization",
    "text": "3.1 Layer Normalization vs. RMS Normalization"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#pre-layer-normalization-vs.-post-layer-normalization",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#pre-layer-normalization-vs.-post-layer-normalization",
    "title": "LLM Part1: Architecture",
    "section": "3.2 Pre-Layer Normalization vs. Post-Layer Normalization",
    "text": "3.2 Pre-Layer Normalization vs. Post-Layer Normalization"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#qk-normalization",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#qk-normalization",
    "title": "LLM Part1: Architecture",
    "section": "3.3 QK Normalization",
    "text": "3.3 QK Normalization"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-headed-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-headed-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.1 Multi Headed Attention",
    "text": "4.1 Multi Headed Attention"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#grouped-query-attention-multi-query-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#grouped-query-attention-multi-query-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.2 Grouped Query Attention / Multi Query Attention",
    "text": "4.2 Grouped Query Attention / Multi Query Attention"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-latent-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-latent-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.3 Multi Latent Attention",
    "text": "4.3 Multi Latent Attention"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#flash-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#flash-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.4 Flash Attention",
    "text": "4.4 Flash Attention"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#window-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#window-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.5 Window Attention",
    "text": "4.5 Window Attention"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#sliding-window-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#sliding-window-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.6 Sliding window attention",
    "text": "4.6 Sliding window attention"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-layer-perceptron-mlp",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-layer-perceptron-mlp",
    "title": "LLM Part1: Architecture",
    "section": "6.1 Multi Layer Perceptron (MLP)",
    "text": "6.1 Multi Layer Perceptron (MLP)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#gated-linear-unit-glu",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#gated-linear-unit-glu",
    "title": "LLM Part1: Architecture",
    "section": "6.2 Gated Linear Unit (GLU)",
    "text": "6.2 Gated Linear Unit (GLU)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#mixture-of-experts-moe",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#mixture-of-experts-moe",
    "title": "LLM Part1: Architecture",
    "section": "6.3 Mixture of Experts (MoE)",
    "text": "6.3 Mixture of Experts (MoE)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#weight-initialization",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#weight-initialization",
    "title": "LLM Part1: Architecture",
    "section": "7.1 Weight Initialization",
    "text": "7.1 Weight Initialization"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#layer-initialization",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#layer-initialization",
    "title": "LLM Part1: Architecture",
    "section": "7.2 Layer Initialization",
    "text": "7.2 Layer Initialization"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#llama",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#llama",
    "title": "LLM Part1: Architecture",
    "section": "8.1 LLaMA",
    "text": "8.1 LLaMA"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#qwen",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#qwen",
    "title": "LLM Part1: Architecture",
    "section": "8.2 Qwen",
    "text": "8.2 Qwen"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#deepseek",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#deepseek",
    "title": "LLM Part1: Architecture",
    "section": "8.3 DeepSeek",
    "text": "8.3 DeepSeek"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#gpt",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#gpt",
    "title": "LLM Part1: Architecture",
    "section": "8.4 GPT",
    "text": "8.4 GPT"
  },
  {
    "objectID": "posts/notes.html",
    "href": "posts/notes.html",
    "title": "Learning Notes",
    "section": "",
    "text": "Note\n\n\n\n\nI have moved all my learning notes to this website. You can find them at Course Notes.\n\n\n\n\n\n\nAbout Page of the Course notes"
  },
  {
    "objectID": "posts/Projects/PaliGemma/pali-gemma.html",
    "href": "posts/Projects/PaliGemma/pali-gemma.html",
    "title": "PaliGemma Inference and Fine Tuning",
    "section": "",
    "text": "Here is the full process of the Pali-Gemma\n\n\n\n\n\n\n\nFigure 1: The Pali-Gemma model is a multi-modal large language model that integrates vision and language tasks. The full process includes data collection, model training, and fine-tuning for specific applications."
  },
  {
    "objectID": "posts/blogs.html",
    "href": "posts/blogs.html",
    "title": "👋🏻Welcome to Yuyang’s Blog",
    "section": "",
    "text": "LLM Part1: Architecture\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn this blog, we will going through different transformer architectures that power LLMs. Such as position encoding, attention mechanisms, and more. In the end, we will explore some SOTA LLM architectures, and study how to they combine those techniques to achieve better performance.\n\n\n\n\n2 min\n\n238 words\n\n\n2025-08-25\n\n\n\n\n\n\n\nLLM Part3: Alignment\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn this blog, we will going through different alignment techniques for LLMs, including how to effectively align them with human values and intentions. We will explore techniques such as reinforcement learning from human feedback (RLHF), and more. By the end, you will have a solid understanding of how to align LLMs for your own applications.\n\n\n\n\n1 min\n\n178 words\n\n\n2025-08-25\n\n\n\n\n\n\n\nLLM Part2: Inference\n\n\n\nLarge Language Model\n\nInference\n\n\n\nIn this blog, we will going through the inference process of LLMs, including how to effectively use them for various tasks. We will explore techniques such as prompt engineering, few-shot learning, and more. By the end, you will have a solid understanding of how to leverage LLMs for your own applications.\n\n\n\n\n1 min\n\n79 words\n\n\n2025-08-25\n\n\n\n\n\n\n\nSpeed Up Training for Neural Networks\n\n\n\nTraining Tricks\n\n\n\nTraining large neural networks can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculating in the single GPUs, and parallelizing the training across multiple GPUs.\n\n\n\n\n\nYuyang Zhang\n\n14 min\n\n2,648 words\n\n\n2025-08-07\n\n\n\n\n\nNo matching items"
  }
]