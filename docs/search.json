[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Yuyang",
    "section": "",
    "text": "I am a recent graduate from Singapore Management University in MITB (AI Track). With a strong foundation in computer science and big data, I have hands-on experience in machine learning and deep learning, and I’m passionate about building impactful AI-driven solutions. I am currently seeking full-time opportunities in the AI field where I can contribute, grow, and make a meaningful impact. Open to Work – feel free to reach out at zhangyuyang1211@gmail.com!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Yuyang",
    "section": "Education",
    "text": "Education\n\n  \n    \n    \n      2023 - 2025\n      Master of IT in Business\n      Singapore Management University\n    \n  \n  \n    \n    \n      2020 - 2023\n      Bachelor of Computer Science\n      University of Wollongong"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About Yuyang",
    "section": "Work Experience",
    "text": "Work Experience\n\n  \n    \n    \n      March 2025  - Present\n      Research Assistant Intern\n      SMART"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "About Yuyang",
    "section": "Projects",
    "text": "Projects\n\n  \n    \n    \n       File-Tuning Large Visual Language Model\n          [GitHub]\n          [Blog]\n      \n        Wrting the"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "About Yuyang",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n  \n    Python Development\n    Algorithm Design\n    Deep Learning\n    Machine Learning\n    Computer Vision\n    Natural Language Processing\n    Deep Reinforcement Learning\n    Graph Neural Networks\n    Large Language Model\n    Model Compression\n    Convex Optimization\n    Probabilistic Graph Model\n    Meta Learning\n    Deep Generative Model"
  },
  {
    "objectID": "posts/projects.html",
    "href": "posts/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Visual Language Model\n\n14 min\n\n\nLarge Language Model\n\nMulti-Modality\n\n\n\nIn this project, I will implement a Visual Language Model (VLM) using ‘pure’ PyTorch, which is a model that can understand and generate text based on visual inputs.\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaliGemma Inference and Fine Tuning\n\n1 min\n\n\nLarge Language Model\n\nMulti-Modality\n\nFine-Tuning\n\n\n\nBuilt a PaliGemma model from scratch in PyTorch, loaded the 3B (224x224) model, fine-tuned it with LoRA for specific tasks, and developed a Gradio app to showcase its…\n\n\n\nYuyang Zhang\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/Projects/VLM/vlm.html",
    "href": "posts/Projects/VLM/vlm.html",
    "title": "Visual Language Model",
    "section": "",
    "text": "0.1 Vision Transformer\n  \n  0.1.1 Patch Embedding\n  0.1.2 Multi Head Self-Attention\n  0.1.3 Feed Forward Network\n  0.1.4 Transformer Block\n  0.1.5 Vision Transformer\n  0.1.6 Modality Projection\n  \n  0.2 Language Model\n  1 Data Prepare"
  },
  {
    "objectID": "posts/Projects/VLM/vlm.html#vision-transformer",
    "href": "posts/Projects/VLM/vlm.html#vision-transformer",
    "title": "Visual Language Model",
    "section": "0.1 Vision Transformer",
    "text": "0.1 Vision Transformer\nThe vision encoder is the classic Vision Transformer (ViT) architecture as proposed in (Dosovitskiy et al. 2021).\n\n\n\n\n\n\nFigure 1: The illustration of the Vision Transformer.\n\n\n\n\n0.1.1 Patch Embedding\nThe first component of the ViT is the Patch Embedding layer, which converts the input image into a sequence of patches. Each patch is treated as a token, similar to how words are treated in NLP models. The patches are flattened and linearly projected into a higher-dimensional space. We can combine those two steps into a single convolutional layer. After patching the image into smaller patches, we can add a learnable class token and positional embeddings to the sequence of patches. The class token is used for classification tasks, while positional embeddings help the model understand the spatial relationships between patches.\nclass ViTPatchEmbedding(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.img_size = config.vit_img_size\n        self.patch_size = config.vit_patch_size\n\n        assert (\n            self.img_size % self.patch_size == 0\n        ), \"Image size must be divisible by patch size.\"\n        self.num_patches = (self.img_size // self.patch_size) ** 2\n        self.cls_flag = config.vit_cls_flag\n        self.embd_dim = config.vit_hidden_dim\n\n        self.conv = nn.Conv2d(\n            in_channels=3,\n            out_channels=self.embd_dim,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n            padding=\"valid\",\n        )\n\n        if self.cls_flag:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embd_dim))  # (B, 1, D)\n            self.position_embeddings = nn.Parameter(\n                torch.zeros(1, self.num_patches + 1, self.embd_dim)\n            )  # (B, P+1, D)\n        else:\n            self.position_embeddings = nn.Parameter(\n                torch.zeros(1, self.num_patches, self.embd_dim)\n            )\n\n    def forward(self, imgs: torch.Tensor):\n        # (B, C, H, W) -&gt; (B, D, H // P, W // P)\n        x = self.conv(imgs)\n        # (B, D, H // P, W // P) -&gt; (B, H // P * W // P, D)\n        x = x.flatten(2).transpose(1, 2)\n\n        if self.cls_flag:\n            cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n            x = torch.cat((cls_tokens, x), dim=1)  # (B, P+1, D)\n\n        x = x + self.position_embeddings  # (B, P+1, D) or (B, P, D)\n        return x\n\n\n0.1.2 Multi Head Self-Attention\nAfter we get the tokens from the Patch Embedding layer, we can feed those tokens into the transformer block. The transformer block consists of a Multi-Head Self-Attention (MHSA) layer and a Feed Forward Network (FFN). The MHSA layer allows the model to attend to different parts of the input sequence simultaneously, capturing complex relationships between patches. The FFN is a simple feed-forward neural network that processes the output of the MHSA layer.\ndef scale_dot_product_attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    dropout: float = 0.0,\n):\n\n    d_k = q.shape[-1]\n\n    # Compute the dot product attention scores\n    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (\n        d_k**0.5\n    )  # Scale by the square root of the dimension\n    if mask is not None:\n        attn_scores = attn_scores.masked_fill(mask == 0, float(\"-inf\"))\n    attn_weights = torch.softmax(attn_scores, dim=-1)\n    if dropout &gt; 0.0:\n        attn_weights = torch.nn.functional.dropout(\n            attn_weights, p=dropout, training=True\n        )\n    # Compute the attention output\n    attn_output = torch.matmul(attn_weights, v)  # Shape: (B, S_q, D)\n\n    return attn_output, attn_weights\n\n\n\nclass ViTMultiHeadAttention(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.n_heads = config.vit_n_heads\n        self.embd_dim = config.vit_hidden_dim\n\n        assert (\n            self.embd_dim % self.n_heads == 0\n        ), \"embd_dim must be divisible by num_heads\"\n        self.head_dim = self.embd_dim // self.n_heads\n\n        self.dropout = config.vit_dropout\n\n        self.qkv_proj = nn.Linear(self.embd_dim, 3 * self.embd_dim)\n        self.out_proj = nn.Linear(self.embd_dim, self.embd_dim)\n\n        # Dropout layer\n        self.attn_dropout = nn.Dropout(self.dropout)\n        self.resid_dropout = nn.Dropout(self.dropout)\n\n        # Use scaled dot product attention\n        self.sdpa = hasattr(F, \"scaled_dot_product_attention\")\n        if not self.sdpa:\n            print(\n                \"Warning: Scaled Dot Product Attention not available. Using custom implementation.\"\n            )\n\n    def forward(self, x: torch.Tensor):\n        B, T, C = x.size()\n\n        q, k, v = map(\n            lambda t: t.view(B, T, self.n_heads, self.head_dim).transpose(1, 2),\n            self.qkv_proj(x).chunk(3, dim=-1),\n        )\n\n        if self.sdpa:\n            y = F.scaled_dot_product_attention(\n                q, k, v, dropout_p=self.dropout if self.training else 0.0\n            )\n        else:\n            y, _ = scale_dot_product_attention(\n                q=q, k=k, v=v, dropout=self.dropout if self.training else 0.0\n            )\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.out_proj(y)\n\n        return self.resid_dropout(y)\n\n\n0.1.3 Feed Forward Network\nThe Feed Forward Network (FFN) is a simple two-layer fully connected network with a GeLU activation function in between. It processes the output of the MHSA layer and applies a residual connection to the input.\nclass MLP(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.activation_fn = nn.GELU(approximate=\"tanh\")\n        self.fc1 = nn.Linear(config.vit_hidden_dim, config.vit_inter_dim)\n        self.fc2 = nn.Linear(config.vit_inter_dim, config.vit_hidden_dim)\n        self.dropout = nn.Dropout(config.vit_dropout)\n\n    def forward(self, x: torch.Tensor):\n        x = self.fc1(x)\n        x = self.activation_fn(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\n0.1.4 Transformer Block\nAfter define the MHSA and FFN layers, we can combine them into a single transformer block. The transformer block applies layer normalization before the MHSA and FFN layers(pre-norm), and it also includes residual connections to help with training stability.\nclass ViTBlock(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.attn = ViTMultiHeadAttention(config)\n        self.mlp = MLP(config)\n        self.ln1 = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n        self.ln2 = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n\n    def forward(self, x: torch.Tensor):\n        # Layer normalization and multi-head attention\n        x = x + self.attn(self.ln1(x))\n        # Layer normalization and MLP\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n\n0.1.5 Vision Transformer\nFinally, we can combine the Patch Embedding layer and the transformer blocks to create the Vision Transformer. The Vision Transformer consists of a series of transformer blocks stacked on top of each other, with the output of the last block being used for classification or further processing.\nclass ViT(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.config = config\n\n        self.patch_embedding = ViTPatchEmbedding(config)\n\n        self.cls_flag = config.vit_cls_flag\n        self.dropout = nn.Dropout(config.vit_dropout)\n\n        self.blocks = nn.ModuleList(\n            [ViTBlock(config) for _ in range(config.vit_n_blocks)]\n        )\n\n        self.layer_norm = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n\n        self.apply(self._init_weights)\n    \n    def forward(self, imgs: torch.Tensor):\n        x = self.patch_embedding(imgs)\n        x = self.dropout(x)\n\n        for block in self.blocks:\n            x = block(x)\n\n        if self.cls_flag:\n            x = x[:, 0]\n        else:\n            x = self.layer_norm(x)\n\n        return x\nSo, that all we need for the Vision Encoder. After we get the output of the\n\n\n0.1.6 Modality Projection\nVision Encoder, we need to project the output into the semantic embedding space to match the text embedding space. This is done using a linear projection layer. One small trick used here is pixel shuffle(Shi et al. 2016), which is used to reduce the number of tokens.\n\n\n\n\n\n\nFigure 2: Illustration of the pixel shuffle operation and un-shuffling process. The pixel shuffle operation rearranges the elements of a tensor to increase the spatial resolution, while the un-shuffle process reverses this operation.\n\n\n\nclass ModalityProjector(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n        \n        self.config = config \n        \n        self.input_dim = config.vit_hidden_dim * (config.mp_pixel_shuffle_factor**2)    \n        self.output_dim = config.lm_hidden_dim\n        self.scale_factor = config.mp_pixel_shuffle_factor\n        \n        \n        self.proj = nn.Linear(self.input_dim, self.output_dim, bias=False)\n        self._init_weight()\n\n    def _init_weight(self):\n        nn.init.normal_(self.proj.weight, mean=0.0, std=0.02)\n    \n    def pixel_shuffle(self, x: torch.Tensor) -&gt; torch.Tensor:\n        B, S, D = x.size()\n        assert S % 2 == 0, \"Input sequence length must be even for pixel shuffle.\"\n        assert S ** 0.5 % self.scale_factor == 0, \"Input sequence length must be a perfect square for pixel shuffle.\"\n        \n        \n        H, W = S, S \n        x = x.view(B, H, W, D) # Convert the flattened sequence into a 2D grid\n        h_out = H // self.scale_factor\n        w_out = W // self.scale_factor\n        \n        x = einops.rearrange(x, \n                             \"b (h sf1) (w sf2) d -&gt; b (h w) (d sf1 sf2)\",\n                             sf1=self.scale_factor, \n                             sf2=self.scale_factor\n                             )\n        \n        return x\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.pixel_shuffle(x)\n        assert x.size(-1) == self.input_dim, f\"Input dimension mismatch: expected {self.input_dim}, got {x.size(-1)}\"\n        \n        x = self.proj(x)\n        return x\nAfter the pixel shuffle, the number of tokens is reduced by a factor of mp_pixel_shuffle_factor**2, which helps to reduce the computational cost while maintaining the semantic information of the visual input. The output of the Modality Projector is then ready to be fed into the Language Model (LM) for further processing."
  },
  {
    "objectID": "posts/Projects/VLM/vlm.html#language-model",
    "href": "posts/Projects/VLM/vlm.html#language-model",
    "title": "Visual Language Model",
    "section": "0.2 Language Model",
    "text": "0.2 Language Model"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html",
    "href": "posts/Blogs/LLM-Architecture/Post.html",
    "title": "LLM Part1: Architecture",
    "section": "",
    "text": "1 The overview of transformer model\n  2 Position Encoding\n  \n  2.1 Learned Position Encoding\n  2.2 Absolute Position Encoding\n  2.3 Relative Position Encoding\n  2.4 RoPE (Rotary Position Embedding)\n  2.5 ALIBI\n  2.6 Extend to longer context\n  \n  2.6.1 Linear Position Interpolation\n  2.6.2 NTK-Aware Position Interpolation\n  2.6.3 YaRN\n  \n  \n  3 Normalization\n  \n  3.1 Layer Normalization vs. RMS Normalization\n  3.2 Pre-Layer Normalization vs. Post-Layer Normalization\n  3.3 QK Norm\n  \n  4 Attention Mechanism\n  \n  4.1 Multi Headed Attention\n  \n  4.1.1 Time Complexity of Scaled Dot-Product Attention\n  \n  4.2 Grouped Query Attention / Multi Query Attention\n  4.3 Sparse Attention\n  \n  4.3.1 Sliding window attention\n  4.3.2 Dilated Attention\n  \n  4.4 Multi Latent Attention\n  4.5 Flash Attention\n  \n  4.5.1 Flash Attention V1 vs. V2 vs. V3\n  \n  4.6 Native Sparse Attention\n  4.7 Attention Sink\n  \n  5 Activations\n  \n  5.1 Swish\n  5.2 Gated Linear Unit (GLU)\n  \n  6 Feed Forward Network & Mixture of Experts\n  \n  6.1 Multi Layer Perceptron (MLP)\n  6.2 Gated Linear Unit (GLU)\n  6.3 Mixture of Experts (MoE)\n  \n  7 Model Initialization\n  \n  7.1 Weight Initialization\n  7.2 Layer Initialization\n  \n  8 Case Study\n  \n  8.1 LLaMA\n  8.2 Qwen\n  8.3 DeepSeek\n  8.4 GPT-Oss\n  \n  9 Other Architectures\n  \n  9.1 Diffusion Language Models\n  9.2 State Space Model (SSM)\n  \n  10 Conclusion\nThis is the part 1 of the LLM series, architecture. In this blog, we will going through different transformer architectures that power LLMs. Such as position encoding, attention mechanisms, and more. In the end, we will explore some SOTA LLM architectures, and study how to they combine those techniques to achieve better performance. We are going from bottom to up:\nBesides that, we will also explore different normalization techniques, such as Layer Normalization and RMS Normalization, and the different position of the normalization layers within the architecture. Then, we will going through the training process and how to effectively train these architectures."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#learned-position-encoding",
    "href": "posts/Blogs/LLM-Architecture/Post.html#learned-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.1 Learned Position Encoding",
    "text": "2.1 Learned Position Encoding\nIn the absolute position encoding, we put our position information into fixed sinusoidal functions. It is hand-crafted for specific tasks and does not adapt to the data. So, is it possible to learn position encodings from the data itself? It is possible, and this leads us to the concept of learned position encodings. The learned position encodings are typically implemented as additional trainable parameters in the model. Instead of using fixed sinusoidal functions, the model learns to generate position embeddings that are optimized for the specific task and dataset.\nThis method is used in such as Vision Transformers (Dosovitskiy et al. 2021)."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#absolute-position-encoding",
    "href": "posts/Blogs/LLM-Architecture/Post.html#absolute-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.2 Absolute Position Encoding",
    "text": "2.2 Absolute Position Encoding\nAs used in the (Vaswani et al. 2023), absolute position encoding assigns a unique position embedding to each position in the input sequence, regardless of the content of the tokens. One common approach is to use a fixed sinusoidal function to generate the position embeddings. For example: for each position \\(pos\\), the position embedding \\(PE(pos)\\) can be defined as:\n\\[\n\\begin{aligned}\n\\text{PE}(pos, 2i) &= \\sin\\!\\left(pos \\times \\frac{1}{10,000^{2i/d_{\\text{model}}}}\\right) \\\\\n\\text{PE}(pos, 2i+1) &= \\cos\\!\\left(pos \\times \\frac{1}{10,000^{2i/d_{\\text{model}}}}\\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere\n\n\\(d_{\\text{model}}\\) is the dimensionality of the embeddings.\n\\(i\\) is the index of the embedding dimension. The sin function is applied to the even indices \\(2i\\), while the cos function is applied to the odd indices \\(2i+1\\).\n\nWe can illustrate the encoding as following:\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Display of the position with different sequence lengths\n\n\n\n\n\n\n\n\n\n\n\n(b) Display of the position with different d_model\n\n\n\n\n\n\n\nFigure 2: Illustration of Absolute Position Encoding\n\n\n\n\nThere are several properties can be read from the Figure 2 and Equation 1:\n\nPeriodicity: The sine and cosine functions used in the position encoding have a periodic nature, which allows the model to easily learn to attend to relative positions. This is evident in Figure 2 (a), where the position encodings exhibit similar patterns for different sequence lengths.\nDimensionality: The choice of \\(d_{\\text{model}}\\) affects the granularity of the position encodings. As shown in Figure 2 (b), increasing the dimensionality results in more fine-grained position encodings, which can help the model capture more subtle positional information.\nThe low-dimension part of the dmodel is change more frequently than the high-dimension part, allowing the model to adapt more easily to different input lengths.\n\n\nWe chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset \\(k\\), \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\).  Attention Is All You Need \n\nHow to understand this sentence. Let’s first redefine the position encoding. \\[\n\\begin{aligned}\n\\mathrm{PE}(pos,2i) &= \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big), \\\\\n\\mathrm{PE}(pos,2i+1) &= \\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big).\n\\end{aligned}\n\\] where \\(\\alpha_i = 10,000^{2i/d_{\\text{model}}}\\). And we consider \\((2i, 2i+1)\\) as one pair. Now, consider the same pair at position \\(pos + k\\). We can write the position encoding as:\n\\[\n\\begin{align}\n\\mathrm{PE}(pos+k,2i)\n&= \\sin\\!\\Big(\\tfrac{pos+k}{\\alpha_i}\\Big)\n    = \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\cos\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n    + \\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\sin\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big) \\\\\n\\mathrm{PE}(pos+k,2i+1)\n&= \\cos\\!\\Big(\\tfrac{pos+k}{\\alpha_i}\\Big)\n    =\\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\cos\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n    - \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\sin\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n\\end{align}\n\\tag{2}\\]\n\n\nAngle addition formulas: \\[\n\\begin{align*}\n&\\sin(a+b) = \\sin(a)\\cos(b) + \\cos(a)\\sin(b) \\\\\n&\\cos(a+b) = \\cos(a)\\cos(b) - \\sin(a)\\sin(b)\n\\end{align*}\n\\]\nWrite this as vector form:\n\\[\n\\mathbf{p}_{pos}^{(i)} =\n\\begin{bmatrix}\n\\sin(pos/\\alpha_i) \\\\\n\\cos(pos/\\alpha_i)\n\\end{bmatrix}\n\\]\nThen \\(\\mathbf{p}_{pos+k}^{(i)}\\) equal to: \\[\n\\mathbf{p}_{pos+k}^{(i)} =\n\\underbrace{\n\\begin{bmatrix}\n\\cos(\\tfrac{k}{\\alpha_i}) & \\ \\ \\sin(\\tfrac{k}{\\alpha_i}) \\\\\n-\\sin(\\tfrac{k}{\\alpha_i}) & \\ \\ \\cos(\\tfrac{k}{\\alpha_i})\n\\end{bmatrix}\n}_{\\displaystyle R_i(k)}\n\\ \\mathbf{p}_{pos}^{(i)}\n\\]\nNotice that \\(R_i(k)\\) is known as rotation matrix which only depends on the relative position \\(k\\) and not on the absolute position \\(pos\\). This is the key insight that allows the model to generalize to different positions.\nStacking all pairs, \\[\n\\mathrm{PE}(pos+k) =\n\\underbrace{\n    \\begin{pmatrix}\n\\cos\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & \\sin\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & 0 & 0 & \\cdots & 0 & 0 \\\\\n-\\sin\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & \\cos\\!\\big(\\tfrac{k}{\\alpha_2}\\big) &\\sin\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\cdots & 0 & 0 \\\\\n0 & 0 &  -\\sin\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & \\cos\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) & \\sin\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) \\\\\n0 & 0 & 0 & 0 & \\cdots & -\\sin\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big)\n\\end{pmatrix}\n}_{R(k)}\n\\cdot\n\\underbrace{\n    \\begin{pmatrix}\n\\sin(\\tfrac{k}{\\alpha_1}) \\\\\n\\cos(\\tfrac{k}{\\alpha_1}) \\\\\n\\sin(\\tfrac{k}{\\alpha_2}) \\\\\n\\cos(\\tfrac{k}{\\alpha_2}) \\\\\n\\vdots \\\\\n\\sin(\\tfrac{k}{\\alpha_{d/2}}) \\\\\n\\cos(\\tfrac{k}{\\alpha_{d/2}})\n\\end{pmatrix}\n}_{\\mathrm{PE}(pos)}\n\\]\nwhere \\(R(k)\\) is block-diagonal with those \\(2\\times2\\) rotations, \\(R(k)\\) depends on \\(k\\) but not on \\(pos\\) → a linear map of \\(\\mathrm{PE}(pos)\\)."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#relative-position-encoding",
    "href": "posts/Blogs/LLM-Architecture/Post.html#relative-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.3 Relative Position Encoding",
    "text": "2.3 Relative Position Encoding\nRelative Position Encoding, first proposed in Transformer-XL (Dai et al. 2019), then adaptive in different language model.\n\\[\nA_{i,j} =\n\\underbrace{Q_i^\\top K_j}_{\\text{content-based addressing}}\n+\n\\underbrace{Q_i^\\top R_{i-j}}_{\\text{content-dependent positional bias}}\n+\n\\underbrace{u^\\top K_j}_{\\text{global content bias}}\n+\n\\underbrace{v^\\top R_{i-j}}_{\\text{global positional bias}}\n\\]\nwhere:\n\n\\(Q_i \\in \\mathbb{R}^d\\): query vector at position \\(i\\)\n\n\\(K_j \\in \\mathbb{R}^d\\): key vector at position \\(j\\)\n\n\\(R_{i-j} \\in \\mathbb{R}^d\\): embedding of the relative distance \\((i-j)\\)\n\n\\(u, v \\in \\mathbb{R}^d\\): learnable global bias vectors\n\n\\(A_{i,j}\\): unnormalized attention score between position \\(i\\) and \\(j\\)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass RelPositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        self.d_model = d_model\n\n        # relative positions: range [-max_len, max_len]\n        range_len = max_len * 2 + 1\n        self.rel_emb = nn.Embedding(range_len, d_model)\n\n        # trainable biases u, v (Transformer-XL)\n        self.u = nn.Parameter(torch.Tensor(d_model))\n        self.v = nn.Parameter(torch.Tensor(d_model))\n\n    def forward(self, q, k, seq_len):\n        B, H, L, Dh = q.size()\n\n        # (L, L): relative position indices\n        pos_idx = torch.arange(L, dtype=torch.long, device=q.device)\n        rel_idx = pos_idx[None, :] - pos_idx[:, None]  # i-j\n        rel_idx = rel_idx + seq_len  # shift to [0, 2*max_len]\n        rel_pos_emb = self.rel_emb(rel_idx)  # (L, L, d_model)\n\n        # compute QK^T (content-based)\n        content_score = torch.matmul(q, k.transpose(-2, -1))  # (B, H, L, L)\n\n        # project queries with R\n        rel_q = q + self.v.view(1, 1, 1, -1)  # add bias v\n        rel_score = torch.einsum('bhld,lrd-&gt;bhlr', rel_q, rel_pos_emb)\n\n        # add global content bias (u)\n        content_bias = torch.einsum('d,bhjd-&gt;bhj', self.u, k).unsqueeze(2)\n\n        # total score\n        logits = content_score + rel_score + content_bias\n        return logits / (Dh ** 0.5)  # scale as in attention"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#rope-rotary-position-embedding",
    "href": "posts/Blogs/LLM-Architecture/Post.html#rope-rotary-position-embedding",
    "title": "LLM Part1: Architecture",
    "section": "2.4 RoPE (Rotary Position Embedding)",
    "text": "2.4 RoPE (Rotary Position Embedding)\nSo far we have see the absolute position encoding and relative position encoding. However, there is an problem with absolute position encoding. For example, for two sentence:\n\nEvery Day I will go to gym\nI will go to gym every day\n\nThe absolute position encoding is totally different from two sentences, even though they have the same words and means. On the other hand, the problem of the relative position encoding is that it does not capture the absolute position information, which is crucial for understanding the meaning of the sentences for some task such as text summarization.\nRoPE (Su et al. 2023) is a method combine those two. The vector rotated certain degree according to the absolute position in the sentence. On the other hand, it relative position information is preserved. According to Equation 2, the relative position is not related to the position.\n\n\n\n\n\n\nFigure 3: Illustration of RoPE\n\n\n\n\\[\nR_{\\Theta,m}^{d} \\mathbf{x}\n=\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\n\\vdots\\\\\nx_{d-1}\\\\\nx_d\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{2})\\\\\n\\cos(m\\theta_{2})\\\\\n\\vdots\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n- x_2\\\\\nx_1\\\\\n- x_4\\\\\nx_3\\\\\n\\vdots\\\\\n- x_d\\\\\nx_{d-1}\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{2})\\\\\n\\sin(m\\theta_{2})\\\\\n\\vdots\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n\\tag{3}\\]\nwhere\n\\[\n\\theta_{i,d} = \\frac{1}{10,000^{2(i - 1) / d}} ,\\quad i \\in [1, 2, \\dots, d / 2 ]\n\\]\nAs we can see, to implement the RoPE in code, we can:\n\nConstruct cos and sin matrices for the given input dimensions and maximum position.\nApply the rotation to the input embeddings using the constructed matrices.\n\n\n\n\n\n\n\nNote\n\n\n\nOne thing always bother me about this implementation is that the rotate_half function actually swap pair of the last dimension as mentioned in the paper. For example:\n&gt;&gt;&gt; x = torch.arange(0, 24).reshape(3, 8) # (B, D)\n&gt;&gt;&gt; x\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23]])\n&gt;&gt;&gt; x1 = rotate_half(x)\n&gt;&gt;&gt; x1\ntensor([[ -4,  -5,  -6,  -7,   0,   1,   2,   3],\n        [-12, -13, -14, -15,   8,   9,  10,  11],\n        [-20, -21, -22, -23,  16,  17,  18,  19]])\nThe above function just change the x to [-x_{d//2}, ..., -x_{d}, x_0, ..., x_{d//2-1}]\nCheck this linkif you are interested.\nIf this really bother you, you can just implement like this:\ndef rotate_half_v2(x):\n    # Assume x is (B, D)\n    x = x.reshape(x.shape[0], -1, 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return x.reshape(x.shape[0], -1)\nwhich is same as they mentioned in the paper.\n&gt;&gt;&gt; x\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23]])\n&gt;&gt;&gt; x2 = rotate_half_v2(x)\n&gt;&gt;&gt; x2\ntensor([[ -1,   0,  -3,   2,  -5,   4,  -7,   6],\n        [ -9,   8, -11,  10, -13,  12, -15,  14],\n        [-17,  16, -19,  18, -21,  20, -23,  22]])\nOr\ndef rotate_half_v3(x: torch.Tensor) -&gt; torch.Tensor:\n    y = torch.empty_like(x)\n    y[..., ::2] = -x[..., 1::2]  # even positions get -odd\n    y[..., 1::2] =  x[..., ::2]  # odd positions get even\n    return y\n\n&gt;&gt;&gt; x3 = rotate_half_v3(x)\n&gt;&gt;&gt; x3\ntensor([[ -1,   0,  -3,   2,  -5,   4,  -7,   6],\n        [ -9,   8, -11,  10, -13,  12, -15,  14],\n        [-17,  16, -19,  18, -21,  20, -23,  22]])\n\n\nThe other implementation of the RoPE is reply on the complex number. We can treat 2D vector \\((x, y)\\) as a complex number \\(z = x + iy\\), and the rotation can be done by multiplying with a complex exponential:\n\\[\nz' = z \\cdot e^{i\\theta} = (x + iy) \\cdot (\\cos \\theta + i \\sin \\theta) = (x \\cos \\theta - y \\sin \\theta) + i (x \\sin \\theta + y \\cos \\theta)\n\\]\nCode adapted from LLaMA model"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#alibi",
    "href": "posts/Blogs/LLM-Architecture/Post.html#alibi",
    "title": "LLM Part1: Architecture",
    "section": "2.5 ALIBI",
    "text": "2.5 ALIBI\n(Press, Smith, and Lewis 2022)\n\n\n\n\n\n\nFigure 4\n\n\n\nAliBi: simple, monotonic bias → strong extrapolation, lower overhead.\nSo far, for the position embedding, we modify the Q, K to add the position information for attention to calculate. However, is it possible to directly modify the attention score? To notify them the position information? This is exactly what ALIBI does. The ALIBI (Attention with Linear Biases) method introduces a linear bias to the attention scores based on the distance between tokens. The bias is added directly to the attention score before applying the softmax function. Mathematically:\n\\[\n\\operatorname{softmax}\\!\\Big( q_i k_j^\\top \\;+\\; m \\cdot (-(i-j)) \\Big)\n\\]\nwhere:\n\n\\(q_i \\in \\mathbb{R}^d\\): query vector at position \\(i\\)\n\\(k_j \\in \\mathbb{R}^d\\): key vector at position \\(j\\)\n\\(m \\in \\mathbb{R}\\): slop (head-dependent constant)\n\\((i - j) \\geq 0\\): relative distance\n\nFor example, for query \\(i\\), the logits against all keys \\([0, 1, \\dots, i ]\\) become: \\[\n\\ell_i \\;=\\;\n\\Big[\\, q_i k_0^\\top - m(i-0),\\;\n       q_i k_1^\\top - m(i-1),\\;\n       \\ldots,\\;\n       q_i k_{i-1}^\\top - m(1),\\;\n       q_i k_i^\\top \\,\\Big]\n\\]"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#extend-to-longer-context",
    "href": "posts/Blogs/LLM-Architecture/Post.html#extend-to-longer-context",
    "title": "LLM Part1: Architecture",
    "section": "2.6 Extend to longer context",
    "text": "2.6 Extend to longer context\nSo far, for all the position encoding we discussed, it has one main drawback: it fixed maximum context length during training.\n\n\n\n\n\n\nFigure 5: (Image Source Video: Long-Context LLM Extension)\n\n\n\nWhen we are training on the fixed context length, the model learns to attend to the positions within that context. However, during inference, we may want to extend the context length beyond what the model was trained on. This is where the challenge lies. One way is to train a longer context length model from the beginning. However this requires more computational resources and may not be feasible for all applications. So, we need to find a way to adapt the model to longer contexts without retraining it from scratch.\nThere are several approaches to address this issue. Let’s discuss a few of them.\n\n2.6.1 Linear Position Interpolation\n\n\n\n\n\n\nFigure 6\n\n\n\ndecreases the frequencies of the basis functions so that more tokens fit within each period. The position interpolation (Chen et al. 2023) mentioned that we can just linearly interpolate the position embeddings for the extended context. This allows the model to generate position embeddings for longer sequences without requiring additional training. It just rescale the \\(m\\) base in the RoPE by: \\[\nf'(\\mathbf{x}, m) = f(\\mathbf{x}, m\\frac{L}{L'})\n\\] where \\(L\\) is the original context length and \\(L'\\) is the new context length.\n\n\n2.6.2 NTK-Aware Position Interpolation\nThe Linear Position Interpolation, if it was possible to pick the correct scale parameter dynamically based on the sequence length rather than having to settle for the fixed tradeoff of maximum sequence length vs. performance on shorter sequences. The NTK-Aware Position Interpolation method leverages the Neural Tangent Kernel (NTK) framework to adaptively adjust the position embeddings during inference. By analyzing the model’s behavior in the NTK regime, we can identify the optimal scaling factors for different sequence lengths, allowing for more effective extrapolation of position information.\n\\[\n\\alpha^{\\text{NTK-RoPE}}_{j} = \\kappa^{-\\frac{2j}{d_k}}\n\\]\n\n\n\n\n\n\nNeural Tangent Kernel (NTK)\n\n\n\n\n\n\n\n\n2.6.3 YaRN\nanother RoPE extension method, uses “NTK-by-parts” interpolation strategies across different dimensions of the embedding space and introduces a temperature factor to adjust the attention distribution for long inputs. But RoPE cannot extrapolate well to sequences longer than training (e.g., a model trained on 2K tokens struggles at 8K).\n\\[\n\\alpha^{\\mathrm{YaRN}}_{j}\n= \\frac{(1-\\gamma_j)\\,\\tfrac{1}{t} + \\gamma_j}{\\sqrt{T}} \\,.\n\\]\nYaRN modifies RoPE to support longer context windows while preserving model stability. (Peng et al. 2023)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#layer-normalization-vs.-rms-normalization",
    "href": "posts/Blogs/LLM-Architecture/Post.html#layer-normalization-vs.-rms-normalization",
    "title": "LLM Part1: Architecture",
    "section": "3.1 Layer Normalization vs. RMS Normalization",
    "text": "3.1 Layer Normalization vs. RMS Normalization\nThe Layer Normalization (Ba, Kiros, and Hinton 2016) is a technique to normalize the inputs across the features for each training example. It is defined as:\n\\[\n\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\n\\tag{4}\\] where:\n\n\\(\\mu(x)\\): the mean of the input features.\n\\(\\sigma(x)\\): the standard deviation of the input features.\n\\(\\gamma\\): a learnable scale parameter.\n\\(\\beta\\): a learnable shift parameter.\n\nThere are two learnable parameters in Layer Normalization: \\(\\gamma\\) and \\(\\beta\\), which have the same shape as the input features \\(d_{\\text{model}}\\).\nHowever, in the Root Mean Square(RMS) Normalization, proposed in (Zhang and Sennrich 2019), that we remove the mean from the normalization process. The RMS Normalization is defined as:\n\\[\n\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma\n\\tag{5}\\] where:\n\n\\(\\epsilon\\): a small constant to prevent division by zero.\n\\(\\gamma\\): a learnable scale parameter, which has the same shape as the input features \\(d_{\\text{model}}\\).\n\nAs we can see, the main difference between Layer Normalization and RMS Normalization is the removal of the mean from the normalization process, and remove the learnable shift parameter \\(\\beta\\). There are several advantage of that:\n\nSimplicity: RMS Normalization is simpler and requires fewer parameters, making it easier to implement and faster to compute. For model with \\(d_{\\text{model}} = 512\\), 8 layers, each layer has 2 normalization. Than the reduction number of parameter is up to \\(512 \\times 8 \\times 2 = 8192\\) parameters.\nFewer operations: no mean subtraction, no bias addition.\nSaves memory bandwidth, which is often the bottleneck in GPUs (not FLOPs).\nWhile reduce the number of parameters, it also maintains similar performance."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#pre-layer-normalization-vs.-post-layer-normalization",
    "href": "posts/Blogs/LLM-Architecture/Post.html#pre-layer-normalization-vs.-post-layer-normalization",
    "title": "LLM Part1: Architecture",
    "section": "3.2 Pre-Layer Normalization vs. Post-Layer Normalization",
    "text": "3.2 Pre-Layer Normalization vs. Post-Layer Normalization\n\n\n\n\n\n\nFigure 7: The figure illustrates the three different position of the normalization layer in the transformer architecture.\n\n\n\nOne main good reason why Pre-Layer Normalization is perform bettern than the Post-Layer Normalization is that, according to (Xiong et al. 2020), the help the gradient flow back through the network without disrupting the residual connections. When using the Pre-Layer Normalization, it tends to converge faster and more stably. And the initlization methods is become less sensitive to the scale of the inputs.\nThere are third type of the normalization position by (Ding et al. 2021) called sandwiched normalization, which is a combination of pre-layer and post-layer normalization. Is is proposed to improve the training for the text-image pair.\n\n\n\n\n\n\nGradient Flow\n\n\n\nOne generalizable lesson is that we should keep residual connections “clean” identity paths."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#qk-norm",
    "href": "posts/Blogs/LLM-Architecture/Post.html#qk-norm",
    "title": "LLM Part1: Architecture",
    "section": "3.3 QK Norm",
    "text": "3.3 QK Norm\nThere is another normalization method called Query-Key Normalization (QK Norm), which is designed to improve the attention mechanism by normalizing the query and key vectors before computing the attention scores."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#multi-headed-attention",
    "href": "posts/Blogs/LLM-Architecture/Post.html#multi-headed-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.1 Multi Headed Attention",
    "text": "4.1 Multi Headed Attention\nThe standard multi headed attention is defined as:\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n\\tag{6}\\]\nwhere each head is computed as:\n\\[\n\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\tag{7}\\] with \\(W_i^Q\\), \\(W_i^K\\), and \\(W_i^V\\) being learned projection matrices.\nAnd the attention function, usually is scaled dot-product attention, is defined as: \\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\tag{8}\\]\nwhere \\(d_k\\) is the dimension of the keys. The reason for the scaling factor \\(\\sqrt{d_k}\\) is to counteract the effect of large dot-product values by the large dimension of \\(K\\), which can push the softmax function into regions with very small gradients.\n\n4.1.1 Time Complexity of Scaled Dot-Product Attention\nThe time complexity of the scaled dot-product attention mechanism can be analyzed as follows:\n\nQuery-Key Dot Product: The computation of the dot product between the query and key matrices has a time complexity of \\(O(n^2 d_k)\\), where \\(n\\) is the sequence length and \\(d_k\\) is the dimension of the keys.\nSoftmax Computation: The softmax function is applied to the dot product results, which has a time complexity of \\(O(n^2)\\).\nValue Weighting: The final step involves multiplying the softmax output with the value matrix, which has a time complexity of \\(O(n^2 d_v)\\), where \\(d_v\\) is the dimension of the values.\n\nOverall, the time complexity of the scaled dot-product attention is dominated by the query-key dot product and can be expressed as:\n\\[\nO(n^2 (d_k + d_v))\n\\tag{9}\\]\nAs we can see, the time complexity is quadratic in the sequence length, which can be a bottleneck for long sequences / contexts. Let’s see how we can improve it."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#grouped-query-attention-multi-query-attention",
    "href": "posts/Blogs/LLM-Architecture/Post.html#grouped-query-attention-multi-query-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.2 Grouped Query Attention / Multi Query Attention",
    "text": "4.2 Grouped Query Attention / Multi Query Attention\n\n\n\n\n\n\nFigure 8: Overview of Grouped Query Attention & Multi Query Attention (Image Source: (Ainslie et al. 2023))\n\n\n\nProposed by the (Ainslie et al. 2023), Grouped Query Attention (GQA) and Multi Query Attention (MQA) are designed to reduce the computational burden of the attention mechanism by grouping queries and sharing keys and values across multiple queries."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#sparse-attention",
    "href": "posts/Blogs/LLM-Architecture/Post.html#sparse-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.3 Sparse Attention",
    "text": "4.3 Sparse Attention\n\nFixed Pattern Attention: for example, sliding window attention\nStrided Attention: for example, dilated attention, every m-th token\nBlock Sparse Attention: for example, BigBird (BigBirdSparseAttention2020zaheer?), Longformer (Beltagy, Peters, and Cohan 2020)\nGlobal Attention:\nLearned / Dynamic Spare Attention\n\n\n4.3.1 Sliding window attention\n\n\n\n\n\n\nFigure 9\n\n\n\n(Beltagy, Peters, and Cohan 2020)\n\n\n4.3.2 Dilated Attention\n\n\n\n\n\n\nFigure 10: The illustration of Sparse Attention. (Image Source: Generating Long Sequences with Sparse Transformers)\n\n\n\n(Child et al. 2019)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#multi-latent-attention",
    "href": "posts/Blogs/LLM-Architecture/Post.html#multi-latent-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.4 Multi Latent Attention",
    "text": "4.4 Multi Latent Attention\n\n\n\n\n\n\nFigure 11: Compare between MHA, Grouped Query Attention, Multi Query Attention and Multi Latent Attention.\n\n\n\nMulti Latent Attention (MLA), proposed in (DeepSeek-AI et al. 2024) is a proposed extension to the attention mechanism that aims to capture more complex relationships within the input data by introducing multiple latent spaces. Each latent space can learn different aspects of the data, allowing for a more nuanced understanding of the input.\n\n\n\n\n\n\nFigure 12: The detail of Multi Latent Attention (MLA)."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#flash-attention",
    "href": "posts/Blogs/LLM-Architecture/Post.html#flash-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.5 Flash Attention",
    "text": "4.5 Flash Attention\nSo far, we see different attention mechanisms that aim to improve the efficiency and effectiveness of the standard attention mechanism. However, all aforementioned methods improve the attention mechanism by approximating the attention calculation. On the other hand, Flash Attention, proposed in (Dao 2023), takes a different approach by optimizing the attention computation itself.\n\n\n\n\n\n\nFigure 13: The illustration of Flash Attention\n\n\n\n\n4.5.1 Flash Attention V1 vs. V2 vs. V3"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#native-sparse-attention",
    "href": "posts/Blogs/LLM-Architecture/Post.html#native-sparse-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.6 Native Sparse Attention",
    "text": "4.6 Native Sparse Attention\n\n\n\n\n\n\nFigure 14: Illustration of Native Sparse Attention (Image Source: Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention)\n\n\n\nProposed in (Yuan et al. 2025), this is a novel approach to sparse attention that aligns with hardware capabilities and allows for efficient training of sparse attention mechanisms."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#attention-sink",
    "href": "posts/Blogs/LLM-Architecture/Post.html#attention-sink",
    "title": "LLM Part1: Architecture",
    "section": "4.7 Attention Sink",
    "text": "4.7 Attention Sink"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#swish",
    "href": "posts/Blogs/LLM-Architecture/Post.html#swish",
    "title": "LLM Part1: Architecture",
    "section": "5.1 Swish",
    "text": "5.1 Swish"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#gated-linear-unit-glu",
    "href": "posts/Blogs/LLM-Architecture/Post.html#gated-linear-unit-glu",
    "title": "LLM Part1: Architecture",
    "section": "5.2 Gated Linear Unit (GLU)",
    "text": "5.2 Gated Linear Unit (GLU)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#multi-layer-perceptron-mlp",
    "href": "posts/Blogs/LLM-Architecture/Post.html#multi-layer-perceptron-mlp",
    "title": "LLM Part1: Architecture",
    "section": "6.1 Multi Layer Perceptron (MLP)",
    "text": "6.1 Multi Layer Perceptron (MLP)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#gated-linear-unit-glu-1",
    "href": "posts/Blogs/LLM-Architecture/Post.html#gated-linear-unit-glu-1",
    "title": "LLM Part1: Architecture",
    "section": "6.2 Gated Linear Unit (GLU)",
    "text": "6.2 Gated Linear Unit (GLU)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#mixture-of-experts-moe",
    "href": "posts/Blogs/LLM-Architecture/Post.html#mixture-of-experts-moe",
    "title": "LLM Part1: Architecture",
    "section": "6.3 Mixture of Experts (MoE)",
    "text": "6.3 Mixture of Experts (MoE)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#weight-initialization",
    "href": "posts/Blogs/LLM-Architecture/Post.html#weight-initialization",
    "title": "LLM Part1: Architecture",
    "section": "7.1 Weight Initialization",
    "text": "7.1 Weight Initialization"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#layer-initialization",
    "href": "posts/Blogs/LLM-Architecture/Post.html#layer-initialization",
    "title": "LLM Part1: Architecture",
    "section": "7.2 Layer Initialization",
    "text": "7.2 Layer Initialization"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#llama",
    "href": "posts/Blogs/LLM-Architecture/Post.html#llama",
    "title": "LLM Part1: Architecture",
    "section": "8.1 LLaMA",
    "text": "8.1 LLaMA"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#qwen",
    "href": "posts/Blogs/LLM-Architecture/Post.html#qwen",
    "title": "LLM Part1: Architecture",
    "section": "8.2 Qwen",
    "text": "8.2 Qwen"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#deepseek",
    "href": "posts/Blogs/LLM-Architecture/Post.html#deepseek",
    "title": "LLM Part1: Architecture",
    "section": "8.3 DeepSeek",
    "text": "8.3 DeepSeek"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#gpt-oss",
    "href": "posts/Blogs/LLM-Architecture/Post.html#gpt-oss",
    "title": "LLM Part1: Architecture",
    "section": "8.4 GPT-Oss",
    "text": "8.4 GPT-Oss"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#diffusion-language-models",
    "href": "posts/Blogs/LLM-Architecture/Post.html#diffusion-language-models",
    "title": "LLM Part1: Architecture",
    "section": "9.1 Diffusion Language Models",
    "text": "9.1 Diffusion Language Models\n\n\n\n\n\n\nFigure 15: Illustration of Diffusion Language Model. (Video Source: Inception Lab)\n\n\n\nLLaDA (Nie et al. 2025) is a diffusion-based language model that leverages the principles of diffusion models to generate text. By modeling the text generation process as a diffusion process, LLaDA aims to improve the quality and diversity of generated text.\n\n\n\n\n\n\nFigure 16: Example of LLaDA generation process. Prompt: “Explain what artificial intelligence is.” (Image Source: LLaDA demo)\n\n\n\n\n\n\n\n\n\nFigure 17: The training process and sampling process of LLaDA. (Image Source: LLaDA)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/Post.html#state-space-model-ssm",
    "href": "posts/Blogs/LLM-Architecture/Post.html#state-space-model-ssm",
    "title": "LLM Part1: Architecture",
    "section": "9.2 State Space Model (SSM)",
    "text": "9.2 State Space Model (SSM)"
  },
  {
    "objectID": "posts/Blogs/LLM-Inference/post.html",
    "href": "posts/Blogs/LLM-Inference/post.html",
    "title": "LLM Part2: Inference",
    "section": "",
    "text": "In the part 1 of the LLM series, we covered the architecture of LLMs, including key components such as position encoding, attention mechanisms, and more. We also explored various normalization techniques and the training process for these models. In the part 2, we will explore different inference techniques, which is necessary for effectively utilizing LLMs in real-world applications. And we will also explore several practical examples and use cases to illustrate these techniques in action, such as:\n\nvLLM\n\nhttps://lilianweng.github.io/posts/2023-01-10-inference-optimization/ # Resource Different Architecture: - Mixture of Recursion: https://arxiv.org/abs/2507.10524 - Diffusion Text:\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html",
    "href": "posts/Blogs/Speed-up-training/post.html",
    "title": "Speed Up Training for Neural Networks",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Data Representation\n  1.2 Calculate Memory Usage of Model\n  1.3 Collective operations\n  \n  2 Profiling\n  \n  2.1 Simple Benchmarking\n  2.2 PyTorch Profiler\n  2.3 NVIDIA Nsight Systems\n  \n  3 Single GPU Optimization\n  \n  3.1 Fusion\n  3.2 Tiling\n  3.3 Memory Coalescing\n  3.4 Mixed Precision Training\n  3.5 Gradient Accumulation\n  3.6 Case Study: Flash Attention\n  \n  4 Multi-GPU Optimization(Parallelism)\n  \n  4.1 Data Parallelism\n  4.2 Model Parallelism\n  4.3 Pipeline Parallelism\n  4.4 Tensor Parallelism\n  4.5 Context Parallelism\n  4.6 Case Study: DeepSpeed\n  4.7 Case Study: Megatron-LM\nTraining large neural networks(such as large language models) can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculations on a single GPU through different techniques such as fusion, tiling, memory coalescing, and parallelizing the training across multiple GPUs such as model parallelism and data parallelism. But before that, we need to understand the basic concepts of GPU, and data types to better understand why and when we need to use these techniques."
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#preliminary",
    "href": "posts/Blogs/Speed-up-training/post.html#preliminary",
    "title": "Speed Up Training for Neural Networks",
    "section": "1 Preliminary",
    "text": "1 Preliminary\n\n1.1 Data Representation\nIn deep learning, we often use different data types to represent our data and model parameters. The most common data types are:\n\nFloat32: also known as single-precision floating-point format. This is the default floating-point representation  used in most deep learning frameworks. It provides a good balance between precision and performance. It use 32 bits (4 bytes) to represent a number.\nFloat16: This is a half-precision floating-point representation that uses 16 bits instead of 32 bits. It can significantly speed up training and reduce memory usage, but it may lead to numerical instability in some cases.\nBFloat16: This is a truncated version of Float32 that retains the exponent bits but reduces the mantissa bits. It is designed to provide a good trade-off between precision and performance, especially for training large models.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The representation of Float32\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The representation of Float16\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) The representation of BFloat16\n\n\n\n\n\n\n\nFigure 1: The representation of float32, float16, and bfloat16 data types. The figure shows how the bits are allocated for the sign, exponent, and mantissa in each data type.\n\n\n\n\n\n\n\n\n\nHow those bits represent the number?\n\n\n\n\\[\n\\text{value} = (-1)^s \\times (1.f) \\times 2^{e - 127}\n\\]\nwhere:\n\n\\(s\\) is the sign bit (0 for positive, 1 for negative)\n\\(f\\) is the mantissa (the fractional part): \\(1.f = 1 + \\sum_{i=1}^{23} b_i \\cdot 2^{-i}\\), where \\(b_i\\) are the bits of the mantissa either 0 or 1.\n\\(e\\) is the exponent (an 8-bit unsigned int) with a bias of 127:\n\nFor Float32, \\(e\\) is 8 bits, which range from [1, 254]\nFor Float16, \\(e\\) is 5 bits, which range from [1, 30]\nFor BFloat16, \\(e\\) is 8 bits, which range from [1, 254]\n\n\n\n\nTo check the data type of a tensor and its properties in PyTorch, you can use the .dtype attribute. For example:\nx = torch.zeros(4, 8)\nx.dtype # check the data type of x\nx.numel() # check the number of elements in x\nx.element_size() # check the size of each element in bytes\nx.numel() * x.element_size() # check the total size in bytes\n\n\n1.2 Calculate Memory Usage of Model\nAssume we have a model with \\(N\\) parameters, and each parameter is represented by float32 (4 bytes). \\(A\\) is the number of activation elements stored during forward (depends on input and model depth). How can we calculate the memory need for training this model? Notice that the memory usage of a model is not only determined by the parameters, but also by:\n\nactivations\ngradients\noptimizer states\n\nFor a single parameter, the memory usage for one forward pass and backward pass is:\n\nparameter: 4 bytes (float32)\nactivation: 4 bytes (float32)\ngradient: 4 bytes (float32)\noptimizer state, which can vary depending on the optimizer used. For example, Adam optimizer requires 2 additional states (momentum and variance), each of which is 4 bytes (float32).\n\nSo, the total memory usage for one parameter is: \\[\n\\text{Memory per parameter} = 4 + 4  + 2 \\times 4 = 16 \\text{ bytes}\n\\]\nThus, the total memory usage for the model is: \\[\n\\text{Total Memory} = N \\times 16 \\text{ bytes} + A \\times 4 \\text{ bytes}\n\\]\n\n\n\n\n\n\nWhy need activation for backward pass?\n\n\n\nSay a layer denotes the input to the layer as \\(\\mathbf{x}\\), the weights as \\(\\theta\\), and the output as \\(\\mathbf{y}\\). The loss function is denoted as \\(L\\), which is a function of the output \\(\\mathbf{y}\\) and the target \\(t\\): \\[\n\\mathbf{y} = f(\\mathbf{x}; \\theta)\n\\] To compute the gradient of the loss with respect to the weights, we need to use the chain rule: \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\theta}\n\\]\nwhere \\(\\frac{\\partial \\mathbf{y}}{\\partial \\theta}\\) is the gradient of the output with respect to the weights \\(\\theta\\), which usually are the function of the input \\(\\mathbf{x}\\) and the weights \\(\\theta\\). To compute this gradient, we need to know the input \\(\\mathbf{x}\\). For example, the linear layer computes: \\[\n\\mathbf{y} = \\mathbf{x} \\cdot \\theta + b\n\\] to compute the gradient, we need to know the input \\(\\mathbf{x}\\), \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\mathbf{x}\n\\] where \\(\\mathbf{x}\\) is the input to the layer, which is the activation of the previous layer. Thus, we need to store the activation for the backward pass.\n\n\n\n\n1.3 Collective operations\nCollective operations are operations that involve multiple processes or devices, such as GPUs, to perform a computation. They are essential for parallelizing the training process across multiple GPUs. Some common collective operations include:\n\nBroadcast(Figure 2 (a)): This operation sends data from one process to all other processes. It is commonly used to share model parameters or hyperparameters across multiple GPUs.\nScatter(Figure 2 (b)): This operation distributes data from one process to multiple processes. It is often used to distribute input data across multiple GPUs.\nGather(Figure 2 (c)): This operation collects data from multiple processes and combines it into a single process. It is useful for aggregating results from multiple GPUs.\nReduce(Figure 2 (d)): This operation combines data from multiple processes into a single process. It is commonly used to compute the sum or maximum of gradients across multiple GPUs.\nAll-gather(Figure 2 (e)): This operation collects data from all processes and distributes it back to all processes. It is often used to gather gradients or model parameters from multiple GPUs without losing any information.\nAll-reduce(Figure 2 (g)): This operation combines data from all processes and distributes the result back to all processes. It is often used to average gradients across multiple GPUs during training.\nReduce-scatter(Figure 2 (f)): This operation combines data from multiple processes and distributes the result to each process. It is often used to reduce the amount of data that needs to be communicated between processes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Broadcast\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter\n\n\n\n\n\n\n\n\n\n\n\n(c) Gather\n\n\n\n\n\n\n\n\n\n\n\n(d) Reduce\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) All-gather\n\n\n\n\n\n\n\n\n\n\n\n(f) Reduce-scatter\n\n\n\n\n\n\n\n\n\n\n\n(g) All-reduce\n\n\n\n\n\n\n\nFigure 2: The illustration of different collective operations. The figure shows how data is communicated between processes in each operation.(Image take from: Stanford CS336)\n\n\n\n\nOne should take note is Reduce-Scatter combines two operations:\n\nReduce: Each process (or GPU) contributes its data, and a reduction operation (usually sum, mean, max, etc.) is applied across processes.\nScatter: The reduced result is partitioned and each process gets only a portion (its shard) of the reduced result.\n\n\n\n\n\n\n\nTip\n\n\n\nWay to remember the terminology:\n\nGather: collects data from multiple sources into one destination(not do any operation)\nReduce: performs some associative/commutative operation (sum, min, max)\nBroadcast/Scatter: is inverse of Gather\nAll: means destination is all devices"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#profiling",
    "href": "posts/Blogs/Speed-up-training/post.html#profiling",
    "title": "Speed Up Training for Neural Networks",
    "section": "2 Profiling",
    "text": "2 Profiling\nTo optimize the training process, we need to first profile our model to identify the bottlenecks. In this section, we will introduce several tools to profile the model and understand where the time is spent during training. We will discuss several tools that can help us profile our model and identify the bottlenecks in the training process:\n\nSimple Benchmarking (Section 2.1): The simplest way to measure the time taken for each operation in your model. You can use the time module in Python to measure the time taken for each operation. For example, you can wrap your forward pass in a timer to measure the time taken for each layer.\nPyTorch Profiler (Section 2.2): PyTorch provides a built-in profiler that can help you analyze the performance of your model. You can use the torch.profiler module to profile your model and visualize the results. The profiler provides detailed information about the time spent on each operation, memory usage, and more.\nNVIDIA Nsight Systems (Section 2.3): This is a powerful profiling tool that can help you analyze the performance of your model on NVIDIA GPUs. It provides detailed information about the GPU utilization, memory usage, and more. You can use it to identify bottlenecks in your model and optimize the performance.\n\n\n2.1 Simple Benchmarking\n\n\n2.2 PyTorch Profiler\n\n\n2.3 NVIDIA Nsight Systems"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#single-gpu-optimization",
    "href": "posts/Blogs/Speed-up-training/post.html#single-gpu-optimization",
    "title": "Speed Up Training for Neural Networks",
    "section": "3 Single GPU Optimization",
    "text": "3 Single GPU Optimization\nIn this section, we will discuss various techniques to optimize the training process on a single GPU. These techniques include: - Fusion(Section 3.1): This technique combines multiple operations into a single operation to reduce the number of kernel launches and improve performance. For example, you can fuse the forward and backward passes of a layer into a single operation. - Tiling(Section 3.2): This technique divides the input data into smaller tiles and processes them in parallel to improve memory access patterns and reduce memory usage. For example, you can tile the input data into smaller chunks and process them in parallel. - Memory Coalescing(Section 3.3): This technique optimizes memory access patterns to improve memory bandwidth utilization. For example, you can coalesce memory accesses to reduce the number of memory transactions and improve performance. - Mixed Precision Training(Section 3.4): This technique uses lower precision data types (such as float16 or bfloat16) to reduce memory usage and improve performance. It can significantly speed up training while maintaining model accuracy. PyTorch provides built-in support for mixed precision training through the torch.cuda.amp module, which allows you to automatically cast your model and inputs to lower precision during training. - Gradient Accumulation(Section 3.5): This technique accumulates gradients over multiple mini-batches before performing a weight update. It can help reduce the number of weight updates and improve training stability, especially when using large batch sizes. You can implement gradient accumulation by accumulating gradients in a buffer and updating the model parameters only after a certain number of mini-batches.\n\n3.1 Fusion\n\n\n3.2 Tiling\n\n\n3.3 Memory Coalescing\n\n\n3.4 Mixed Precision Training\n\n\n3.5 Gradient Accumulation\n\n\n3.6 Case Study: Flash Attention"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#multi-gpu-optimizationparallelism",
    "href": "posts/Blogs/Speed-up-training/post.html#multi-gpu-optimizationparallelism",
    "title": "Speed Up Training for Neural Networks",
    "section": "4 Multi-GPU Optimization(Parallelism)",
    "text": "4 Multi-GPU Optimization(Parallelism)\nIn this section, we will discuss various techniques to optimize the training process across multiple GPUs. These techniques include: - Data Parallelism(Section 4.1): This technique splits the input data across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different subset of the input data, and the gradients are averaged across GPUs before updating the model parameters. - Model Parallelism(Section 4.2): This technique splits the model across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the model, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large models that do not fit into a single GPU’s memory. - Pipeline Parallelism(Section 4.3): This technique splits the model into multiple stages and processes each stage in parallel across multiple GPUs. Each GPU processes a different stage of the model, and the output of one stage is passed to the next stage. This can help improve throughput and reduce memory usage, especially for large models. - Tensor Parallelism(Section 4.4): This technique splits the tensors across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the tensor, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large tensors that do not fit into a single GPU’s memory. - Context Parallelism(Section 4.5): This technique splits the context across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the context, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large contexts that do not fit into a single GPU’s memory.\n\n4.1 Data Parallelism\n\n\n4.2 Model Parallelism\n\n\n4.3 Pipeline Parallelism\n\n\n4.4 Tensor Parallelism\n\n\n4.5 Context Parallelism\n\n\n4.6 Case Study: DeepSpeed\n\n\n4.7 Case Study: Megatron-LM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nSelf CPU %\nSelf CPU\nCPU total %\nCPU total\nCPU time avg\nSelf CUDA\nSelf CUDA %\nCUDA total\nCUDA time avg\n# of Calls\n\n\n\n\naten::gelu\n6.76%\n669.785us\n13.48%\n1.336ms\n1.336ms\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\nvoid at::native::vectorized_elementwise_kernel&lt;…&gt;\n0.00%\n0.000us\n0.00%\n0.000us\n0.000us\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\ncudaLaunchKernel\n6.72%\n665.807us\n6.72%\n665.807us\n665.807us\n0.000us\n0.00%\n0.000us\n0.000us\n1\n\n\ncudaDeviceSynchronize\n86.52%\n8.574ms\n86.52%\n8.574ms\n4.287ms\n0.000us\n0.00%\n0.000us\n0.000us\n2\n\n\n\nSelf CPU time total: 9.909 ms\nSelf CUDA time total: 8.642 ms"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html",
    "href": "posts/Blogs/LLM-Alignment/post.html",
    "title": "LLM Part3: Alignment",
    "section": "",
    "text": "On this page\n   \n  \n  1 Supervised Fine-Tuning (SFT)\n  2 Review of Reinforcement Learning\n  3 RLHF\n  \n\nIn the part 1 and part 2 of the LLM series, we covered the architecture and inference techniques for LLMs. In this part 3, we will focus on alignment techniques, which are crucial for ensuring that LLMs behave in ways that are consistent with human values and intentions. We will explore various methods for aligning LLMs, including reinforcement learning from human feedback (RLHF), and discuss their implications for the development and deployment of these models. We will first explore the simple Supervised Fine-Tuning (SFT) approach, which involves fine-tuning LLMs on curated datasets that reflect human values and preferences. Than we will explore different RLHF techniques, which involve training LLMs using feedback from human evaluators to improve their alignment with human intentions. We will explore algorithms from PPO, DPO to GRPO and\n\n1 Supervised Fine-Tuning (SFT)\n\n\n2 Review of Reinforcement Learning\nIn this part, we will first review the basics of reinforcement learning, including key concepts such as rewards, policy, loss function, actor-critic methods, and more. This will provide a solid foundation for understanding the RLHF techniques we will explore later.\n\n\n3 RLHF\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html",
    "title": "Diffusion Model",
    "section": "",
    "text": "1 Prelimeany\n  \n  1.1 ELBO\n  1.2 DDPM\n  \n  1.2.1 Loss Function\n  \n  1.3 Time Embedding\n  1.4 Sampling\n  1.5 DDPM\n  1.6 DDIM\n  1.7 Conditioned Generation\n  \n  1.7.1 Classifier Generation\n  1.7.2 Classifier-Free Generation\n  \n  1.8 Speed Up Diffusion Models\n  \n  1.8.1 Consistency Models\n  1.8.2 Latent Variable Space\n  \n  1.9 Score Matching\n  \n  2 From ODE and SDE view point\n  \n  2.1 ODE vs. SDE\n  \n  2.1.1 Vector Field\n  \n  2.2 Mean Flow\n  \n  3 Model Architecture\n  \n  3.1 U-Net\n  3.2 Control Net\n  3.3 Diffusion Transformer (DiT)\n  \n  4 Case Study\n  \n  4.1 Imagen\n  \n  5 DALL·E\n  \n  5.1 Stable Diffusion\n  5.2 Meta Movie Gen Video\n  \n  6 Learning Resource\n  7 KAIST CS492 Diffusion Models and Their Applications"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#elbo",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#elbo",
    "title": "Diffusion Model",
    "section": "1.1 ELBO",
    "text": "1.1 ELBO\n\\[\n\\begin{align}\n\\log_{\\theta}P(\\mathrm{x})   \n&= \\log \\int P_{\\theta}(\\mathrm{x} | \\mathrm{z}) \\, d\\mathrm{z} \\\\\n&=  \\log \\int P_{\\theta}(\\mathrm{x, z}) \\frac{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\, dx   \\\\\n&=  \\log \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}} \\left[ \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right] \\\\\n&\\geq   \\boxed{\\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}} \\left[ \\log  \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right] } \\\\\n&=  \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}} \\left[  \\log\\frac{P_{\\theta}(\\mathrm{x} | \\mathrm{z}) P(\\mathrm{z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}  \\right]    \\\\\n& = \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}}[\\log P_{\\theta}(\\mathrm{x} | \\mathrm{z})] - D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z})]\n\\end{align}\n\\]\n\\[\n\\begin{align}\nD_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z} | \\mathrm{x})]   \n& = \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\left[ \\log \\frac{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}{P(\\mathrm{z} | \\mathrm{x})} \\right] \\\\\n& = \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}{P(\\mathrm{z} | \\mathrm{x})} \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P(\\mathrm{z} | \\mathrm{x})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P(\\mathrm{z} | \\mathrm{x}) P_{\\theta}(\\mathrm{x})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) P_{\\theta}(\\mathrm{x})}  \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) P_{\\theta}(\\mathrm{x})}  \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}  \\, d\\mathrm{z}   + \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log P_{\\theta}(\\mathrm{x})  \\, d\\mathrm{z}  \\\\\n& = - \\boxed{\\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right]}  + \\log P_{\\theta}(\\mathrm{x})\n\\end{align}\n\\]\nThat lead to: \\[\n\\log P_{\\theta}(\\mathrm{x}) = \\underbrace{ \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right]  }_{ ELBO }+ D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z} | \\mathrm{x})]    \n\\] We use \\(Q_{\\phi}(\\mathrm{z} | \\mathrm{x})\\) to approximate the true posterior distribution \\(P(\\mathrm{z} | \\mathrm{x})\\). \\[\nEBLO =  \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right]  =  \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}}[\\log P_{\\theta}(\\mathrm{x} | \\mathrm{z})] - D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z})]\n\\]"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#ddpm",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#ddpm",
    "title": "Diffusion Model",
    "section": "1.2 DDPM",
    "text": "1.2 DDPM\nWe will derive three predictor: - Image predictor \\(\\hat{\\mathrm{x}}\\) - Mean Predictor \\(\\hat{\\mu}\\) - Noise Predictor \\(\\hat{\\epsilon}\\) To solve the DDPM problem, it is ok if you don’t understand.\n\n1.2.1 Loss Function\n\\[\n\\begin{align}\n\\log P_{\\theta}(\\mathrm{x}) & =  ELBO + D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z} | \\mathrm{x})]     \\\\\n& \\geq   \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right]    \\\\\n& = \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x}_{0}, \\mathrm{x}_{1:T})}{Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\right]  \n\\end{align}\n\\]\nOne thing good about DDPM is that, we know what \\(Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})\\) exactly: \\[\nQ_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0}) = \\prod_{t=1}^{T}P(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})\n\\]\n\\[\n\\begin{align}\nEBLO   \n& = \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x}_{0}, \\mathrm{x}_{1:T})}{Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\right]   \\\\\n& =  \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) \\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})P(\\mathrm{x}_{T})}\n{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})\\prod_{t=2}^{T}Q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})}  \\right] \\\\\n& = \\mathbb{E}_{ Q(\\mathrm{x}_{T} | \\mathrm{x}_{0})} [\\log  P(\\mathrm{x}_{T})] +  \\mathbb{E}_{ Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\frac{\\prod_{t=2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}{\\prod_{t=2}^{T}Q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})} \\right] + \\mathbb{E}_{ Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x}_{1})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}  \\right] \\\\\n& = \\mathbb{E}_{ Q(\\mathrm{x}_{T} | \\mathrm{x}_{0})} [\\log  P(\\mathrm{x}_{T})]  \n+ \\log  \\sum_{t=2}^{T}\\mathbb{E}_{ Q(\\mathrm{x}_{t-1}, \\mathrm{x_{t}} | \\mathrm{x}_{0})}\\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})}{Q(\\mathrm{x}_{t} | \\mathrm{x}_{t-1})}  \\right]\n+ \\mathbb{E}_{ Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x}_{1})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}  \\right]\n\\end{align}\n\\]\nAs we can see, to calculate the second term, we need to sample from two random distribution, to get \\(\\mathrm{x_{t}}, \\mathrm{x_{t-1}}\\). This will create very noisy estimate with high variance. So, we need to re-write the ELBO, to make it better low variance.\n\\[\n\\begin{align}\nELBO &=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})} \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]    \\\\\n&=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t} | \\mathrm{x}_{t - 1}, \\mathrm{x}_{0}})} \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]  \\\\\n&=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})} \\frac{Q(\\mathrm{x}_{t-1} | \\mathrm{x}_{0})}{Q(\\mathrm{x}_{t}|\\mathrm{x}_{0})}\\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]  \\\\\n&=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})} \\frac{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})}\\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]  \\\\\n&=  \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( \\frac{P(\\mathrm{x}_{T})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})} \\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})}P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) \\right)  \\right]   \\\\\n&=  \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( \\frac{P(\\mathrm{x}_{T})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})} \\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})}P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) \\right)  \\right]   \\\\\n& = \\mathbb{E}_{\\mathrm{x}_{T} \\sim Q(\\mathrm{x}_{T} | \\mathrm{x}_{0})} \\left[\\log \\frac{P(\\mathrm{x}_{T})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})} \\right] + \\sum_{t=2}^{T} \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} \\left[\\log \\frac{P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})} \\right]\n+ \\mathbb{E}_{\\mathrm{x}_{1} \\sim Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}[P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) ]  \\\\\n& = -D_{KL}[Q(\\mathrm{x}_{T} | \\mathrm{x}_{0}) \\| P(\\mathrm{x}_{T})]  \\\\ &\n\\quad - \\sum_{t=2}^{T}\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ]\\\\&\n\\quad +  \\mathbb{E}_{\\mathrm{x}_{1} \\sim Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}[P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) ]\n\\end{align}\n\\]\nThe first term is the prior matching term, which is the constant, no need to optimize. The third term is the reconstruction term, which is the The second term is the consistent term, which the KL Divergence between two gaussian distribution, which has close form: \\[\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ] = \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} | \\mathrm{x}_{0})} \\left [\\frac{1}{2\\tilde{\\sigma}_{t}^{2}}\\| \\mu_{\\theta}(\\mathrm{x}_{t}, t)  - \\tilde{\\mu}(\\mathrm{x}_{t}, \\mathrm{x}_{0})\\|^{2} \\right]\n\\]\nThis is called the mean predictor. What if we want to precict get the \\(\\mathrm{x}_{0}\\). \\[\n\\begin{align}\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ] &= \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} | \\mathrm{x}_{0})} \\left [\\frac{1}{2\\tilde{\\sigma}_{t}^{2}}\\| \\mu_{\\theta}(\\mathrm{x}_{t}, t)  - \\tilde{\\mu}(\\mathrm{x}_{t}, \\mathrm{x}_{0})\\|^{2} \\right] \\\\\n& = \\frac{1}{2 \\tilde{\\sigma}_t^2} \\cdot \n\\frac{\\bar{\\alpha}_{t-1} \\beta_t^2}{(1-\\bar{\\alpha}_t)^2} \n\\mathbb{E}_{\\mathrm{x}_{t}\\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)} \n\\left[ \\| \\hat{x}_\\theta(x_t, t) - x_0 \\|^2 \\right] \\\\\n& =\\omega_t\n\\mathbb{E}_{\\mathrm{x}_{t}\\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)} \n\\left[ \\| \\hat{\\mathrm{x}}_\\theta(\\mathrm{x}_t, t) - \\mathrm{x}_0 \\|^2 \\right]\n\\end{align}\n\\]\nThis is called the \\(\\mathrm{x}_{0}\\) predictor, finally, we can get the \\(\\varepsilon_t\\)-predictor: \\[\n\\begin{align}\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ] &= \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} | \\mathrm{x}_{0})} \\left [\\frac{1}{2\\tilde{\\sigma}_{t}^{2}}\\| \\mu_{\\theta}(\\mathrm{x}_{t}, t)  - \\tilde{\\mu}(\\mathrm{x}_{t}, \\mathrm{x}_{0})\\|^{2} \\right] \\\\\n& = \\frac{1}{2 \\tilde{\\sigma}_t^2}\n\\cdot\n\\frac{(1-\\bar{\\alpha}_t)^2}{\\bar{\\alpha}_t (1-\\bar{\\alpha}_t)}\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)}\n\\left[ \\| \\hat{\\varepsilon}_\\theta(x_t, t) - \\varepsilon_t \\|^2 \\right]\n\\\\\n& =\\omega_{t}'\n\\mathbb{E}_{\\mathrm{x}_{t}\\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)} \n\\left[ \\| \\hat{\\varepsilon}_\\theta(x_t, t) - \\varepsilon_t \\|^2 \\right]\n\\end{align}\n\\]\nThis is the noise predictor.\n\n\n\nIn practice, we can simply drop the weight term in training: and use noise predictor\nIn this blog, we will first introduce what is the diffusion models, than we will introduce how to implement the DDPM from scratch using PyTorch. After that, we will explore the flow matching and score matching model through the ODE/SDE. By the end of the blog, I believe you will gain a comprehensive understanding of the diffusion model, and SOTA generative models.\nThe central idea of DDPM is take each training image and to corrupt it using a multi-step noise process to transform it into a sample from a Gaussian distribution. Than, a neural network, known as denoiser \\(\\epsilon_{\\theta}\\), is trained to inver this process. Once the denoiser is trained, it can than generate new images starting with samples from Gaussian.\n\nDiffusion Model\n\n\nForward Diffusion Process: \\[\nq(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1}) =\\mathcal{N}(\n\\mathrm{x}_{t};\n\\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t}, \\beta_{t}\\mathbf{I}\n)\n\\]\n\\[\n\\mathrm{x}_{t} =  \\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t} + \\beta_{t}\\epsilon_{t}, \\quad \\text{where} \\ \\epsilon_{t} \\sim \\mathcal{N}(0, \\mathbf{I}_{})\n\\]\n\\[\n\\mathrm{x}_{t} = \\sqrt{ \\bar{\\alpha}_{t} }\\mathrm{x_{0}} +  \\sqrt{ 1 - \\bar{\\alpha}_{t} }\\epsilon\n\\]\n Langevin dynamics: $$ t = {t-1} + {} p({t-1}) + ,_t, _t (0, )\n$$\nBackward Diffusion Process: $$ \\[\\begin{align}\n& p_{\\theta}(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod_{t=1}^T p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)  \\\\\n\n&  p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}\\!\\left(\\mathbf{x}_{t-1}; \\mu_{\\theta}(\\mathbf{x}_t, t), \\Sigma_{\\theta}(\\mathbf{x}_t, t)\\right)\n\n\\end{align}\\] $$\nThe above content is intractable, one thing to notice that is is tractable when we conditioned on the \\(\\mathrm{x}_{0}\\) \\[\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\n= \\mathcal{N}\\!\\left(\\mathbf{x}_{t-1};\n\\textcolor{blue}{\\tilde{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0)}, \\,\n\\textcolor{red}{\\tilde{\\beta}_t \\mathbf{I}}\\right)\n\\] where : \\[\n\\begin{align}\n\\tilde{\\mu}_t\n& = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}\\mathbf{x}_t\n+ \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t}\n   \\frac{1}{\\sqrt{\\alpha_t}} \\left(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t}\\,\\epsilon_t\\right) \\\\\n& = \\textcolor{cyan}{\\frac{1}{\\sqrt{\\alpha_t}}\n   \\left(\\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\,\\epsilon_t\\right)}\n\\end{align}\n\\]\nSo, the loss function become: $$\n\\[\\begin{align}\n\\mathcal{L}_t^{\\text{simple}}\n& = \\mathbb{E}_{t \\sim [1,T], \\mathbf{x}_0, \\epsilon_t}\n  \\left[ \\left\\| \\epsilon_t - \\epsilon_\\theta(\\mathbf{x}_t, t) \\right\\|^2 \\right] \\\\\n& = \\mathbb{E}_{t \\sim [1,T], \\mathbf{x}_0, \\epsilon_t}\n  \\left[ \\left\\| \\epsilon_t - \\epsilon_\\theta\\!\\left(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0\n  + \\sqrt{1 - \\bar{\\alpha}_t}\\,\\epsilon_t,\\, t \\right) \\right\\|^2 \\right]\n\\end{align}\\] $$\nand the loss is: \\[\n\\mathcal{L} = \\mathcal{L}_{t} + C\n\\] where \\(C\\) is some constant not depend on \\(\\theta\\)"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#time-embedding",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#time-embedding",
    "title": "Diffusion Model",
    "section": "1.3 Time Embedding",
    "text": "1.3 Time Embedding\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#sampling",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#sampling",
    "title": "Diffusion Model",
    "section": "1.4 Sampling",
    "text": "1.4 Sampling\nAfter training a noise denoiser, we can sample from the \\(p_{\\text{init}}\\), and convert it to the \\(p_{\\text{data}}\\). There are several approaches"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#ddpm-1",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#ddpm-1",
    "title": "Diffusion Model",
    "section": "1.5 DDPM",
    "text": "1.5 DDPM\nThis are random samples."
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#ddim",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#ddim",
    "title": "Diffusion Model",
    "section": "1.6 DDIM",
    "text": "1.6 DDIM\nDDIM is determinstic"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#conditioned-generation",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#conditioned-generation",
    "title": "Diffusion Model",
    "section": "1.7 Conditioned Generation",
    "text": "1.7 Conditioned Generation\nSo far in the DDPM model, the image generated is un-conditioned. How can we generated content from some condition \\(y\\) such as some prompts\n\n1.7.1 Classifier Generation\n\n\n1.7.2 Classifier-Free Generation"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#speed-up-diffusion-models",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#speed-up-diffusion-models",
    "title": "Diffusion Model",
    "section": "1.8 Speed Up Diffusion Models",
    "text": "1.8 Speed Up Diffusion Models\n\n1.8.1 Consistency Models\n\n\n1.8.2 Latent Variable Space\nVariance Autoencoder"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#score-matching",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#score-matching",
    "title": "Diffusion Model",
    "section": "1.9 Score Matching",
    "text": "1.9 Score Matching\n\\[\n\\nabla_{x_t} \\log q(x_t|x_0)\n= \\nabla_x \\left( - \\frac{\\| x_t - \\sqrt{\\bar{\\alpha}_t} x_0 \\|^2}{2(1-\\bar{\\alpha}_t)} \\right)\n= - \\frac{x_t - \\sqrt{\\bar{\\alpha}_t} x_0}{1-\\bar{\\alpha}_t}\n\\]\n$$ _{x_t} q(x_t|x_0) = - = - \n$$\nSo, can be interpreted as predicting the score \\(\\nabla_{x_t} \\log q(x_t|x_0)\\) up to a scaling factor \\(- \\frac{1}{\\sqrt{1-\\bar{\\alpha}_t}}\\)\nAccording to the Tweedie’s formula, we have: \\[\n\\nabla_{x_t} \\log q(x_t)\n= - \\frac{x_t - \\sqrt{\\bar{\\alpha}_t}\\,\\mathbb{E}[x_0 \\mid x_t]}{1-\\bar{\\alpha}_t}\n\\]\n \n\nSo, this is the Noise-Conditional Score-Based Models \n\nSo, the solution is the Annealed Langevin Dynamics  At the beginning (when \\(\\sigma_{t}\\) is large), As time progresses (and \\(\\sigma_{t}\\) decreases),"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#ode-vs.-sde",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#ode-vs.-sde",
    "title": "Diffusion Model",
    "section": "2.1 ODE vs. SDE",
    "text": "2.1 ODE vs. SDE\nBefore talk about the ODE and SDE, let’s first understand some concepts to solid our understanding.\n\n2.1.1 Vector Field\nVector Field is a function that assign a vector to every point in space. For example: imagine a weather map: at each location, an arrow shows the wind’s direction and strength. That arrow map is a vector field.\n\n\\[\nF: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}\n\\]\nAnd every ODE \\(u\\) is defined by a vector field: \\[\nu: \\mathbb{R}^{d} \\times [0, 1] \\to \\mathbb{R}^{d}, \\quad (x, t) \\to u_{t}(x)\n\\] that for every time \\(t\\) and location \\(\\mathrm{x}\\), we get a vector \\(u_{t}(\\mathrm{x})  \\in \\mathbb{R}^{d}\\) that point to some direction.\n\nWhy we need time \\(t\\)? Because for every location \\(\\mathrm{x}\\), we might arrive same location at different time, due to the random start point \\(\\mathrm{x}_{0}\\)\n\\[\n\\begin{align}\n\\frac{d}{dt}\\mathrm{x}_{t } &= u_{t}(\\mathrm{x}_{t}) \\\\\n\\mathrm{x_{0}}&=x_{0}\n\\end{align}\n\\]\nSo, another question we want to ask it: when we start at \\(x_{0}\\), where are we at \\(t\\). This can be solved by flow, which is a solution to the ODE:\n$$\n\\[\\begin{align}\n\\psi : \\mathbb{R}^d \\times [0,1] \\mapsto \\mathbb{R}^d &,\n\\quad (x_0, t) \\mapsto \\psi_t(x_0) \\\\\n\n\\frac{d}{dt} \\psi_t(x_0) & = u_t(\\psi_t(x_0)) \\\\\n\n\\psi_0(x_0)& = x_0\\\\\n\\end{align}\\] $$\n\\[\n\\mathrm{x}_{1} \\sim  p_{\\text{data}}  \n\\] However, we can not solve the problem. But we can use the numerical analysis. One of the simplest and intuitive methods is Euler method:\n\\[\n\\mathrm{x}_{t + h} = \\mathrm{x}_{t} +  h u_{t}(\\mathrm{x}_{t}) \\quad (t = 0, h, 2h, 3h, \\dots,  1- h)\n\\]\nStochastic Differential Equations extend the ODEs with stochastic(random) trajectories, which is also known as stochastic process. The stochastic is add through a Brownian motion. A Brownain motion \\(W = (W_{t})_{0\\leq t \\leq 1}\\) is a stochastic process such that: \\(W_{0} = 0\\): - Normal Increments: \\(W_{t} - W_{s} \\sim \\mathcal{N}(0, (t - s)\\mathbf{I}_{d})\\) for all \\(0 \\leq s \\leq t\\) - Independent Increments\nBrownian Motion is also known as Wiener Process: \\[\nW_{t + h} = W_{t} + \\sqrt{ h }\\epsilon_{t}, \\quad \\text{where} \\ \\epsilon_{t} \\sim \\mathcal{N}(0, \\mathbf{I}_{d})\n\\]\nOrnstein-Unlenbeck(OU) process\nEuler-Maruyama Method is a numerical method."
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#mean-flow",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#mean-flow",
    "title": "Diffusion Model",
    "section": "2.2 Mean Flow",
    "text": "2.2 Mean Flow\nMean Flows for One-step Generative Modeling\nMMDiT"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#u-net",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#u-net",
    "title": "Diffusion Model",
    "section": "3.1 U-Net",
    "text": "3.1 U-Net\nU-Net: Convolutional Networks for Biomedical Image Segmentation"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#control-net",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#control-net",
    "title": "Diffusion Model",
    "section": "3.2 Control Net",
    "text": "3.2 Control Net\nAdding Conditional Control to Text-to-Image Diffusion Models"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#diffusion-transformer-dit",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#diffusion-transformer-dit",
    "title": "Diffusion Model",
    "section": "3.3 Diffusion Transformer (DiT)",
    "text": "3.3 Diffusion Transformer (DiT)"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#imagen",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#imagen",
    "title": "Diffusion Model",
    "section": "4.1 Imagen",
    "text": "4.1 Imagen\nPhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#stable-diffusion",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#stable-diffusion",
    "title": "Diffusion Model",
    "section": "5.1 Stable Diffusion",
    "text": "5.1 Stable Diffusion"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/Diffusion Model.html#meta-movie-gen-video",
    "href": "posts/Blogs/DiffusionModels/Diffusion Model.html#meta-movie-gen-video",
    "title": "Diffusion Model",
    "section": "5.2 Meta Movie Gen Video",
    "text": "5.2 Meta Movie Gen Video\nMovie Gen: A Cast of Media Foundation Models\nRectified Flow: Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow https://arxiv.org/pdf/2209.03003\nMean Flow Mean Flows for One-step Generative Modeling https://arxiv.org/pdf/2505.13447"
  },
  {
    "objectID": "posts/notes.html",
    "href": "posts/notes.html",
    "title": "Learning Notes",
    "section": "",
    "text": "Note\n\n\n\n\nI have moved all my learning notes to this website. You can find them at Course Notes.\n\n\n\n\n\n\nAbout Page of the Course notes\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Projects/PaliGemma/pali-gemma.html",
    "href": "posts/Projects/PaliGemma/pali-gemma.html",
    "title": "PaliGemma Inference and Fine Tuning",
    "section": "",
    "text": "Here is the full process of the Pali-Gemma\n\n\n\n\n\n\n\nFigure 1: The Pali-Gemma model is a multi-modal large language model that integrates vision and language tasks. The full process includes data collection, model training, and fine-tuning for specific applications.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/blogs.html",
    "href": "posts/blogs.html",
    "title": "👋🏻Welcome to Yuyang’s Blog",
    "section": "",
    "text": "Diffusion Model\n\n\n\nDecisionMaking\n\nNN-Training-Tricks\n\n\n\nThis is a decition mode ls\n\n\n\n\n\nYuyang Zhang\n\n12 min\n\n2,287 words\n\n\n2025-09-21\n\n\n\n\n\n\n\nLLM Part1: Architecture\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn this blog, we will going through different transformer architectures that power LLMs. Such as position encoding, attention mechanisms, and more. In the end, we will explore some SOTA LLM architectures, and study how to they combine those techniques to achieve better performance.\n\n\n\n\n\nYuyang Zhang\n\n28 min\n\n5,428 words\n\n\n2025-09-22\n\n\n\n\n\n\n\nLLM Part3: Alignment\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn this blog, we will going through different alignment techniques for LLMs, including how to effectively align them with human values and intentions. We will explore techniques such as reinforcement learning from human feedback (RLHF), and more. By the end, you will have a solid understanding of how to align LLMs for your own applications.\n\n\n\n\n\nYuyang Zhang\n\n1 min\n\n193 words\n\n\n2025-09-21\n\n\n\n\n\n\n\nLLM Part2: Inference\n\n\n\nLarge Language Model\n\nInference\n\n\n\nIn this blog, we will going through the inference process of LLMs, including how to effectively use them for various tasks. We will explore techniques such as prompt engineering, few-shot learning, and more. By the end, you will have a solid understanding of how to leverage LLMs for your own applications.\n\n\n\n\n\nYuyang Zhang\n\n1 min\n\n88 words\n\n\n2025-09-21\n\n\n\n\n\n\n\nSpeed Up Training for Neural Networks\n\n\n\nTraining Tricks\n\n\n\nTraining large neural networks can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculating in the single GPUs, and parallelizing the training across multiple GPUs.\n\n\n\n\n\nYuyang Zhang\n\n14 min\n\n2,648 words\n\n\n2025-09-21\n\n\n\n\n\nNo matching items\n Back to top"
  }
]