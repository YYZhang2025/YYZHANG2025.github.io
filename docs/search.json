[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Yuyang",
    "section": "",
    "text": "I am a recent graduate from Singapore Management University in MITB (AI Track). With a strong foundation in computer science and big data, I have hands-on experience in machine learning and deep learning, and I‚Äôm passionate about building impactful AI-driven solutions. I am currently seeking full-time opportunities in the AI field where I can contribute, grow, and make a meaningful impact. Open to Work ‚Äì feel free to reach out at zhangyuyang1211@gmail.com!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Yuyang",
    "section": "1 Education",
    "text": "1 Education\n\n  \n    \n    \n      2023 - 2025\n      Master of IT in Business\n      Singapore Management University\n    \n  \n  \n    \n    \n      2020 - 2023\n      Bachelor of Computer Science\n      University of Wollongong"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About Yuyang",
    "section": "2 Work Experience",
    "text": "2 Work Experience\n\n  \n    \n    \n      March 2025  - Present\n      Research Assistant Intern\n      SMART"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "About Yuyang",
    "section": "3 Projects",
    "text": "3 Projects\n\n  \n    \n    \n       File-Tuning Large Visual Language Model\n          [GitHub]\n          [Blog]\n      \n        Wrting the"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "About Yuyang",
    "section": "4 Technical Skills",
    "text": "4 Technical Skills\n\n  \n    Python Development\n    Algorithm Design\n    Deep Learning\n    Machine Learning\n    Computer Vision\n    Natural Language Processing\n    Deep Reinforcement Learning\n    Graph Neural Networks\n    Large Language Model\n    Model Compression\n    Convex Optimization\n    Probabilistic Graph Model\n    Meta Learning\n    Deep Generative Model"
  },
  {
    "objectID": "posts/projects.html",
    "href": "posts/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Visual Language Model\n\n\n\nLarge Language Model\n\nMulti-Modality\n\n\n\nIn this project, I will implement a Visual Language Model (VLM) using ‚Äòpure‚Äô PyTorch, which is a model that can understand and generate text based on visual inputs.\n\n\n\nYuyang Zhang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/notes.html",
    "href": "posts/notes.html",
    "title": "Learning Notes",
    "section": "",
    "text": "Note\n\n\n\n\nI have moved all my learning notes to this website. You can find them at Course Notes.\n\n\n\n\n\n\nAbout Page of the Course notes"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html",
    "title": "LLM Part1: Pre Training",
    "section": "",
    "text": "This is the first part of the LLM series, Pre-training, the Pre-training the most time consuming part of the LLM. In the nutshell, it just the next-word-prediction task. That‚Äôs it. Not anything fancy. However, what make the LLM hard to train and only achieve in recent years because it is LARGE. People need to develop and invent some efficient training algorithm to speed up the training process, make it efficient to training and deploying. In the article, I am going to take the path going through the training LLM from scratch, make the LLM easy to understand."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#word-level",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#word-level",
    "title": "LLM Part1: Pre Training",
    "section": "2.1 Word Level",
    "text": "2.1 Word Level"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#byte-pair-encoding",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#byte-pair-encoding",
    "title": "LLM Part1: Pre Training",
    "section": "2.2 Byte-Pair-Encoding",
    "text": "2.2 Byte-Pair-Encoding"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#bit-encoding",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#bit-encoding",
    "title": "LLM Part1: Pre Training",
    "section": "2.3 Bit Encoding",
    "text": "2.3 Bit Encoding"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#embedding",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#embedding",
    "title": "LLM Part1: Pre Training",
    "section": "2.4 Embedding",
    "text": "2.4 Embedding\nEmbedding is"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#position-encoding",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#position-encoding",
    "title": "LLM Part1: Pre Training",
    "section": "3.1 Position Encoding",
    "text": "3.1 Position Encoding"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#normalization",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#normalization",
    "title": "LLM Part1: Pre Training",
    "section": "3.2 Normalization",
    "text": "3.2 Normalization"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#layer-normalization",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#layer-normalization",
    "title": "LLM Part1: Pre Training",
    "section": "3.3 Layer Normalization",
    "text": "3.3 Layer Normalization"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#rms-normalization",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#rms-normalization",
    "title": "LLM Part1: Pre Training",
    "section": "3.4 RMS Normalization",
    "text": "3.4 RMS Normalization\n\n3.4.1 Post-Norm vs.¬†Pre-Norm"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#without-normalization",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#without-normalization",
    "title": "LLM Part1: Pre Training",
    "section": "3.5 Without Normalization",
    "text": "3.5 Without Normalization\nRecently, (zhuTransformersNormalization2025?) proposed that we can remove the normalization without harm the performance of the neural network. It replace the normalization layer with a scaled tanh function, named Dynamic Tanh, defined as:\n\\[\n\\text{DyT}(x) = \\gamma * \\tanh(\\alpha x) + \\beta\n\\]\n\n\n\n\n\n\nFigure¬†1: Block with Dynamic Tanh(DyT) (Image Source: Transformers without Normalization)\n\n\n\nIt adjust the input activation range via a learnable scaling factor \\(\\alpha\\) and then squashed the extreme values through an S-shaped tanh function."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#attention-mechnism",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#attention-mechnism",
    "title": "LLM Part1: Pre Training",
    "section": "3.6 Attention Mechnism",
    "text": "3.6 Attention Mechnism"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#feed-forward-network",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#feed-forward-network",
    "title": "LLM Part1: Pre Training",
    "section": "3.7 Feed Forward Network",
    "text": "3.7 Feed Forward Network\n\n3.7.1 Mixture of Expert"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#cross-entropy-loss",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#cross-entropy-loss",
    "title": "LLM Part1: Pre Training",
    "section": "4.1 Cross Entropy Loss",
    "text": "4.1 Cross Entropy Loss"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#model-initilization",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#model-initilization",
    "title": "LLM Part1: Pre Training",
    "section": "5.1 Model Initilization",
    "text": "5.1 Model Initilization"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#optimizer",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#optimizer",
    "title": "LLM Part1: Pre Training",
    "section": "5.2 Optimizer",
    "text": "5.2 Optimizer"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#about-gradients",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#about-gradients",
    "title": "LLM Part1: Pre Training",
    "section": "5.3 About Gradients",
    "text": "5.3 About Gradients\n\n5.3.1 Gradient Accumulations\n\n\n5.3.2 Gradiant Clipping"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#mixed-precision",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#mixed-precision",
    "title": "LLM Part1: Pre Training",
    "section": "5.4 Mixed Precision",
    "text": "5.4 Mixed Precision"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#parallellism",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#parallellism",
    "title": "LLM Part1: Pre Training",
    "section": "5.5 Parallellism",
    "text": "5.5 Parallellism"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#supervised-fine-tuning",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#supervised-fine-tuning",
    "title": "LLM Part1: Pre Training",
    "section": "6.1 Supervised-Fine-Tuning",
    "text": "6.1 Supervised-Fine-Tuning"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#reinforcement-learning-from-human-feedback",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#reinforcement-learning-from-human-feedback",
    "title": "LLM Part1: Pre Training",
    "section": "6.2 Reinforcement Learning from Human Feedback",
    "text": "6.2 Reinforcement Learning from Human Feedback\n\n6.2.1 PPO\n\n\n6.2.2 DPO\n\n\n6.2.3 GRPO"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#quantization",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#quantization",
    "title": "LLM Part1: Pre Training",
    "section": "7.1 Quantization",
    "text": "7.1 Quantization"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#knowledge-distillation",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#knowledge-distillation",
    "title": "LLM Part1: Pre Training",
    "section": "7.2 Knowledge Distillation",
    "text": "7.2 Knowledge Distillation"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#prompt-enginnering",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#prompt-enginnering",
    "title": "LLM Part1: Pre Training",
    "section": "9.1 Prompt Enginnering",
    "text": "9.1 Prompt Enginnering"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#prefix-tuning",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#prefix-tuning",
    "title": "LLM Part1: Pre Training",
    "section": "9.2 Prefix-Tuning",
    "text": "9.2 Prefix-Tuning"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#adapter",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#adapter",
    "title": "LLM Part1: Pre Training",
    "section": "9.3 Adapter",
    "text": "9.3 Adapter\n\n9.3.1 LoRA\n\n\n9.3.2 Q-LoRA"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#chatbot",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#chatbot",
    "title": "LLM Part1: Pre Training",
    "section": "11.1 ChatBot",
    "text": "11.1 ChatBot\nMost know ChatGPT,"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#ai-agent",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#ai-agent",
    "title": "LLM Part1: Pre Training",
    "section": "11.2 AI Agent",
    "text": "11.2 AI Agent"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-PreTraining.html#llm-as-optimizer",
    "href": "posts/Blogs/LLM/LLM-PreTraining.html#llm-as-optimizer",
    "title": "LLM Part1: Pre Training",
    "section": "11.3 LLM as Optimizer",
    "text": "11.3 LLM as Optimizer"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Applications.html",
    "href": "posts/Blogs/LLM/LLM-Applications.html",
    "title": "LLM Part3: Applications",
    "section": "",
    "text": "This is the part 3 of the LLM series, application. What can we do after we have an LLM. One of the most significant application is the LLM Based Agent. With the ability of the LLM. The AI agent has acquire incredible ability. Besides the AI Agent, it has so many other applications that can be improve with the help of the LLM. We are going to dig into it."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Overview.html",
    "href": "posts/Blogs/LLM/LLM-Overview.html",
    "title": "LLM Overview",
    "section": "",
    "text": "A language model is nothing model that outputs a probability distribution over the next token in a sequence given the previous tokens in the sequence, mathematically, it denoted as:\n\\[\nP_{\\theta}(x_t | x_{1:t-1})\n\\tag{1}\\]\nNowaday, when we say a LLM, we usually mean a decoder model, which mean predict the next token based on the previous tokens. However previous, there is another type of the model, not only depends on the previous tokens, but also depends on other conditions, such as, other language for the machine translation task, which denoted as:\n\\[\nP_{\\theta}(x_t | x_{1:t-1}, z)\n\\tag{2}\\]\nIn the blog, we mainly focus on the first type, which has the form of Equation¬†1. The second type is not the focus of this blog, but we might mention it in the future blog. There two main steps of the training an LLM, the pre-training and fine-tuning. The pre-training is the most time consuming part of the LLM. In just the next-word-prediction task. After we have the pre-trained model, the model can only generate the next word based on the previous words. So, we need to fine-tune the model to make it output the desired output. This the job of the post-training. In the post-training, there are two main tasks, the supervised fine-tuning(SFT) and reinforcement learning from human feedback(RLHF). The SFT is the supervised fine-tuning, which is the most common way to fine-tune the model. The RLHF is a new way to fine-tune the model, which is more efficient than the SFT."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Overview.html#supervised-fine-tuning-sft",
    "href": "posts/Blogs/LLM/LLM-Overview.html#supervised-fine-tuning-sft",
    "title": "LLM Overview",
    "section": "4.1 Supervised Fine Tuning (SFT)",
    "text": "4.1 Supervised Fine Tuning (SFT)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Overview.html#reinforcement-learning-from-human-feedback-rlhf",
    "href": "posts/Blogs/LLM/LLM-Overview.html#reinforcement-learning-from-human-feedback-rlhf",
    "title": "LLM Overview",
    "section": "4.2 Reinforcement Learning from Human Feedback (RLHF)",
    "text": "4.2 Reinforcement Learning from Human Feedback (RLHF)"
  },
  {
    "objectID": "posts/Blogs/LLM/RAG.html",
    "href": "posts/Blogs/LLM/RAG.html",
    "title": "LLM: RAG",
    "section": "",
    "text": "Retrieval-Augmented Generation (RAG) is a technique used to equip the LLM the ability to use the updated news. It has here are several, it often use vector DB to store the LLM.\nThis is good for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. It retrieving relevant document chunks from external knowledge base through semantic similarity calculation. RAG effectively reduces the problem of generating factually incorrect content.\nThe In-Context Learning(ICL) abilities first display in the GPT3, which enable RAG to answer more complex and knowledge intensive tasks during the inference stages. Unlike previously, which used the knowledge from RAG to fine-tuning model. Later, the enhancement of RAG began to incorporate more with LLM fine-tuning techniques.\nAs name indicated, there are several techniques used in the ‚ÄúRetrieval‚Äù, ‚ÄòGeneration‚Äô and ‚ÄúAugmentation‚Äù.\nThere are three main parts of the RAG:"
  },
  {
    "objectID": "posts/Blogs/LLM/RAG.html#pre-retrieval-process",
    "href": "posts/Blogs/LLM/RAG.html#pre-retrieval-process",
    "title": "LLM: RAG",
    "section": "2.1 Pre-Retrieval Process",
    "text": "2.1 Pre-Retrieval Process\nThe primary focus is on optimizing the indexing structure and the original query.\n\nOptimizing Indexing: enhance the quality of the content being indexed:\n\nEnhancing data granularity\nOptimizing index structures,\nAdding Meta-data\nAlignment optimization\nMixed retrieval\n\nOptimizing Query: make the user‚Äôs original question clearer and more suitable for the retrieval task, through:\n\nQuery rewritting\nQuery Transformation\nQuery Expansion"
  },
  {
    "objectID": "posts/Blogs/LLM/RAG.html#post-retrieval-process",
    "href": "posts/Blogs/LLM/RAG.html#post-retrieval-process",
    "title": "LLM: RAG",
    "section": "2.2 Post-Retrieval Process",
    "text": "2.2 Post-Retrieval Process\nOnce relevant context is retrieved, it is crucial to integrate it effectively with the query. The main methods in this stage are:\n\nRe-rank chunks: Re-ranking the retrieved information to relocated the most relevant content to the edges of the prompt\nContext Compressing: concentrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed."
  },
  {
    "objectID": "posts/Blogs/LLM/RAG.html#new-modules",
    "href": "posts/Blogs/LLM/RAG.html#new-modules",
    "title": "LLM: RAG",
    "section": "3.1 New Modules",
    "text": "3.1 New Modules\n\n3.1.1 Search Module\nThe search module adapts to specific scenarios, enabling direct searches across various sources like search engines, database, and knowledge graphs, using LLM-generated code and query languages\nThe RAG-Fusing addresses search limitations by employing a multi-query strategy that expands user queries into diverse perspectives, utilizing parallel vector searches and intelligent re-ranking to uncover both explicit and transformative knowledge, utilizing parallel vector searches and intelligent re-ranking to uncover both explict and transformative knowledge.\n\n\n3.1.2 Memory Module\nThe Memory module leverages the LLM‚Äôs memory to guide retrieval, creating an unbounded memory pool that aligns the text more closely with data distribution through iterative self-enhancement.\n\n\n3.1.3 Routing Module\nThe Routing navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, database searches, or merging different information streams.\n\n\n3.1.4 The Preict Module\nThe predict module aims to reduce redundancy and noise by generating context directly through the LLM, ensuring relevance and accuracy.\n\n\n3.1.5 Task Adapter Module\nThe adapter module tailors RAG to various downstream tasks, automating prompt retrieval for zero-shot inputs and creating task-specific retrievers through few-shot query generationg."
  },
  {
    "objectID": "posts/Blogs/LLM/RAG.html#new-pattern",
    "href": "posts/Blogs/LLM/RAG.html#new-pattern",
    "title": "LLM: RAG",
    "section": "3.2 New Pattern",
    "text": "3.2 New Pattern\nModular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. Modular RAG expands the flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks."
  },
  {
    "objectID": "posts/blogs.html",
    "href": "posts/blogs.html",
    "title": "üëãüèªWelcome to Yuyang‚Äôs Blog",
    "section": "",
    "text": "LLM Part3: Applications\n\n\n\nOverview\n\nLarge Language Model\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model. I will also display the pros and cons between different models, and how we can combine those models to get better performance\n\n\n\n\n1 min\n\n70 words\n\n\n2025-08-07\n\n\n\n\n\n\n\nLLM Overview\n\n\n\nLarge Language Model\n\nOverview\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model. I will also display the pros and cons between different models, and how we can combine those models to get better performance\n\n\n\n\n3 min\n\n564 words\n\n\n2025-08-07\n\n\n\n\n\n\n\nLLM Part1: Pre Training\n\n\n\nOverview\n\nLarge Language Model\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model. I will also display the pros and cons between different models, and how we can combine those models to get better performance\n\n\n\n\n3 min\n\n419 words\n\n\n2025-08-07\n\n\n\n\n\n\n\nLLM: RAG\n\n\n\nLarge Language Model\n\nOverview\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model. I will also display the pros and cons between different models, and how we can combine those models to get better performance\n\n\n\n\n5 min\n\n965 words\n\n\n2025-08-07\n\n\n\n\n\n\n\nLLM Part2: Post Training\n\n\n\nOverview\n\nLarge Language Model\n\n\n\nIn this blog, I am going to summary the 5 most common generative models: Auto-Regreesive Model, Variational AutoEncoder, Engery Based Model, Flow Model and Diffusion Model. I will also display the pros and cons between different models, and how we can combine those models to get better performance\n\n\n\n\n1 min\n\n1 words\n\n\n2025-08-07\n\n\n\n\n\n\n\nSpeed Up Training for Neural Networks\n\n\n\nTraining Tricks\n\n\n\nTraining large neural networks can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculating in the single GPUs, and parallelizing the training across multiple GPUs.\n\n\n\n\n\nYuyang Zhang\n\n14 min\n\n2,648 words\n\n\n2025-08-07\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html",
    "href": "posts/Blogs/speed-up-training.html",
    "title": "Speed Up Training for Neural Networks",
    "section": "",
    "text": "Training large neural networks(such as large language models) can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculations on a single GPU through different techniques such as fusion, tiling, memory coalescing, and parallelizing the training across multiple GPUs such as model parallelism and data parallelism. But before that, we need to understand the basic concepts of GPU, and data types to better understand why and when we need to use these techniques."
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#preliminary",
    "href": "posts/Blogs/speed-up-training.html#preliminary",
    "title": "Speed Up Training for Neural Networks",
    "section": "1 Preliminary",
    "text": "1 Preliminary\n\n1.1 Data Representation\nIn deep learning, we often use different data types to represent our data and model parameters. The most common data types are:\n\nFloat32: also known as single-precision floating-point format. This is the default floating-point representation  used in most deep learning frameworks. It provides a good balance between precision and performance. It use 32 bits (4 bytes) to represent a number.\nFloat16: This is a half-precision floating-point representation that uses 16 bits instead of 32 bits. It can significantly speed up training and reduce memory usage, but it may lead to numerical instability in some cases.\nBFloat16: This is a truncated version of Float32 that retains the exponent bits but reduces the mantissa bits. It is designed to provide a good trade-off between precision and performance, especially for training large models.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The representation of Float32\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The representation of Float16\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) The representation of BFloat16\n\n\n\n\n\n\n\nFigure¬†1: The representation of float32, float16, and bfloat16 data types. The figure shows how the bits are allocated for the sign, exponent, and mantissa in each data type.\n\n\n\n\n\n\n\n\n\nHow those bits represent the number?\n\n\n\n\\[\n\\text{value} = (-1)^s \\times (1.f) \\times 2^{e - 127}\n\\]\nwhere:\n\n\\(s\\) is the sign bit (0 for positive, 1 for negative)\n\\(f\\) is the mantissa (the fractional part): \\(1.f = 1 + \\sum_{i=1}^{23} b_i \\cdot 2^{-i}\\), where \\(b_i\\) are the bits of the mantissa either 0 or 1.\n\\(e\\) is the exponent (an 8-bit unsigned int) with a bias of 127:\n\nFor Float32, \\(e\\) is 8 bits, which range from [1, 254]\nFor Float16, \\(e\\) is 5 bits, which range from [1, 30]\nFor BFloat16, \\(e\\) is 8 bits, which range from [1, 254]\n\n\n\n\nTo check the data type of a tensor and its properties in PyTorch, you can use the .dtype attribute. For example:\nx = torch.zeros(4, 8)\nx.dtype # check the data type of x\nx.numel() # check the number of elements in x\nx.element_size() # check the size of each element in bytes\nx.numel() * x.element_size() # check the total size in bytes\n\n\n1.2 Calculate Memory Usage of Model\nAssume we have a model with \\(N\\) parameters, and each parameter is represented by float32 (4 bytes). \\(A\\) is the number of activation elements stored during forward (depends on input and model depth). How can we calculate the memory need for training this model? Notice that the memory usage of a model is not only determined by the parameters, but also by:\n\nactivations\ngradients\noptimizer states\n\nFor a single parameter, the memory usage for one forward pass and backward pass is:\n\nparameter: 4 bytes (float32)\nactivation: 4 bytes (float32)\ngradient: 4 bytes (float32)\noptimizer state, which can vary depending on the optimizer used. For example, Adam optimizer requires 2 additional states (momentum and variance), each of which is 4 bytes (float32).\n\nSo, the total memory usage for one parameter is: \\[\n\\text{Memory per parameter} = 4 + 4  + 2 \\times 4 = 16 \\text{ bytes}\n\\]\nThus, the total memory usage for the model is: \\[\n\\text{Total Memory} = N \\times 16 \\text{ bytes} + A \\times 4 \\text{ bytes}\n\\]\n\n\n\n\n\n\nWhy need activation for backward pass?\n\n\n\nSay a layer denotes the input to the layer as \\(\\mathbf{x}\\), the weights as \\(\\theta\\), and the output as \\(\\mathbf{y}\\). The loss function is denoted as \\(L\\), which is a function of the output \\(\\mathbf{y}\\) and the target \\(t\\): \\[\n\\mathbf{y} = f(\\mathbf{x}; \\theta)\n\\] To compute the gradient of the loss with respect to the weights, we need to use the chain rule: \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\theta}\n\\]\nwhere \\(\\frac{\\partial \\mathbf{y}}{\\partial \\theta}\\) is the gradient of the output with respect to the weights \\(\\theta\\), which usually are the function of the input \\(\\mathbf{x}\\) and the weights \\(\\theta\\). To compute this gradient, we need to know the input \\(\\mathbf{x}\\). For example, the linear layer computes: \\[\n\\mathbf{y} = \\mathbf{x} \\cdot \\theta + b\n\\] to compute the gradient, we need to know the input \\(\\mathbf{x}\\), \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\mathbf{x}\n\\] where \\(\\mathbf{x}\\) is the input to the layer, which is the activation of the previous layer. Thus, we need to store the activation for the backward pass.\n\n\n\n\nCalculate Memory Usage of Model\nimport torch\nimport torch.nn as nn\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define a simple model\nmodel = nn.Sequential(\n    nn.Linear(100, 200),\n    nn.ReLU(),\n    nn.Linear(200, 50),\n    nn.ReLU(),\n    nn.Linear(50, 10)\n).to(device)\n\n# Register hooks to track activation sizes\nactivation_sizes = []\n\ndef hook_fn(module, input, output):\n    if isinstance(output, torch.Tensor):\n        activation_sizes.append(output.numel())\n    elif isinstance(output, (list, tuple)):\n        activation_sizes.extend(o.numel() for o in output if isinstance(o, torch.Tensor))\n\nhooks = []\nfor layer in model:\n    hooks.append(layer.register_forward_hook(hook_fn))\n\n# Create input\nx = torch.randn(32, 100, device=device, requires_grad=True)\n\n# Forward pass\noutput = model(x)\nloss = output.mean()\n\n# Backward pass\nloss.backward()\n\n# Remove hooks\nfor h in hooks:\n    h.remove()\n\n# --------- Memory Estimation ---------\n# 1. Parameters\nnum_params = sum(p.numel() for p in model.parameters())\nparam_memory = num_params * 4\ngrad_memory = num_params * 4\noptimizer_memory = num_params * 8\n\n# 2. Activations (sum of all layer outputs + input)\nactivation_memory = (x.numel() + sum(activation_sizes)) * 4  # float32\n\n# 3. Total\ntotal_bytes = param_memory + grad_memory + optimizer_memory + activation_memory\ntotal_MB = total_bytes / (1024 ** 2)\n\n# Print\nprint(f\"Total parameters: {num_params}\")\nprint(f\"Activation elements: {sum(activation_sizes)}\")\nprint(f\"Total training memory (float32): {total_MB:.2f} MB\")\n\n\n\n\n1.3 Collective operations\nCollective operations are operations that involve multiple processes or devices, such as GPUs, to perform a computation. They are essential for parallelizing the training process across multiple GPUs. Some common collective operations include:\n\nBroadcast(Figure¬†2 (a)): This operation sends data from one process to all other processes. It is commonly used to share model parameters or hyperparameters across multiple GPUs.\nScatter(Figure¬†2 (b)): This operation distributes data from one process to multiple processes. It is often used to distribute input data across multiple GPUs.\nGather(Figure¬†2 (c)): This operation collects data from multiple processes and combines it into a single process. It is useful for aggregating results from multiple GPUs.\nReduce(Figure¬†2 (d)): This operation combines data from multiple processes into a single process. It is commonly used to compute the sum or maximum of gradients across multiple GPUs.\nAll-gather(Figure¬†2 (e)): This operation collects data from all processes and distributes it back to all processes. It is often used to gather gradients or model parameters from multiple GPUs without losing any information.\nAll-reduce(Figure¬†2 (g)): This operation combines data from all processes and distributes the result back to all processes. It is often used to average gradients across multiple GPUs during training.\nReduce-scatter(Figure¬†2 (f)): This operation combines data from multiple processes and distributes the result to each process. It is often used to reduce the amount of data that needs to be communicated between processes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Broadcast\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter\n\n\n\n\n\n\n\n\n\n\n\n(c) Gather\n\n\n\n\n\n\n\n\n\n\n\n(d) Reduce\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) All-gather\n\n\n\n\n\n\n\n\n\n\n\n(f) Reduce-scatter\n\n\n\n\n\n\n\n\n\n\n\n(g) All-reduce\n\n\n\n\n\n\n\nFigure¬†2: The illustration of different collective operations. The figure shows how data is communicated between processes in each operation.(Image take from: Stanford CS336)\n\n\n\n\nOne should take note is Reduce-Scatter combines two operations:\n\nReduce: Each process (or GPU) contributes its data, and a reduction operation (usually sum, mean, max, etc.) is applied across processes.\nScatter: The reduced result is partitioned and each process gets only a portion (its shard) of the reduced result.\n\n\n\n\n\n\n\nTip\n\n\n\nWay to remember the terminology:\n\nGather: collects data from multiple sources into one destination(not do any operation)\nReduce: performs some associative/commutative operation (sum, min, max)\nBroadcast/Scatter: is inverse of Gather\nAll: means destination is all devices"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#profiling",
    "href": "posts/Blogs/speed-up-training.html#profiling",
    "title": "Speed Up Training for Neural Networks",
    "section": "2 Profiling",
    "text": "2 Profiling\nTo optimize the training process, we need to first profile our model to identify the bottlenecks. In this section, we will introduce several tools to profile the model and understand where the time is spent during training. We will discuss several tools that can help us profile our model and identify the bottlenecks in the training process:\n\nSimple Benchmarking (Section¬†2.1): The simplest way to measure the time taken for each operation in your model. You can use the time module in Python to measure the time taken for each operation. For example, you can wrap your forward pass in a timer to measure the time taken for each layer.\nPyTorch Profiler (Section¬†2.2): PyTorch provides a built-in profiler that can help you analyze the performance of your model. You can use the torch.profiler module to profile your model and visualize the results. The profiler provides detailed information about the time spent on each operation, memory usage, and more.\nNVIDIA Nsight Systems (Section¬†2.3): This is a powerful profiling tool that can help you analyze the performance of your model on NVIDIA GPUs. It provides detailed information about the GPU utilization, memory usage, and more. You can use it to identify bottlenecks in your model and optimize the performance.\n\n\n2.1 Simple Benchmarking\n\n\n2.2 PyTorch Profiler\n\n\n2.3 NVIDIA Nsight Systems"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#single-gpu-optimization",
    "href": "posts/Blogs/speed-up-training.html#single-gpu-optimization",
    "title": "Speed Up Training for Neural Networks",
    "section": "3 Single GPU Optimization",
    "text": "3 Single GPU Optimization\nIn this section, we will discuss various techniques to optimize the training process on a single GPU. These techniques include: - Fusion(Section¬†3.1): This technique combines multiple operations into a single operation to reduce the number of kernel launches and improve performance. For example, you can fuse the forward and backward passes of a layer into a single operation. - Tiling(Section¬†3.2): This technique divides the input data into smaller tiles and processes them in parallel to improve memory access patterns and reduce memory usage. For example, you can tile the input data into smaller chunks and process them in parallel. - Memory Coalescing(Section¬†3.3): This technique optimizes memory access patterns to improve memory bandwidth utilization. For example, you can coalesce memory accesses to reduce the number of memory transactions and improve performance. - Mixed Precision Training(Section¬†3.4): This technique uses lower precision data types (such as float16 or bfloat16) to reduce memory usage and improve performance. It can significantly speed up training while maintaining model accuracy. PyTorch provides built-in support for mixed precision training through the torch.cuda.amp module, which allows you to automatically cast your model and inputs to lower precision during training. - Gradient Accumulation(Section¬†3.5): This technique accumulates gradients over multiple mini-batches before performing a weight update. It can help reduce the number of weight updates and improve training stability, especially when using large batch sizes. You can implement gradient accumulation by accumulating gradients in a buffer and updating the model parameters only after a certain number of mini-batches.\n\n3.1 Fusion\n\n\n3.2 Tiling\n\n\n3.3 Memory Coalescing\n\n\n3.4 Mixed Precision Training\n\n\n3.5 Gradient Accumulation\n\n\n3.6 Case Study: Flash Attention"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#multi-gpu-optimizationparallelism",
    "href": "posts/Blogs/speed-up-training.html#multi-gpu-optimizationparallelism",
    "title": "Speed Up Training for Neural Networks",
    "section": "4 Multi-GPU Optimization(Parallelism)",
    "text": "4 Multi-GPU Optimization(Parallelism)\nIn this section, we will discuss various techniques to optimize the training process across multiple GPUs. These techniques include: - Data Parallelism(Section¬†4.1): This technique splits the input data across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different subset of the input data, and the gradients are averaged across GPUs before updating the model parameters. - Model Parallelism(Section¬†4.2): This technique splits the model across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the model, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large models that do not fit into a single GPU‚Äôs memory. - Pipeline Parallelism(Section¬†4.3): This technique splits the model into multiple stages and processes each stage in parallel across multiple GPUs. Each GPU processes a different stage of the model, and the output of one stage is passed to the next stage. This can help improve throughput and reduce memory usage, especially for large models. - Tensor Parallelism(Section¬†4.4): This technique splits the tensors across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the tensor, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large tensors that do not fit into a single GPU‚Äôs memory. - Context Parallelism(Section¬†4.5): This technique splits the context across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the context, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large contexts that do not fit into a single GPU‚Äôs memory.\n\n4.1 Data Parallelism\n\n\n4.2 Model Parallelism\n\n\n4.3 Pipeline Parallelism\n\n\n4.4 Tensor Parallelism\n\n\n4.5 Context Parallelism\n\n\n4.6 Case Study: DeepSpeed\n\n\n4.7 Case Study: Megatron-LM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nSelf CPU %\nSelf CPU\nCPU total %\nCPU total\nCPU time avg\nSelf CUDA\nSelf CUDA %\nCUDA total\nCUDA time avg\n# of Calls\n\n\n\n\naten::gelu\n6.76%\n669.785us\n13.48%\n1.336ms\n1.336ms\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\nvoid at::native::vectorized_elementwise_kernel&lt;‚Ä¶&gt;\n0.00%\n0.000us\n0.00%\n0.000us\n0.000us\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\ncudaLaunchKernel\n6.72%\n665.807us\n6.72%\n665.807us\n665.807us\n0.000us\n0.00%\n0.000us\n0.000us\n1\n\n\ncudaDeviceSynchronize\n86.52%\n8.574ms\n86.52%\n8.574ms\n4.287ms\n0.000us\n0.00%\n0.000us\n0.000us\n2\n\n\n\nSelf CPU time total: 9.909 ms\nSelf CUDA time total: 8.642 ms"
  },
  {
    "objectID": "posts/Projects/VLM/vlm.html",
    "href": "posts/Projects/VLM/vlm.html",
    "title": "Visual Language Model",
    "section": "",
    "text": "The vision encoder is the classic Vision Transformer (ViT) architecture as proposed in (Dosovitskiy et al. 2021).\n\n\n\n\n\n\nFigure¬†1: The illustration of the Vision Transformer.\n\n\n\n\n\nThe first component of the ViT is the Patch Embedding layer, which converts the input image into a sequence of patches. Each patch is treated as a token, similar to how words are treated in NLP models. The patches are flattened and linearly projected into a higher-dimensional space. We can combine those two steps into a single convolutional layer. After patching the image into smaller patches, we can add a learnable class token and positional embeddings to the sequence of patches. The class token is used for classification tasks, while positional embeddings help the model understand the spatial relationships between patches.\nclass ViTPatchEmbedding(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.img_size = config.vit_img_size\n        self.patch_size = config.vit_patch_size\n\n        assert (\n            self.img_size % self.patch_size == 0\n        ), \"Image size must be divisible by patch size.\"\n        self.num_patches = (self.img_size // self.patch_size) ** 2\n        self.cls_flag = config.vit_cls_flag\n        self.embd_dim = config.vit_hidden_dim\n\n        self.conv = nn.Conv2d(\n            in_channels=3,\n            out_channels=self.embd_dim,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n            padding=\"valid\",\n        )\n\n        if self.cls_flag:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embd_dim))  # (B, 1, D)\n            self.position_embeddings = nn.Parameter(\n                torch.zeros(1, self.num_patches + 1, self.embd_dim)\n            )  # (B, P+1, D)\n        else:\n            self.position_embeddings = nn.Parameter(\n                torch.zeros(1, self.num_patches, self.embd_dim)\n            )\n\n    def forward(self, imgs: torch.Tensor):\n        # (B, C, H, W) -&gt; (B, D, H // P, W // P)\n        x = self.conv(imgs)\n        # (B, D, H // P, W // P) -&gt; (B, H // P * W // P, D)\n        x = x.flatten(2).transpose(1, 2)\n\n        if self.cls_flag:\n            cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n            x = torch.cat((cls_tokens, x), dim=1)  # (B, P+1, D)\n\n        x = x + self.position_embeddings  # (B, P+1, D) or (B, P, D)\n        return x\n\n\n\nAfter we get the tokens from the Patch Embedding layer, we can feed those tokens into the transformer block. The transformer block consists of a Multi-Head Self-Attention (MHSA) layer and a Feed Forward Network (FFN). The MHSA layer allows the model to attend to different parts of the input sequence simultaneously, capturing complex relationships between patches. The FFN is a simple feed-forward neural network that processes the output of the MHSA layer.\ndef scale_dot_product_attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    dropout: float = 0.0,\n):\n\n    d_k = q.shape[-1]\n\n    # Compute the dot product attention scores\n    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (\n        d_k**0.5\n    )  # Scale by the square root of the dimension\n    if mask is not None:\n        attn_scores = attn_scores.masked_fill(mask == 0, float(\"-inf\"))\n    attn_weights = torch.softmax(attn_scores, dim=-1)\n    if dropout &gt; 0.0:\n        attn_weights = torch.nn.functional.dropout(\n            attn_weights, p=dropout, training=True\n        )\n    # Compute the attention output\n    attn_output = torch.matmul(attn_weights, v)  # Shape: (B, S_q, D)\n\n    return attn_output, attn_weights\n\n\n\nclass ViTMultiHeadAttention(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.n_heads = config.vit_n_heads\n        self.embd_dim = config.vit_hidden_dim\n\n        assert (\n            self.embd_dim % self.n_heads == 0\n        ), \"embd_dim must be divisible by num_heads\"\n        self.head_dim = self.embd_dim // self.n_heads\n\n        self.dropout = config.vit_dropout\n\n        self.qkv_proj = nn.Linear(self.embd_dim, 3 * self.embd_dim)\n        self.out_proj = nn.Linear(self.embd_dim, self.embd_dim)\n\n        # Dropout layer\n        self.attn_dropout = nn.Dropout(self.dropout)\n        self.resid_dropout = nn.Dropout(self.dropout)\n\n        # Use scaled dot product attention\n        self.sdpa = hasattr(F, \"scaled_dot_product_attention\")\n        if not self.sdpa:\n            print(\n                \"Warning: Scaled Dot Product Attention not available. Using custom implementation.\"\n            )\n\n    def forward(self, x: torch.Tensor):\n        B, T, C = x.size()\n\n        q, k, v = map(\n            lambda t: t.view(B, T, self.n_heads, self.head_dim).transpose(1, 2),\n            self.qkv_proj(x).chunk(3, dim=-1),\n        )\n\n        if self.sdpa:\n            y = F.scaled_dot_product_attention(\n                q, k, v, dropout_p=self.dropout if self.training else 0.0\n            )\n        else:\n            y, _ = scale_dot_product_attention(\n                q=q, k=k, v=v, dropout=self.dropout if self.training else 0.0\n            )\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.out_proj(y)\n\n        return self.resid_dropout(y)\n\n\n\nThe Feed Forward Network (FFN) is a simple two-layer fully connected network with a GeLU activation function in between. It processes the output of the MHSA layer and applies a residual connection to the input.\nclass MLP(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.activation_fn = nn.GELU(approximate=\"tanh\")\n        self.fc1 = nn.Linear(config.vit_hidden_dim, config.vit_inter_dim)\n        self.fc2 = nn.Linear(config.vit_inter_dim, config.vit_hidden_dim)\n        self.dropout = nn.Dropout(config.vit_dropout)\n\n    def forward(self, x: torch.Tensor):\n        x = self.fc1(x)\n        x = self.activation_fn(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\n\nAfter define the MHSA and FFN layers, we can combine them into a single transformer block. The transformer block applies layer normalization before the MHSA and FFN layers(pre-norm), and it also includes residual connections to help with training stability.\nclass ViTBlock(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.attn = ViTMultiHeadAttention(config)\n        self.mlp = MLP(config)\n        self.ln1 = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n        self.ln2 = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n\n    def forward(self, x: torch.Tensor):\n        # Layer normalization and multi-head attention\n        x = x + self.attn(self.ln1(x))\n        # Layer normalization and MLP\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n\n\nFinally, we can combine the Patch Embedding layer and the transformer blocks to create the Vision Transformer. The Vision Transformer consists of a series of transformer blocks stacked on top of each other, with the output of the last block being used for classification or further processing.\nclass ViT(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.config = config\n\n        self.patch_embedding = ViTPatchEmbedding(config)\n\n        self.cls_flag = config.vit_cls_flag\n        self.dropout = nn.Dropout(config.vit_dropout)\n\n        self.blocks = nn.ModuleList(\n            [ViTBlock(config) for _ in range(config.vit_n_blocks)]\n        )\n\n        self.layer_norm = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n\n        self.apply(self._init_weights)\n    \n    def forward(self, imgs: torch.Tensor):\n        x = self.patch_embedding(imgs)\n        x = self.dropout(x)\n\n        for block in self.blocks:\n            x = block(x)\n\n        if self.cls_flag:\n            x = x[:, 0]\n        else:\n            x = self.layer_norm(x)\n\n        return x\nSo, that all we need for the Vision Encoder. After we get the output of the\n\n\n\nVision Encoder, we need to project the output into the semantic embedding space to match the text embedding space. This is done using a linear projection layer. One small trick used here is pixel shuffle(Shi et al. 2016), which is used to reduce the number of tokens.\n\n\n\n\n\n\nFigure¬†2: Illustration of the pixel shuffle operation and un-shuffling process. The pixel shuffle operation rearranges the elements of a tensor to increase the spatial resolution, while the un-shuffle process reverses this operation.\n\n\n\nclass ModalityProjector(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n        \n        self.config = config \n        \n        self.input_dim = config.vit_hidden_dim * (config.mp_pixel_shuffle_factor**2)    \n        self.output_dim = config.lm_hidden_dim\n        self.scale_factor = config.mp_pixel_shuffle_factor\n        \n        \n        self.proj = nn.Linear(self.input_dim, self.output_dim, bias=False)\n        self._init_weight()\n\n    def _init_weight(self):\n        nn.init.normal_(self.proj.weight, mean=0.0, std=0.02)\n    \n    def pixel_shuffle(self, x: torch.Tensor) -&gt; torch.Tensor:\n        B, S, D = x.size()\n        assert S % 2 == 0, \"Input sequence length must be even for pixel shuffle.\"\n        assert S ** 0.5 % self.scale_factor == 0, \"Input sequence length must be a perfect square for pixel shuffle.\"\n        \n        \n        H, W = S, S \n        x = x.view(B, H, W, D) # Convert the flattened sequence into a 2D grid\n        h_out = H // self.scale_factor\n        w_out = W // self.scale_factor\n        \n        x = einops.rearrange(x, \n                             \"b (h sf1) (w sf2) d -&gt; b (h w) (d sf1 sf2)\",\n                             sf1=self.scale_factor, \n                             sf2=self.scale_factor\n                             )\n        \n        return x\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.pixel_shuffle(x)\n        assert x.size(-1) == self.input_dim, f\"Input dimension mismatch: expected {self.input_dim}, got {x.size(-1)}\"\n        \n        x = self.proj(x)\n        return x\nAfter the pixel shuffle, the number of tokens is reduced by a factor of mp_pixel_shuffle_factor**2, which helps to reduce the computational cost while maintaining the semantic information of the visual input. The output of the Modality Projector is then ready to be fed into the Language Model (LM) for further processing."
  },
  {
    "objectID": "posts/Projects/VLM/vlm.html#vision-transformer",
    "href": "posts/Projects/VLM/vlm.html#vision-transformer",
    "title": "Visual Language Model",
    "section": "",
    "text": "The vision encoder is the classic Vision Transformer (ViT) architecture as proposed in (Dosovitskiy et al. 2021).\n\n\n\n\n\n\nFigure¬†1: The illustration of the Vision Transformer.\n\n\n\n\n\nThe first component of the ViT is the Patch Embedding layer, which converts the input image into a sequence of patches. Each patch is treated as a token, similar to how words are treated in NLP models. The patches are flattened and linearly projected into a higher-dimensional space. We can combine those two steps into a single convolutional layer. After patching the image into smaller patches, we can add a learnable class token and positional embeddings to the sequence of patches. The class token is used for classification tasks, while positional embeddings help the model understand the spatial relationships between patches.\nclass ViTPatchEmbedding(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.img_size = config.vit_img_size\n        self.patch_size = config.vit_patch_size\n\n        assert (\n            self.img_size % self.patch_size == 0\n        ), \"Image size must be divisible by patch size.\"\n        self.num_patches = (self.img_size // self.patch_size) ** 2\n        self.cls_flag = config.vit_cls_flag\n        self.embd_dim = config.vit_hidden_dim\n\n        self.conv = nn.Conv2d(\n            in_channels=3,\n            out_channels=self.embd_dim,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n            padding=\"valid\",\n        )\n\n        if self.cls_flag:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embd_dim))  # (B, 1, D)\n            self.position_embeddings = nn.Parameter(\n                torch.zeros(1, self.num_patches + 1, self.embd_dim)\n            )  # (B, P+1, D)\n        else:\n            self.position_embeddings = nn.Parameter(\n                torch.zeros(1, self.num_patches, self.embd_dim)\n            )\n\n    def forward(self, imgs: torch.Tensor):\n        # (B, C, H, W) -&gt; (B, D, H // P, W // P)\n        x = self.conv(imgs)\n        # (B, D, H // P, W // P) -&gt; (B, H // P * W // P, D)\n        x = x.flatten(2).transpose(1, 2)\n\n        if self.cls_flag:\n            cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n            x = torch.cat((cls_tokens, x), dim=1)  # (B, P+1, D)\n\n        x = x + self.position_embeddings  # (B, P+1, D) or (B, P, D)\n        return x\n\n\n\nAfter we get the tokens from the Patch Embedding layer, we can feed those tokens into the transformer block. The transformer block consists of a Multi-Head Self-Attention (MHSA) layer and a Feed Forward Network (FFN). The MHSA layer allows the model to attend to different parts of the input sequence simultaneously, capturing complex relationships between patches. The FFN is a simple feed-forward neural network that processes the output of the MHSA layer.\ndef scale_dot_product_attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    dropout: float = 0.0,\n):\n\n    d_k = q.shape[-1]\n\n    # Compute the dot product attention scores\n    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (\n        d_k**0.5\n    )  # Scale by the square root of the dimension\n    if mask is not None:\n        attn_scores = attn_scores.masked_fill(mask == 0, float(\"-inf\"))\n    attn_weights = torch.softmax(attn_scores, dim=-1)\n    if dropout &gt; 0.0:\n        attn_weights = torch.nn.functional.dropout(\n            attn_weights, p=dropout, training=True\n        )\n    # Compute the attention output\n    attn_output = torch.matmul(attn_weights, v)  # Shape: (B, S_q, D)\n\n    return attn_output, attn_weights\n\n\n\nclass ViTMultiHeadAttention(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.n_heads = config.vit_n_heads\n        self.embd_dim = config.vit_hidden_dim\n\n        assert (\n            self.embd_dim % self.n_heads == 0\n        ), \"embd_dim must be divisible by num_heads\"\n        self.head_dim = self.embd_dim // self.n_heads\n\n        self.dropout = config.vit_dropout\n\n        self.qkv_proj = nn.Linear(self.embd_dim, 3 * self.embd_dim)\n        self.out_proj = nn.Linear(self.embd_dim, self.embd_dim)\n\n        # Dropout layer\n        self.attn_dropout = nn.Dropout(self.dropout)\n        self.resid_dropout = nn.Dropout(self.dropout)\n\n        # Use scaled dot product attention\n        self.sdpa = hasattr(F, \"scaled_dot_product_attention\")\n        if not self.sdpa:\n            print(\n                \"Warning: Scaled Dot Product Attention not available. Using custom implementation.\"\n            )\n\n    def forward(self, x: torch.Tensor):\n        B, T, C = x.size()\n\n        q, k, v = map(\n            lambda t: t.view(B, T, self.n_heads, self.head_dim).transpose(1, 2),\n            self.qkv_proj(x).chunk(3, dim=-1),\n        )\n\n        if self.sdpa:\n            y = F.scaled_dot_product_attention(\n                q, k, v, dropout_p=self.dropout if self.training else 0.0\n            )\n        else:\n            y, _ = scale_dot_product_attention(\n                q=q, k=k, v=v, dropout=self.dropout if self.training else 0.0\n            )\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.out_proj(y)\n\n        return self.resid_dropout(y)\n\n\n\nThe Feed Forward Network (FFN) is a simple two-layer fully connected network with a GeLU activation function in between. It processes the output of the MHSA layer and applies a residual connection to the input.\nclass MLP(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.activation_fn = nn.GELU(approximate=\"tanh\")\n        self.fc1 = nn.Linear(config.vit_hidden_dim, config.vit_inter_dim)\n        self.fc2 = nn.Linear(config.vit_inter_dim, config.vit_hidden_dim)\n        self.dropout = nn.Dropout(config.vit_dropout)\n\n    def forward(self, x: torch.Tensor):\n        x = self.fc1(x)\n        x = self.activation_fn(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\n\nAfter define the MHSA and FFN layers, we can combine them into a single transformer block. The transformer block applies layer normalization before the MHSA and FFN layers(pre-norm), and it also includes residual connections to help with training stability.\nclass ViTBlock(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.attn = ViTMultiHeadAttention(config)\n        self.mlp = MLP(config)\n        self.ln1 = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n        self.ln2 = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n\n    def forward(self, x: torch.Tensor):\n        # Layer normalization and multi-head attention\n        x = x + self.attn(self.ln1(x))\n        # Layer normalization and MLP\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n\n\nFinally, we can combine the Patch Embedding layer and the transformer blocks to create the Vision Transformer. The Vision Transformer consists of a series of transformer blocks stacked on top of each other, with the output of the last block being used for classification or further processing.\nclass ViT(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.config = config\n\n        self.patch_embedding = ViTPatchEmbedding(config)\n\n        self.cls_flag = config.vit_cls_flag\n        self.dropout = nn.Dropout(config.vit_dropout)\n\n        self.blocks = nn.ModuleList(\n            [ViTBlock(config) for _ in range(config.vit_n_blocks)]\n        )\n\n        self.layer_norm = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n\n        self.apply(self._init_weights)\n    \n    def forward(self, imgs: torch.Tensor):\n        x = self.patch_embedding(imgs)\n        x = self.dropout(x)\n\n        for block in self.blocks:\n            x = block(x)\n\n        if self.cls_flag:\n            x = x[:, 0]\n        else:\n            x = self.layer_norm(x)\n\n        return x\nSo, that all we need for the Vision Encoder. After we get the output of the\n\n\n\nVision Encoder, we need to project the output into the semantic embedding space to match the text embedding space. This is done using a linear projection layer. One small trick used here is pixel shuffle(Shi et al. 2016), which is used to reduce the number of tokens.\n\n\n\n\n\n\nFigure¬†2: Illustration of the pixel shuffle operation and un-shuffling process. The pixel shuffle operation rearranges the elements of a tensor to increase the spatial resolution, while the un-shuffle process reverses this operation.\n\n\n\nclass ModalityProjector(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n        \n        self.config = config \n        \n        self.input_dim = config.vit_hidden_dim * (config.mp_pixel_shuffle_factor**2)    \n        self.output_dim = config.lm_hidden_dim\n        self.scale_factor = config.mp_pixel_shuffle_factor\n        \n        \n        self.proj = nn.Linear(self.input_dim, self.output_dim, bias=False)\n        self._init_weight()\n\n    def _init_weight(self):\n        nn.init.normal_(self.proj.weight, mean=0.0, std=0.02)\n    \n    def pixel_shuffle(self, x: torch.Tensor) -&gt; torch.Tensor:\n        B, S, D = x.size()\n        assert S % 2 == 0, \"Input sequence length must be even for pixel shuffle.\"\n        assert S ** 0.5 % self.scale_factor == 0, \"Input sequence length must be a perfect square for pixel shuffle.\"\n        \n        \n        H, W = S, S \n        x = x.view(B, H, W, D) # Convert the flattened sequence into a 2D grid\n        h_out = H // self.scale_factor\n        w_out = W // self.scale_factor\n        \n        x = einops.rearrange(x, \n                             \"b (h sf1) (w sf2) d -&gt; b (h w) (d sf1 sf2)\",\n                             sf1=self.scale_factor, \n                             sf2=self.scale_factor\n                             )\n        \n        return x\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.pixel_shuffle(x)\n        assert x.size(-1) == self.input_dim, f\"Input dimension mismatch: expected {self.input_dim}, got {x.size(-1)}\"\n        \n        x = self.proj(x)\n        return x\nAfter the pixel shuffle, the number of tokens is reduced by a factor of mp_pixel_shuffle_factor**2, which helps to reduce the computational cost while maintaining the semantic information of the visual input. The output of the Modality Projector is then ready to be fed into the Language Model (LM) for further processing."
  }
]