[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Yuyang",
    "section": "",
    "text": "I am a recent graduate from Singapore Management University in MITB (AI Track). With a strong foundation in computer science and big data, I have hands-on experience in machine learning and deep learning, and I’m passionate about building impactful AI-driven solutions. I am currently seeking full-time opportunities in the AI field where I can contribute, grow, and make a meaningful impact. Open to Work – feel free to reach out at zhangyuyang1211@gmail.com!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Yuyang",
    "section": "Education",
    "text": "Education\n\n  \n    \n    \n      2023 - 2025\n      Master of IT in Business\n      Singapore Management University\n    \n  \n  \n    \n    \n      2020 - 2023\n      Bachelor of Computer Science\n      University of Wollongong"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About Yuyang",
    "section": "Work Experience",
    "text": "Work Experience\n\n  \n    \n    \n      March 2025  - Present\n      Research Assistant Intern\n      SMART"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "About Yuyang",
    "section": "Projects",
    "text": "Projects\n\n  \n    \n    \n       File-Tuning Large Visual Language Model\n          [GitHub]\n          [Blog]\n      \n        Wrting the"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "About Yuyang",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n  \n    Python Development\n    Algorithm Design\n    Deep Learning\n    Machine Learning\n    Computer Vision\n    Natural Language Processing\n    Deep Reinforcement Learning\n    Graph Neural Networks\n    Large Language Model\n    Model Compression\n    Convex Optimization\n    Probabilistic Graph Model\n    Meta Learning\n    Deep Generative Model"
  },
  {
    "objectID": "posts/projects.html",
    "href": "posts/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Visual Language Model\n\n14 min\n\n\nLarge Language Model\n\nMulti-Modality\n\n\n\nIn this project, I will implement a Visual Language Model (VLM) using ‘pure’ PyTorch, which is a model that can understand and generate text based on visual inputs.\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaliGemma Inference and Fine Tuning\n\n1 min\n\n\nLarge Language Model\n\nMulti-Modality\n\nFine-Tuning\n\n\n\nBuilt a PaliGemma model from scratch in PyTorch, loaded the 3B (224x224) model, fine-tuned it with LoRA for specific tasks, and developed a Gradio app to showcase its…\n\n\n\nYuyang Zhang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Projects/VLM/vlm.html",
    "href": "posts/Projects/VLM/vlm.html",
    "title": "Visual Language Model",
    "section": "",
    "text": "0.1 Vision Transformer\n  \n  0.1.1 Patch Embedding\n  0.1.2 Multi Head Self-Attention\n  0.1.3 Feed Forward Network\n  0.1.4 Transformer Block\n  0.1.5 Vision Transformer\n  0.1.6 Modality Projection\n  \n  0.2 Language Model\n  1 Data Prepare"
  },
  {
    "objectID": "posts/Projects/VLM/vlm.html#vision-transformer",
    "href": "posts/Projects/VLM/vlm.html#vision-transformer",
    "title": "Visual Language Model",
    "section": "0.1 Vision Transformer",
    "text": "0.1 Vision Transformer\nThe vision encoder is the classic Vision Transformer (ViT) architecture as proposed in (Dosovitskiy et al. 2021).\n\n\n\n\n\n\nFigure 1: The illustration of the Vision Transformer.\n\n\n\n\n0.1.1 Patch Embedding\nThe first component of the ViT is the Patch Embedding layer, which converts the input image into a sequence of patches. Each patch is treated as a token, similar to how words are treated in NLP models. The patches are flattened and linearly projected into a higher-dimensional space. We can combine those two steps into a single convolutional layer. After patching the image into smaller patches, we can add a learnable class token and positional embeddings to the sequence of patches. The class token is used for classification tasks, while positional embeddings help the model understand the spatial relationships between patches.\nclass ViTPatchEmbedding(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.img_size = config.vit_img_size\n        self.patch_size = config.vit_patch_size\n\n        assert (\n            self.img_size % self.patch_size == 0\n        ), \"Image size must be divisible by patch size.\"\n        self.num_patches = (self.img_size // self.patch_size) ** 2\n        self.cls_flag = config.vit_cls_flag\n        self.embd_dim = config.vit_hidden_dim\n\n        self.conv = nn.Conv2d(\n            in_channels=3,\n            out_channels=self.embd_dim,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n            padding=\"valid\",\n        )\n\n        if self.cls_flag:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embd_dim))  # (B, 1, D)\n            self.position_embeddings = nn.Parameter(\n                torch.zeros(1, self.num_patches + 1, self.embd_dim)\n            )  # (B, P+1, D)\n        else:\n            self.position_embeddings = nn.Parameter(\n                torch.zeros(1, self.num_patches, self.embd_dim)\n            )\n\n    def forward(self, imgs: torch.Tensor):\n        # (B, C, H, W) -&gt; (B, D, H // P, W // P)\n        x = self.conv(imgs)\n        # (B, D, H // P, W // P) -&gt; (B, H // P * W // P, D)\n        x = x.flatten(2).transpose(1, 2)\n\n        if self.cls_flag:\n            cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n            x = torch.cat((cls_tokens, x), dim=1)  # (B, P+1, D)\n\n        x = x + self.position_embeddings  # (B, P+1, D) or (B, P, D)\n        return x\n\n\n0.1.2 Multi Head Self-Attention\nAfter we get the tokens from the Patch Embedding layer, we can feed those tokens into the transformer block. The transformer block consists of a Multi-Head Self-Attention (MHSA) layer and a Feed Forward Network (FFN). The MHSA layer allows the model to attend to different parts of the input sequence simultaneously, capturing complex relationships between patches. The FFN is a simple feed-forward neural network that processes the output of the MHSA layer.\ndef scale_dot_product_attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    dropout: float = 0.0,\n):\n\n    d_k = q.shape[-1]\n\n    # Compute the dot product attention scores\n    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (\n        d_k**0.5\n    )  # Scale by the square root of the dimension\n    if mask is not None:\n        attn_scores = attn_scores.masked_fill(mask == 0, float(\"-inf\"))\n    attn_weights = torch.softmax(attn_scores, dim=-1)\n    if dropout &gt; 0.0:\n        attn_weights = torch.nn.functional.dropout(\n            attn_weights, p=dropout, training=True\n        )\n    # Compute the attention output\n    attn_output = torch.matmul(attn_weights, v)  # Shape: (B, S_q, D)\n\n    return attn_output, attn_weights\n\n\n\nclass ViTMultiHeadAttention(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.n_heads = config.vit_n_heads\n        self.embd_dim = config.vit_hidden_dim\n\n        assert (\n            self.embd_dim % self.n_heads == 0\n        ), \"embd_dim must be divisible by num_heads\"\n        self.head_dim = self.embd_dim // self.n_heads\n\n        self.dropout = config.vit_dropout\n\n        self.qkv_proj = nn.Linear(self.embd_dim, 3 * self.embd_dim)\n        self.out_proj = nn.Linear(self.embd_dim, self.embd_dim)\n\n        # Dropout layer\n        self.attn_dropout = nn.Dropout(self.dropout)\n        self.resid_dropout = nn.Dropout(self.dropout)\n\n        # Use scaled dot product attention\n        self.sdpa = hasattr(F, \"scaled_dot_product_attention\")\n        if not self.sdpa:\n            print(\n                \"Warning: Scaled Dot Product Attention not available. Using custom implementation.\"\n            )\n\n    def forward(self, x: torch.Tensor):\n        B, T, C = x.size()\n\n        q, k, v = map(\n            lambda t: t.view(B, T, self.n_heads, self.head_dim).transpose(1, 2),\n            self.qkv_proj(x).chunk(3, dim=-1),\n        )\n\n        if self.sdpa:\n            y = F.scaled_dot_product_attention(\n                q, k, v, dropout_p=self.dropout if self.training else 0.0\n            )\n        else:\n            y, _ = scale_dot_product_attention(\n                q=q, k=k, v=v, dropout=self.dropout if self.training else 0.0\n            )\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.out_proj(y)\n\n        return self.resid_dropout(y)\n\n\n0.1.3 Feed Forward Network\nThe Feed Forward Network (FFN) is a simple two-layer fully connected network with a GeLU activation function in between. It processes the output of the MHSA layer and applies a residual connection to the input.\nclass MLP(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.activation_fn = nn.GELU(approximate=\"tanh\")\n        self.fc1 = nn.Linear(config.vit_hidden_dim, config.vit_inter_dim)\n        self.fc2 = nn.Linear(config.vit_inter_dim, config.vit_hidden_dim)\n        self.dropout = nn.Dropout(config.vit_dropout)\n\n    def forward(self, x: torch.Tensor):\n        x = self.fc1(x)\n        x = self.activation_fn(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\n0.1.4 Transformer Block\nAfter define the MHSA and FFN layers, we can combine them into a single transformer block. The transformer block applies layer normalization before the MHSA and FFN layers(pre-norm), and it also includes residual connections to help with training stability.\nclass ViTBlock(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.attn = ViTMultiHeadAttention(config)\n        self.mlp = MLP(config)\n        self.ln1 = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n        self.ln2 = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n\n    def forward(self, x: torch.Tensor):\n        # Layer normalization and multi-head attention\n        x = x + self.attn(self.ln1(x))\n        # Layer normalization and MLP\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n\n0.1.5 Vision Transformer\nFinally, we can combine the Patch Embedding layer and the transformer blocks to create the Vision Transformer. The Vision Transformer consists of a series of transformer blocks stacked on top of each other, with the output of the last block being used for classification or further processing.\nclass ViT(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n\n        self.config = config\n\n        self.patch_embedding = ViTPatchEmbedding(config)\n\n        self.cls_flag = config.vit_cls_flag\n        self.dropout = nn.Dropout(config.vit_dropout)\n\n        self.blocks = nn.ModuleList(\n            [ViTBlock(config) for _ in range(config.vit_n_blocks)]\n        )\n\n        self.layer_norm = nn.LayerNorm(config.vit_hidden_dim, eps=config.vit_ln_eps)\n\n        self.apply(self._init_weights)\n    \n    def forward(self, imgs: torch.Tensor):\n        x = self.patch_embedding(imgs)\n        x = self.dropout(x)\n\n        for block in self.blocks:\n            x = block(x)\n\n        if self.cls_flag:\n            x = x[:, 0]\n        else:\n            x = self.layer_norm(x)\n\n        return x\nSo, that all we need for the Vision Encoder. After we get the output of the\n\n\n0.1.6 Modality Projection\nVision Encoder, we need to project the output into the semantic embedding space to match the text embedding space. This is done using a linear projection layer. One small trick used here is pixel shuffle(Shi et al. 2016), which is used to reduce the number of tokens.\n\n\n\n\n\n\nFigure 2: Illustration of the pixel shuffle operation and un-shuffling process. The pixel shuffle operation rearranges the elements of a tensor to increase the spatial resolution, while the un-shuffle process reverses this operation.\n\n\n\nclass ModalityProjector(nn.Module):\n    def __init__(self, config: VLMConfig):\n        super().__init__()\n        \n        self.config = config \n        \n        self.input_dim = config.vit_hidden_dim * (config.mp_pixel_shuffle_factor**2)    \n        self.output_dim = config.lm_hidden_dim\n        self.scale_factor = config.mp_pixel_shuffle_factor\n        \n        \n        self.proj = nn.Linear(self.input_dim, self.output_dim, bias=False)\n        self._init_weight()\n\n    def _init_weight(self):\n        nn.init.normal_(self.proj.weight, mean=0.0, std=0.02)\n    \n    def pixel_shuffle(self, x: torch.Tensor) -&gt; torch.Tensor:\n        B, S, D = x.size()\n        assert S % 2 == 0, \"Input sequence length must be even for pixel shuffle.\"\n        assert S ** 0.5 % self.scale_factor == 0, \"Input sequence length must be a perfect square for pixel shuffle.\"\n        \n        \n        H, W = S, S \n        x = x.view(B, H, W, D) # Convert the flattened sequence into a 2D grid\n        h_out = H // self.scale_factor\n        w_out = W // self.scale_factor\n        \n        x = einops.rearrange(x, \n                             \"b (h sf1) (w sf2) d -&gt; b (h w) (d sf1 sf2)\",\n                             sf1=self.scale_factor, \n                             sf2=self.scale_factor\n                             )\n        \n        return x\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.pixel_shuffle(x)\n        assert x.size(-1) == self.input_dim, f\"Input dimension mismatch: expected {self.input_dim}, got {x.size(-1)}\"\n        \n        x = self.proj(x)\n        return x\nAfter the pixel shuffle, the number of tokens is reduced by a factor of mp_pixel_shuffle_factor**2, which helps to reduce the computational cost while maintaining the semantic information of the visual input. The output of the Modality Projector is then ready to be fed into the Language Model (LM) for further processing."
  },
  {
    "objectID": "posts/Projects/VLM/vlm.html#language-model",
    "href": "posts/Projects/VLM/vlm.html#language-model",
    "title": "Visual Language Model",
    "section": "0.2 Language Model",
    "text": "0.2 Language Model"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html",
    "href": "posts/Blogs/speed-up-training.html",
    "title": "Speed Up Training for Neural Networks",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Data Representation\n  1.2 Calculate Memory Usage of Model\n  1.3 Collective operations\n  \n  2 Profiling\n  \n  2.1 Simple Benchmarking\n  2.2 PyTorch Profiler\n  2.3 NVIDIA Nsight Systems\n  \n  3 Single GPU Optimization\n  \n  3.1 Fusion\n  3.2 Tiling\n  3.3 Memory Coalescing\n  3.4 Mixed Precision Training\n  3.5 Gradient Accumulation\n  3.6 Case Study: Flash Attention\n  \n  4 Multi-GPU Optimization(Parallelism)\n  \n  4.1 Data Parallelism\n  4.2 Model Parallelism\n  4.3 Pipeline Parallelism\n  4.4 Tensor Parallelism\n  4.5 Context Parallelism\n  4.6 Case Study: DeepSpeed\n  4.7 Case Study: Megatron-LM\nTraining large neural networks(such as large language models) can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculations on a single GPU through different techniques such as fusion, tiling, memory coalescing, and parallelizing the training across multiple GPUs such as model parallelism and data parallelism. But before that, we need to understand the basic concepts of GPU, and data types to better understand why and when we need to use these techniques."
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#preliminary",
    "href": "posts/Blogs/speed-up-training.html#preliminary",
    "title": "Speed Up Training for Neural Networks",
    "section": "1 Preliminary",
    "text": "1 Preliminary\n\n1.1 Data Representation\nIn deep learning, we often use different data types to represent our data and model parameters. The most common data types are:\n\nFloat32: also known as single-precision floating-point format. This is the default floating-point representation  used in most deep learning frameworks. It provides a good balance between precision and performance. It use 32 bits (4 bytes) to represent a number.\nFloat16: This is a half-precision floating-point representation that uses 16 bits instead of 32 bits. It can significantly speed up training and reduce memory usage, but it may lead to numerical instability in some cases.\nBFloat16: This is a truncated version of Float32 that retains the exponent bits but reduces the mantissa bits. It is designed to provide a good trade-off between precision and performance, especially for training large models.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The representation of Float32\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The representation of Float16\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) The representation of BFloat16\n\n\n\n\n\n\n\nFigure 1: The representation of float32, float16, and bfloat16 data types. The figure shows how the bits are allocated for the sign, exponent, and mantissa in each data type.\n\n\n\n\n\n\n\n\n\nHow those bits represent the number?\n\n\n\n\\[\n\\text{value} = (-1)^s \\times (1.f) \\times 2^{e - 127}\n\\]\nwhere:\n\n\\(s\\) is the sign bit (0 for positive, 1 for negative)\n\\(f\\) is the mantissa (the fractional part): \\(1.f = 1 + \\sum_{i=1}^{23} b_i \\cdot 2^{-i}\\), where \\(b_i\\) are the bits of the mantissa either 0 or 1.\n\\(e\\) is the exponent (an 8-bit unsigned int) with a bias of 127:\n\nFor Float32, \\(e\\) is 8 bits, which range from [1, 254]\nFor Float16, \\(e\\) is 5 bits, which range from [1, 30]\nFor BFloat16, \\(e\\) is 8 bits, which range from [1, 254]\n\n\n\n\nTo check the data type of a tensor and its properties in PyTorch, you can use the .dtype attribute. For example:\nx = torch.zeros(4, 8)\nx.dtype # check the data type of x\nx.numel() # check the number of elements in x\nx.element_size() # check the size of each element in bytes\nx.numel() * x.element_size() # check the total size in bytes\n\n\n1.2 Calculate Memory Usage of Model\nAssume we have a model with \\(N\\) parameters, and each parameter is represented by float32 (4 bytes). \\(A\\) is the number of activation elements stored during forward (depends on input and model depth). How can we calculate the memory need for training this model? Notice that the memory usage of a model is not only determined by the parameters, but also by:\n\nactivations\ngradients\noptimizer states\n\nFor a single parameter, the memory usage for one forward pass and backward pass is:\n\nparameter: 4 bytes (float32)\nactivation: 4 bytes (float32)\ngradient: 4 bytes (float32)\noptimizer state, which can vary depending on the optimizer used. For example, Adam optimizer requires 2 additional states (momentum and variance), each of which is 4 bytes (float32).\n\nSo, the total memory usage for one parameter is: \\[\n\\text{Memory per parameter} = 4 + 4  + 2 \\times 4 = 16 \\text{ bytes}\n\\]\nThus, the total memory usage for the model is: \\[\n\\text{Total Memory} = N \\times 16 \\text{ bytes} + A \\times 4 \\text{ bytes}\n\\]\n\n\n\n\n\n\nWhy need activation for backward pass?\n\n\n\nSay a layer denotes the input to the layer as \\(\\mathbf{x}\\), the weights as \\(\\theta\\), and the output as \\(\\mathbf{y}\\). The loss function is denoted as \\(L\\), which is a function of the output \\(\\mathbf{y}\\) and the target \\(t\\): \\[\n\\mathbf{y} = f(\\mathbf{x}; \\theta)\n\\] To compute the gradient of the loss with respect to the weights, we need to use the chain rule: \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\theta}\n\\]\nwhere \\(\\frac{\\partial \\mathbf{y}}{\\partial \\theta}\\) is the gradient of the output with respect to the weights \\(\\theta\\), which usually are the function of the input \\(\\mathbf{x}\\) and the weights \\(\\theta\\). To compute this gradient, we need to know the input \\(\\mathbf{x}\\). For example, the linear layer computes: \\[\n\\mathbf{y} = \\mathbf{x} \\cdot \\theta + b\n\\] to compute the gradient, we need to know the input \\(\\mathbf{x}\\), \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\mathbf{x}\n\\] where \\(\\mathbf{x}\\) is the input to the layer, which is the activation of the previous layer. Thus, we need to store the activation for the backward pass.\n\n\n\n\nCalculate Memory Usage of Model\nimport torch\nimport torch.nn as nn\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define a simple model\nmodel = nn.Sequential(\n    nn.Linear(100, 200),\n    nn.ReLU(),\n    nn.Linear(200, 50),\n    nn.ReLU(),\n    nn.Linear(50, 10)\n).to(device)\n\n# Register hooks to track activation sizes\nactivation_sizes = []\n\ndef hook_fn(module, input, output):\n    if isinstance(output, torch.Tensor):\n        activation_sizes.append(output.numel())\n    elif isinstance(output, (list, tuple)):\n        activation_sizes.extend(o.numel() for o in output if isinstance(o, torch.Tensor))\n\nhooks = []\nfor layer in model:\n    hooks.append(layer.register_forward_hook(hook_fn))\n\n# Create input\nx = torch.randn(32, 100, device=device, requires_grad=True)\n\n# Forward pass\noutput = model(x)\nloss = output.mean()\n\n# Backward pass\nloss.backward()\n\n# Remove hooks\nfor h in hooks:\n    h.remove()\n\n# --------- Memory Estimation ---------\n# 1. Parameters\nnum_params = sum(p.numel() for p in model.parameters())\nparam_memory = num_params * 4\ngrad_memory = num_params * 4\noptimizer_memory = num_params * 8\n\n# 2. Activations (sum of all layer outputs + input)\nactivation_memory = (x.numel() + sum(activation_sizes)) * 4  # float32\n\n# 3. Total\ntotal_bytes = param_memory + grad_memory + optimizer_memory + activation_memory\ntotal_MB = total_bytes / (1024 ** 2)\n\n# Print\nprint(f\"Total parameters: {num_params}\")\nprint(f\"Activation elements: {sum(activation_sizes)}\")\nprint(f\"Total training memory (float32): {total_MB:.2f} MB\")\n\n\n\n\n1.3 Collective operations\nCollective operations are operations that involve multiple processes or devices, such as GPUs, to perform a computation. They are essential for parallelizing the training process across multiple GPUs. Some common collective operations include:\n\nBroadcast(Figure 2 (a)): This operation sends data from one process to all other processes. It is commonly used to share model parameters or hyperparameters across multiple GPUs.\nScatter(Figure 2 (b)): This operation distributes data from one process to multiple processes. It is often used to distribute input data across multiple GPUs.\nGather(Figure 2 (c)): This operation collects data from multiple processes and combines it into a single process. It is useful for aggregating results from multiple GPUs.\nReduce(Figure 2 (d)): This operation combines data from multiple processes into a single process. It is commonly used to compute the sum or maximum of gradients across multiple GPUs.\nAll-gather(Figure 2 (e)): This operation collects data from all processes and distributes it back to all processes. It is often used to gather gradients or model parameters from multiple GPUs without losing any information.\nAll-reduce(Figure 2 (g)): This operation combines data from all processes and distributes the result back to all processes. It is often used to average gradients across multiple GPUs during training.\nReduce-scatter(Figure 2 (f)): This operation combines data from multiple processes and distributes the result to each process. It is often used to reduce the amount of data that needs to be communicated between processes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Broadcast\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter\n\n\n\n\n\n\n\n\n\n\n\n(c) Gather\n\n\n\n\n\n\n\n\n\n\n\n(d) Reduce\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) All-gather\n\n\n\n\n\n\n\n\n\n\n\n(f) Reduce-scatter\n\n\n\n\n\n\n\n\n\n\n\n(g) All-reduce\n\n\n\n\n\n\n\nFigure 2: The illustration of different collective operations. The figure shows how data is communicated between processes in each operation.(Image take from: Stanford CS336)\n\n\n\n\nOne should take note is Reduce-Scatter combines two operations:\n\nReduce: Each process (or GPU) contributes its data, and a reduction operation (usually sum, mean, max, etc.) is applied across processes.\nScatter: The reduced result is partitioned and each process gets only a portion (its shard) of the reduced result.\n\n\n\n\n\n\n\nTip\n\n\n\nWay to remember the terminology:\n\nGather: collects data from multiple sources into one destination(not do any operation)\nReduce: performs some associative/commutative operation (sum, min, max)\nBroadcast/Scatter: is inverse of Gather\nAll: means destination is all devices"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#profiling",
    "href": "posts/Blogs/speed-up-training.html#profiling",
    "title": "Speed Up Training for Neural Networks",
    "section": "2 Profiling",
    "text": "2 Profiling\nTo optimize the training process, we need to first profile our model to identify the bottlenecks. In this section, we will introduce several tools to profile the model and understand where the time is spent during training. We will discuss several tools that can help us profile our model and identify the bottlenecks in the training process:\n\nSimple Benchmarking (Section 2.1): The simplest way to measure the time taken for each operation in your model. You can use the time module in Python to measure the time taken for each operation. For example, you can wrap your forward pass in a timer to measure the time taken for each layer.\nPyTorch Profiler (Section 2.2): PyTorch provides a built-in profiler that can help you analyze the performance of your model. You can use the torch.profiler module to profile your model and visualize the results. The profiler provides detailed information about the time spent on each operation, memory usage, and more.\nNVIDIA Nsight Systems (Section 2.3): This is a powerful profiling tool that can help you analyze the performance of your model on NVIDIA GPUs. It provides detailed information about the GPU utilization, memory usage, and more. You can use it to identify bottlenecks in your model and optimize the performance.\n\n\n2.1 Simple Benchmarking\n\n\n2.2 PyTorch Profiler\n\n\n2.3 NVIDIA Nsight Systems"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#single-gpu-optimization",
    "href": "posts/Blogs/speed-up-training.html#single-gpu-optimization",
    "title": "Speed Up Training for Neural Networks",
    "section": "3 Single GPU Optimization",
    "text": "3 Single GPU Optimization\nIn this section, we will discuss various techniques to optimize the training process on a single GPU. These techniques include: - Fusion(Section 3.1): This technique combines multiple operations into a single operation to reduce the number of kernel launches and improve performance. For example, you can fuse the forward and backward passes of a layer into a single operation. - Tiling(Section 3.2): This technique divides the input data into smaller tiles and processes them in parallel to improve memory access patterns and reduce memory usage. For example, you can tile the input data into smaller chunks and process them in parallel. - Memory Coalescing(Section 3.3): This technique optimizes memory access patterns to improve memory bandwidth utilization. For example, you can coalesce memory accesses to reduce the number of memory transactions and improve performance. - Mixed Precision Training(Section 3.4): This technique uses lower precision data types (such as float16 or bfloat16) to reduce memory usage and improve performance. It can significantly speed up training while maintaining model accuracy. PyTorch provides built-in support for mixed precision training through the torch.cuda.amp module, which allows you to automatically cast your model and inputs to lower precision during training. - Gradient Accumulation(Section 3.5): This technique accumulates gradients over multiple mini-batches before performing a weight update. It can help reduce the number of weight updates and improve training stability, especially when using large batch sizes. You can implement gradient accumulation by accumulating gradients in a buffer and updating the model parameters only after a certain number of mini-batches.\n\n3.1 Fusion\n\n\n3.2 Tiling\n\n\n3.3 Memory Coalescing\n\n\n3.4 Mixed Precision Training\n\n\n3.5 Gradient Accumulation\n\n\n3.6 Case Study: Flash Attention"
  },
  {
    "objectID": "posts/Blogs/speed-up-training.html#multi-gpu-optimizationparallelism",
    "href": "posts/Blogs/speed-up-training.html#multi-gpu-optimizationparallelism",
    "title": "Speed Up Training for Neural Networks",
    "section": "4 Multi-GPU Optimization(Parallelism)",
    "text": "4 Multi-GPU Optimization(Parallelism)\nIn this section, we will discuss various techniques to optimize the training process across multiple GPUs. These techniques include: - Data Parallelism(Section 4.1): This technique splits the input data across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different subset of the input data, and the gradients are averaged across GPUs before updating the model parameters. - Model Parallelism(Section 4.2): This technique splits the model across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the model, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large models that do not fit into a single GPU’s memory. - Pipeline Parallelism(Section 4.3): This technique splits the model into multiple stages and processes each stage in parallel across multiple GPUs. Each GPU processes a different stage of the model, and the output of one stage is passed to the next stage. This can help improve throughput and reduce memory usage, especially for large models. - Tensor Parallelism(Section 4.4): This technique splits the tensors across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the tensor, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large tensors that do not fit into a single GPU’s memory. - Context Parallelism(Section 4.5): This technique splits the context across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the context, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large contexts that do not fit into a single GPU’s memory.\n\n4.1 Data Parallelism\n\n\n4.2 Model Parallelism\n\n\n4.3 Pipeline Parallelism\n\n\n4.4 Tensor Parallelism\n\n\n4.5 Context Parallelism\n\n\n4.6 Case Study: DeepSpeed\n\n\n4.7 Case Study: Megatron-LM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nSelf CPU %\nSelf CPU\nCPU total %\nCPU total\nCPU time avg\nSelf CUDA\nSelf CUDA %\nCUDA total\nCUDA time avg\n# of Calls\n\n\n\n\naten::gelu\n6.76%\n669.785us\n13.48%\n1.336ms\n1.336ms\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\nvoid at::native::vectorized_elementwise_kernel&lt;…&gt;\n0.00%\n0.000us\n0.00%\n0.000us\n0.000us\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\ncudaLaunchKernel\n6.72%\n665.807us\n6.72%\n665.807us\n665.807us\n0.000us\n0.00%\n0.000us\n0.000us\n1\n\n\ncudaDeviceSynchronize\n86.52%\n8.574ms\n86.52%\n8.574ms\n4.287ms\n0.000us\n0.00%\n0.000us\n0.000us\n2\n\n\n\nSelf CPU time total: 9.909 ms\nSelf CUDA time total: 8.642 ms"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html",
    "title": "LLM Part1: Architecture",
    "section": "",
    "text": "1 The overview of transformer model\n  2 Position Encoding\n  \n  2.1 Learned Position Encoding\n  2.2 Absolute Position Encoding\n  2.3 Relative Position Encoding\n  2.4 RoPE (Rotary Position Embedding)\n  2.5 ALIBI\n  2.6 Extend to longer context\n  \n  2.6.1 Linear Position Interpolation\n  2.6.2 NTK-Aware Position Interpolation\n  2.6.3 YaRN\n  \n  \n  3 Normalization\n  \n  3.1 Layer Normalization vs. RMS Normalization\n  3.2 Pre-Layer Normalization vs. Post-Layer Normalization\n  3.3 QK Norm\n  \n  4 Attention Mechanism\n  \n  4.1 Multi Headed Attention\n  \n  4.1.1 Time Complexity of Scaled Dot-Product Attention\n  \n  4.2 Grouped Query Attention / Multi Query Attention\n  4.3 Sliding window attention\n  4.4 Sparse Attention\n  4.5 Multi Latent Attention\n  4.6 Flash Attention\n  \n  4.6.1 Flash Attention V1 vs. V2 vs. V3\n  \n  4.7 Native Sparse Attention\n  4.8 Attention Sink\n  \n  5 Activations\n  \n  5.1 Swish\n  5.2 Gated Linear Unit (GLU)\n  \n  6 Feed Forward Network & Mixture of Experts\n  \n  6.1 Multi Layer Perceptron (MLP)\n  6.2 Gated Linear Unit (GLU)\n  6.3 Mixture of Experts (MoE)\n  \n  7 Model Initialization\n  \n  7.1 Weight Initialization\n  7.2 Layer Initialization\n  \n  8 Case Study\n  \n  8.1 LLaMA\n  8.2 Qwen\n  8.3 DeepSeek\n  8.4 GPT-Oss\n  \n  9 Other Architectures\n  \n  9.1 Diffusion Language Models\n  9.2 State Space Model (SSM)\n  \n  10 Conclusion\nThis is the part 1 of the LLM series, architecture. In this blog, we will going through different transformer architectures that power LLMs. Such as position encoding, attention mechanisms, and more. In the end, we will explore some SOTA LLM architectures, and study how to they combine those techniques to achieve better performance. We are going from bottom to up:\nBesides that, we will also explore different normalization techniques, such as Layer Normalization and RMS Normalization, and the different position of the normalization layers within the architecture. Then, we will going through the training process and how to effectively train these architectures."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#learned-position-encoding",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#learned-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.1 Learned Position Encoding",
    "text": "2.1 Learned Position Encoding\nIn the absolute position encoding, we put our position information into fixed sinusoidal functions. It is hand-crafted for specific tasks and does not adapt to the data. So, is it possible to learn position encodings from the data itself? It is possible, and this leads us to the concept of learned position encodings. The learned position encodings are typically implemented as additional trainable parameters in the model. Instead of using fixed sinusoidal functions, the model learns to generate position embeddings that are optimized for the specific task and dataset.\n\nimport torch\nimport torch.nn as nn\n\nclass LearnedPositionalEncoding(nn.Module):\n    def __init__(self, max_len, d_model):\n        super().__init__()\n\n        self.position_embeddings = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        seq_length = x.size(1)\n        # (1, seq_length)\n        positions = torch.arange(0, seq_length, device=x.device).unsqueeze(0)\n        return x + self.position_embeddings(positions)\n\nThis method is used in such as Vision Transformers (Dosovitskiy et al. 2021)."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#absolute-position-encoding",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#absolute-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.2 Absolute Position Encoding",
    "text": "2.2 Absolute Position Encoding\nAs used in the (Vaswani et al. 2023), absolute position encoding assigns a unique position embedding to each position in the input sequence, regardless of the content of the tokens. One common approach is to use a fixed sinusoidal function to generate the position embeddings. For example: for each position \\(pos\\), the position embedding \\(PE(pos)\\) can be defined as:\n\\[\n\\begin{aligned}\n\\text{PE}(pos, 2i) &= \\sin\\!\\left(pos \\times \\frac{1}{10,000^{2i/d_{\\text{model}}}}\\right) \\\\\n\\text{PE}(pos, 2i+1) &= \\cos\\!\\left(pos \\times \\frac{1}{10,000^{2i/d_{\\text{model}}}}\\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere\n\n\\(d_{\\text{model}}\\) is the dimensionality of the embeddings.\n\\(i\\) is the index of the embedding dimension. The sin function is applied to the even indices \\(2i\\), while the cos function is applied to the odd indices \\(2i+1\\).\n\nWe can illustrate the encoding as following:\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Display of the position with different sequence lengths\n\n\n\n\n\n\n\n\n\n\n\n(b) Display of the position with different d_model\n\n\n\n\n\n\n\nFigure 2: Illustration of Absolute Position Encoding\n\n\n\n\nThere are several properties can be read from the Figure 2 and Equation 1:\n\nPeriodicity: The sine and cosine functions used in the position encoding have a periodic nature, which allows the model to easily learn to attend to relative positions. This is evident in Figure 2 (a), where the position encodings exhibit similar patterns for different sequence lengths.\nDimensionality: The choice of \\(d_{\\text{model}}\\) affects the granularity of the position encodings. As shown in Figure 2 (b), increasing the dimensionality results in more fine-grained position encodings, which can help the model capture more subtle positional information.\nThe low-dimension part of the dmodel is change more frequently than the high-dimension part, allowing the model to adapt more easily to different input lengths.\n\n\nimport torch \nimport torch.nn as nn \n\nclass AbsolutePositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  \n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() \n            * \n            (-torch.log(torch.tensor(10000.0)) / d_model)\n        )\n\n        pe = torch.zeros(max_len, d_model)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        # (1, max_len, d_model): Expand the batch dimension\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        position_encoding = self.pe[:, :x.size(1), :]\n        x = x + position_encoding\n        return x\n\n\nWe chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset \\(k\\), \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\).  Attention Is All You Need \n\nHow to understand this sentence. Let’s first redefine the position encoding. \\[\n\\begin{aligned}\n\\mathrm{PE}(pos,2i) &= \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big), \\\\\n\\mathrm{PE}(pos,2i+1) &= \\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big).\n\\end{aligned}\n\\] where \\(\\alpha_i = 10,000^{2i/d_{\\text{model}}}\\). And we consider \\((2i, 2i+1)\\) as one pair. Now, consider the same pair at position \\(pos + k\\). We can write the position encoding as:\n\\[\n\\begin{align}\n\\mathrm{PE}(pos+k,2i)\n&= \\sin\\!\\Big(\\tfrac{pos+k}{\\alpha_i}\\Big)\n    = \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\cos\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n    + \\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\sin\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big) \\\\\n\\mathrm{PE}(pos+k,2i+1)\n&= \\cos\\!\\Big(\\tfrac{pos+k}{\\alpha_i}\\Big)\n    =\\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\cos\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n    - \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\sin\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n\\end{align}\n\\tag{2}\\]\n\n\nAngle addition formulas: \\[\n\\begin{align*}\n&\\sin(a+b) = \\sin(a)\\cos(b) + \\cos(a)\\sin(b) \\\\\n&\\cos(a+b) = \\cos(a)\\cos(b) - \\sin(a)\\sin(b)\n\\end{align*}\n\\]\nWrite this as vector form:\n\\[\n\\mathbf{p}_{pos}^{(i)} =\n\\begin{bmatrix}\n\\sin(pos/\\alpha_i) \\\\\n\\cos(pos/\\alpha_i)\n\\end{bmatrix}\n\\]\nThen \\(\\mathbf{p}_{pos+k}^{(i)}\\) equal to: \\[\n\\mathbf{p}_{pos+k}^{(i)} =\n\\underbrace{\n\\begin{bmatrix}\n\\cos(\\tfrac{k}{\\alpha_i}) & \\ \\ \\sin(\\tfrac{k}{\\alpha_i}) \\\\\n-\\sin(\\tfrac{k}{\\alpha_i}) & \\ \\ \\cos(\\tfrac{k}{\\alpha_i})\n\\end{bmatrix}\n}_{\\displaystyle R_i(k)}\n\\ \\mathbf{p}_{pos}^{(i)}\n\\]\nNotice that \\(R_i(k)\\) is known as rotation matrix which only depends on the relative position \\(k\\) and not on the absolute position \\(pos\\). This is the key insight that allows the model to generalize to different positions.\nStacking all pairs, \\[\n\\mathrm{PE}(pos+k) =\n\\underbrace{\n    \\begin{pmatrix}\n\\cos\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & \\sin\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & 0 & 0 & \\cdots & 0 & 0 \\\\\n-\\sin\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & \\cos\\!\\big(\\tfrac{k}{\\alpha_2}\\big) &\\sin\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\cdots & 0 & 0 \\\\\n0 & 0 &  -\\sin\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & \\cos\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) & \\sin\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) \\\\\n0 & 0 & 0 & 0 & \\cdots & -\\sin\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big)\n\\end{pmatrix}\n}_{R(k)}\n\\cdot\n\\underbrace{\n    \\begin{pmatrix}\n\\sin(\\tfrac{k}{\\alpha_1}) \\\\\n\\cos(\\tfrac{k}{\\alpha_1}) \\\\\n\\sin(\\tfrac{k}{\\alpha_2}) \\\\\n\\cos(\\tfrac{k}{\\alpha_2}) \\\\\n\\vdots \\\\\n\\sin(\\tfrac{k}{\\alpha_{d/2}}) \\\\\n\\cos(\\tfrac{k}{\\alpha_{d/2}})\n\\end{pmatrix}\n}_{\\mathrm{PE}(pos)}\n\\]\nwhere \\(R(k)\\) is block-diagonal with those \\(2\\times2\\) rotations, \\(R(k)\\) depends on \\(k\\) but not on \\(pos\\) → a linear map of \\(\\mathrm{PE}(pos)\\)."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#relative-position-encoding",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#relative-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.3 Relative Position Encoding",
    "text": "2.3 Relative Position Encoding\nRelative Position Encoding, first proposed in Transformer-XL (Dai et al. 2019), then adaptive in different language model.\n\\[\nA_{i,j} =\n\\underbrace{Q_i^\\top K_j}_{\\text{content-based addressing}}\n+\n\\underbrace{Q_i^\\top R_{i-j}}_{\\text{content-dependent positional bias}}\n+\n\\underbrace{u^\\top K_j}_{\\text{global content bias}}\n+\n\\underbrace{v^\\top R_{i-j}}_{\\text{global positional bias}}\n\\]\nwhere:\n\n\\(Q_i \\in \\mathbb{R}^d\\): query vector at position \\(i\\)\n\n\\(K_j \\in \\mathbb{R}^d\\): key vector at position \\(j\\)\n\n\\(R_{i-j} \\in \\mathbb{R}^d\\): embedding of the relative distance \\((i-j)\\)\n\n\\(u, v \\in \\mathbb{R}^d\\): learnable global bias vectors\n\n\\(A_{i,j}\\): unnormalized attention score between position \\(i\\) and \\(j\\)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass RelPositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        self.d_model = d_model\n\n        # relative positions: range [-max_len, max_len]\n        range_len = max_len * 2 + 1\n        self.rel_emb = nn.Embedding(range_len, d_model)\n\n        # trainable biases u, v (Transformer-XL)\n        self.u = nn.Parameter(torch.Tensor(d_model))\n        self.v = nn.Parameter(torch.Tensor(d_model))\n\n    def forward(self, q, k, seq_len):\n        B, H, L, Dh = q.size()\n\n        # (L, L): relative position indices\n        pos_idx = torch.arange(L, dtype=torch.long, device=q.device)\n        rel_idx = pos_idx[None, :] - pos_idx[:, None]  # i-j\n        rel_idx = rel_idx + seq_len  # shift to [0, 2*max_len]\n        rel_pos_emb = self.rel_emb(rel_idx)  # (L, L, d_model)\n\n        # compute QK^T (content-based)\n        content_score = torch.matmul(q, k.transpose(-2, -1))  # (B, H, L, L)\n\n        # project queries with R\n        rel_q = q + self.v.view(1, 1, 1, -1)  # add bias v\n        rel_score = torch.einsum('bhld,lrd-&gt;bhlr', rel_q, rel_pos_emb)\n\n        # add global content bias (u)\n        content_bias = torch.einsum('d,bhjd-&gt;bhj', self.u, k).unsqueeze(2)\n\n        # total score\n        logits = content_score + rel_score + content_bias\n        return logits / (Dh ** 0.5)  # scale as in attention"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#rope-rotary-position-embedding",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#rope-rotary-position-embedding",
    "title": "LLM Part1: Architecture",
    "section": "2.4 RoPE (Rotary Position Embedding)",
    "text": "2.4 RoPE (Rotary Position Embedding)\nSo far we have see the absolute position encoding and relative position encoding. However, there is an problem with absolute position encoding. For example, for two sentence:\n\nEvery Day I will go to gym\nI will go to gym every day\n\nThe absolute position encoding is totally different from two sentences, even though they have the same words and means. On the other hand, the problem of the relative position encoding is that it does not capture the absolute position information, which is crucial for understanding the meaning of the sentences for some task such as text summarization.\nRoPE (Su et al. 2023) is a method combine those two. The vector rotated certain degree according to the absolute position in the sentence. On the other hand, it relative position information is preserved. According to Equation 2, the relative position is not related to the position.\n\n\n\n\n\n\nFigure 3: Illustration of RoPE\n\n\n\n\\[\nR_{\\Theta,m}^{d} \\mathbf{x}\n=\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\n\\vdots\\\\\nx_{d-1}\\\\\nx_d\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{2})\\\\\n\\cos(m\\theta_{2})\\\\\n\\vdots\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n- x_2\\\\\nx_1\\\\\n- x_4\\\\\nx_3\\\\\n\\vdots\\\\\n- x_d\\\\\nx_{d-1}\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{2})\\\\\n\\sin(m\\theta_{2})\\\\\n\\vdots\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n\\tag{3}\\]\nwhere\n\\[\n\\theta_{i,d} = \\frac{1}{10,000^{2(i - 1) / d}} ,\\quad i \\in [1, 2, \\dots, d / 2 ]\n\\]\nAs we can see, to implement the RoPE in code, we can:\n\nConstruct cos and sin matrices for the given input dimensions and maximum position.\nApply the rotation to the input embeddings using the constructed matrices.\n\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, head_dim, max_position_embedding, base):\n        super().__init__()\n\n        self.head_dim = head_dim\n        self.max_position_embedding = max_position_embedding\n        self.base = base \n\n        # (head_dim // 2)\n        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    def forward(self, x, position_ids, seq_len = None ):\n\n        # (head_dim // 2) =&gt; (1, head_dim // 2, 1) =&gt; (B, head_dim // 2, 1)\n        inv_freq_expanded = self.inv_freq[None, : , None].expand(x.size(0), -1, 1)\n\n        # (B, L)  =&gt; (B, 1, L)\n        position_ids_expanded = position_ids[:, None, :]\n\n        # Outer product \n        freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n        emb = torch.cat((freqs, freqs), dim=-1)  # (B, L, d_model)\n\n        cos = emb.cos()\n        sin = emb.sin()\n    \n        return cos, sin \n\ndef rotate_half(x):\n    \"\"\"\n    Rotate the last dimension of x by half.\n    \"\"\"\n    x1 = x[..., : x.shape[-1] // 2] \n    x2 = x[..., x.shape[-1] // 2 :] \n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n    # (B, L, head_dim) =&gt; (B, 1, L, head_dim)\n    cos = cos.unsqueeze(unsqueeze_dim) # Add the head dimension\n    sin = sin.unsqueeze(unsqueeze_dim) # Add the head dimension\n\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\n\n\n\n\n\nNote\n\n\n\nOne thing always bother me about this implementation is that the rotate_half function actually swap pair of the last dimension as mentioned in the paper. For example:\n&gt;&gt;&gt; x = torch.arange(0, 24).reshape(3, 8) # (B, D)\n&gt;&gt;&gt; x\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23]])\n&gt;&gt;&gt; x1 = rotate_half(x)\n&gt;&gt;&gt; x1\ntensor([[ -4,  -5,  -6,  -7,   0,   1,   2,   3],\n        [-12, -13, -14, -15,   8,   9,  10,  11],\n        [-20, -21, -22, -23,  16,  17,  18,  19]])\nThe above function just change the x to [-x_{d//2}, ..., -x_{d}, x_0, ..., x_{d//2-1}]\nCheck this linkif you are interested.\nIf this really bother you, you can just implement like this:\ndef rotate_half_v2(x):\n    # Assume x is (B, D)\n    x = x.reshape(x.shape[0], -1, 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return x.reshape(x.shape[0], -1)\nwhich is same as they mentioned in the paper.\n&gt;&gt;&gt; x\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23]])\n&gt;&gt;&gt; x2 = rotate_half_v2(x)\n&gt;&gt;&gt; x2\ntensor([[ -1,   0,  -3,   2,  -5,   4,  -7,   6],\n        [ -9,   8, -11,  10, -13,  12, -15,  14],\n        [-17,  16, -19,  18, -21,  20, -23,  22]])\nOr\ndef rotate_half_v3(x: torch.Tensor) -&gt; torch.Tensor:\n    y = torch.empty_like(x)\n    y[..., ::2] = -x[..., 1::2]  # even positions get -odd\n    y[..., 1::2] =  x[..., ::2]  # odd positions get even\n    return y\n\n&gt;&gt;&gt; x3 = rotate_half_v3(x)\n&gt;&gt;&gt; x3\ntensor([[ -1,   0,  -3,   2,  -5,   4,  -7,   6],\n        [ -9,   8, -11,  10, -13,  12, -15,  14],\n        [-17,  16, -19,  18, -21,  20, -23,  22]])\n\n\nThe other implementation of the RoPE is reply on the complex number. We can treat 2D vector \\((x, y)\\) as a complex number \\(z = x + iy\\), and the rotation can be done by multiplying with a complex exponential:\n\\[\nz' = z \\cdot e^{i\\theta} = (x + iy) \\cdot (\\cos \\theta + i \\sin \\theta) = (x \\cos \\theta - y \\sin \\theta) + i (x \\sin \\theta + y \\cos \\theta)\n\\]\nCode adapted from LLaMA model\n\nimport torch\nfrom typing import Tuple\n\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -&gt; torch.Tensor:\n    \"\"\"\n    Returns freqs_cis with shape (end, dim//2), dtype complex64.\n    \"\"\"\n    assert dim % 2 == 0, \"head_dim (dim) must be even\"\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n    t = torch.arange(end, device=freqs.device, dtype=torch.float32)          # (end,)\n    angles = torch.outer(t, freqs)                                           # (end, dim//2)\n    freqs_cis = torch.polar(torch.ones_like(angles), angles)                 # complex64\n    return freqs_cis\n\ndef apply_rotary_emb(\n    xq: torch.Tensor,   # (B, T, H, D)\n    xk: torch.Tensor,   # (B, T, H, D)\n    freqs_cis: torch.Tensor,  # (T, D//2) complex\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Complex RoPE multiply with inline broadcasting reshape (no helper).\n    \"\"\"\n    B, T, H, D = xq.shape\n    assert xk.shape == (B, T, H, D)\n    assert D % 2 == 0\n    assert freqs_cis.shape == (T, D // 2), f\"expected {(T, D//2)}, got {freqs_cis.shape}\"\n\n    # view as complex: (..., D/2)\n    xq_c = torch.view_as_complex(xq.float().reshape(B, T, H, D // 2, 2))\n    xk_c = torch.view_as_complex(xk.float().reshape(B, T, H, D // 2, 2))\n\n    # INLINE reshape for broadcasting: (1, T, 1, D/2)\n    freqs_cis_b = freqs_cis.view(1, T, 1, D // 2)\n\n    # rotate and return to real, original shape & dtype\n    xq_out = torch.view_as_real(xq_c * freqs_cis_b).reshape_as(xq).type_as(xq)\n    xk_out = torch.view_as_real(xk_c * freqs_cis_b).reshape_as(xk).type_as(xk)\n    return xq_out, xk_out"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#alibi",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#alibi",
    "title": "LLM Part1: Architecture",
    "section": "2.5 ALIBI",
    "text": "2.5 ALIBI\n(Press, Smith, and Lewis 2022)\n\n\n\n\n\n\nFigure 4\n\n\n\nAliBi: simple, monotonic bias → strong extrapolation, lower overhead.\nSo far, for the position embedding, we modify the Q, K to add the position information for attention to calculate. However, is it possible to directly modify the attention score? To notify them the position information? This is exactly what ALIBI does. The ALIBI (Attention with Linear Biases) method introduces a linear bias to the attention scores based on the distance between tokens. The bias is added directly to the attention score before applying the softmax function. Mathematically:\n\\[\n\\operatorname{softmax}\\!\\Big( q_i k_j^\\top \\;+\\; m \\cdot (-(i-j)) \\Big)\n\\]\nwhere:\n\n\\(q_i \\in \\mathbb{R}^d\\): query vector at position \\(i\\)\n\\(k_j \\in \\mathbb{R}^d\\): key vector at position \\(j\\)\n\\(m \\in \\mathbb{R}\\): slop (head-dependent constant)\n\\((i - j) \\geq 0\\): relative distance\n\nFor example, for query \\(i\\), the logits against all keys \\([0, 1, \\dots, i ]\\) become: \\[\n\\ell_i \\;=\\;\n\\Big[\\, q_i k_0^\\top - m(i-0),\\;\n       q_i k_1^\\top - m(i-1),\\;\n       \\ldots,\\;\n       q_i k_{i-1}^\\top - m(1),\\;\n       q_i k_i^\\top \\,\\Big]\n\\]\n\ndef get_relative_positions(seq_len: int) -&gt; torch.tensor:\n    x = torch.arange(seq_len)[None, :]\n    y = torch.arange(seq_len)[:, None]\n    return x - y\n\n\ndef get_alibi_slope(num_heads):\n    x = (2 ** 8) ** (1 / num_heads)\n    return (\n        torch.tensor([1 / x ** (i + 1) for i in range(num_heads)])\n        .unsqueeze(-1)\n        .unsqueeze(-1)\n    )\n\nclass AliBiMultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, d_model):\n        super().__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.head_dim = d_model // num_heads\n        self.alibi_slope = get_alibi_slope(num_heads)\n\n        self.kqv = nn.Linear(self.d_model, 3 * self.d_model)\n    \n    def forward(self, x):\n        batch_size, seq_len, _ = x.shape\n\n        key, query, value = self.kqv(x).chunk(3, dim=-1)\n        query = query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n        key   = key.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n        value = value.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n\n        bias = (self.alibi_slope * get_relative_positions(seq_len)).unsqueeze(0)\n        score = query @ key.transpose(-2, -1) / (self.head_dim ** 0.5) + bias\n\n        mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)\n        mask = mask[None, None, :, :]  # (1, 1, seq_len, seq_len)\n        score = score.masked_fill(\n            mask[:, :, :seq_len, :seq_len] == 0, float(\"-inf\")\n        )\n\n        attn = F.softmax(score, dim=-1)\n        out = torch.matmul(attn, value)\n        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n        out = self.dropout(out)\n        return out"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#extend-to-longer-context",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#extend-to-longer-context",
    "title": "LLM Part1: Architecture",
    "section": "2.6 Extend to longer context",
    "text": "2.6 Extend to longer context\nSo far, for all the position encoding we discussed, it has one main drawback: it fixed maximum context length during training.\n\n\n\n\n\n\nFigure 5: (Image Source Video: Long-Context LLM Extension)\n\n\n\nWhen we are training on the fixed context length, the model learns to attend to the positions within that context. However, during inference, we may want to extend the context length beyond what the model was trained on. This is where the challenge lies. One way is to train a longer context length model from the beginning. However this requires more computational resources and may not be feasible for all applications. So, we need to find a way to adapt the model to longer contexts without retraining it from scratch.\nThere are several approaches to address this issue. Let’s discuss a few of them.\n\n2.6.1 Linear Position Interpolation\n\n\n\n\n\n\nFigure 6\n\n\n\ndecreases the frequencies of the basis functions so that more tokens fit within each period. The position interpolation (Chen et al. 2023) mentioned that we can just linearly interpolate the position embeddings for the extended context. This allows the model to generate position embeddings for longer sequences without requiring additional training. It just rescale the \\(m\\) base in the RoPE by: \\[\nf'(\\mathbf{x}, m) = f(\\mathbf{x}, m\\frac{L}{L'})\n\\] where \\(L\\) is the original context length and \\(L'\\) is the new context length.\n\n\n2.6.2 NTK-Aware Position Interpolation\nThe Linear Position Interpolation, if it was possible to pick the correct scale parameter dynamically based on the sequence length rather than having to settle for the fixed tradeoff of maximum sequence length vs. performance on shorter sequences. The NTK-Aware Position Interpolation method leverages the Neural Tangent Kernel (NTK) framework to adaptively adjust the position embeddings during inference. By analyzing the model’s behavior in the NTK regime, we can identify the optimal scaling factors for different sequence lengths, allowing for more effective extrapolation of position information.\n\\[\n\\alpha^{\\text{NTK-RoPE}}_{j} = \\kappa^{-\\frac{2j}{d_k}}\n\\]\n\n\n\n\n\n\nNeural Tangent Kernel (NTK)\n\n\n\n\n\n\n\n\n2.6.3 YaRN\nanother RoPE extension method, uses “NTK-by-parts” interpolation strategies across different dimensions of the embedding space and introduces a temperature factor to adjust the attention distribution for long inputs. But RoPE cannot extrapolate well to sequences longer than training (e.g., a model trained on 2K tokens struggles at 8K).\n\\[\n\\alpha^{\\mathrm{YaRN}}_{j}\n= \\frac{(1-\\gamma_j)\\,\\tfrac{1}{t} + \\gamma_j}{\\sqrt{T}} \\,.\n\\]\nYaRN modifies RoPE to support longer context windows while preserving model stability. (Peng et al. 2023)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#layer-normalization-vs.-rms-normalization",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#layer-normalization-vs.-rms-normalization",
    "title": "LLM Part1: Architecture",
    "section": "3.1 Layer Normalization vs. RMS Normalization",
    "text": "3.1 Layer Normalization vs. RMS Normalization\nThe Layer Normalization (Ba, Kiros, and Hinton 2016) is a technique to normalize the inputs across the features for each training example. It is defined as:\n\\[\n\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\n\\tag{4}\\] where:\n\n\\(\\mu(x)\\): the mean of the input features.\n\\(\\sigma(x)\\): the standard deviation of the input features.\n\\(\\gamma\\): a learnable scale parameter.\n\\(\\beta\\): a learnable shift parameter.\n\nThere are two learnable parameters in Layer Normalization: \\(\\gamma\\) and \\(\\beta\\), which have the same shape as the input features \\(d_{\\text{model}}\\).\nHowever, in the Root Mean Square(RMS) Normalization, proposed in (Zhang and Sennrich 2019), that we remove the mean from the normalization process. The RMS Normalization is defined as:\n\\[\n\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma\n\\tag{5}\\] where:\n\n\\(\\epsilon\\): a small constant to prevent division by zero.\n\\(\\gamma\\): a learnable scale parameter, which has the same shape as the input features \\(d_{\\text{model}}\\).\n\nAs we can see, the main difference between Layer Normalization and RMS Normalization is the removal of the mean from the normalization process, and remove the learnable shift parameter \\(\\beta\\). There are several advantage of that:\n\nSimplicity: RMS Normalization is simpler and requires fewer parameters, making it easier to implement and faster to compute. For model with \\(d_{\\text{model}} = 512\\), 8 layers, each layer has 2 normalization. Than the reduction number of parameter is up to \\(512 \\times 8 \\times 2 = 8192\\) parameters.\nFewer operations: no mean subtraction, no bias addition.\nSaves memory bandwidth, which is often the bottleneck in GPUs (not FLOPs).\nWhile reduce the number of parameters, it also maintains similar performance."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#pre-layer-normalization-vs.-post-layer-normalization",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#pre-layer-normalization-vs.-post-layer-normalization",
    "title": "LLM Part1: Architecture",
    "section": "3.2 Pre-Layer Normalization vs. Post-Layer Normalization",
    "text": "3.2 Pre-Layer Normalization vs. Post-Layer Normalization\n\n\n\n\n\n\nFigure 7: The figure illustrates the three different position of the normalization layer in the transformer architecture.\n\n\n\nOne main good reason why Pre-Layer Normalization is perform bettern than the Post-Layer Normalization is that, according to (Xiong et al. 2020), the help the gradient flow back through the network without disrupting the residual connections. When using the Pre-Layer Normalization, it tends to converge faster and more stably. And the initlization methods is become less sensitive to the scale of the inputs.\nThere are third type of the normalization position by (Ding et al. 2021) called sandwiched normalization, which is a combination of pre-layer and post-layer normalization. Is is proposed to improve the training for the text-image pair.\n\n\n\n\n\n\nGradient Flow\n\n\n\nOne generalizable lesson is that we should keep residual connections “clean” identity paths."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#qk-norm",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#qk-norm",
    "title": "LLM Part1: Architecture",
    "section": "3.3 QK Norm",
    "text": "3.3 QK Norm\nThere is another normalization method called Query-Key Normalization (QK Norm), which is designed to improve the attention mechanism by normalizing the query and key vectors before computing the attention scores."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-headed-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-headed-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.1 Multi Headed Attention",
    "text": "4.1 Multi Headed Attention\nThe standard multi headed attention is defined as:\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n\\tag{6}\\]\nwhere each head is computed as:\n\\[\n\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\tag{7}\\] with \\(W_i^Q\\), \\(W_i^K\\), and \\(W_i^V\\) being learned projection matrices.\nAnd the attention function, usually is scaled dot-product attention, is defined as: \\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\tag{8}\\]\nwhere \\(d_k\\) is the dimension of the keys. The reason for the scaling factor \\(\\sqrt{d_k}\\) is to counteract the effect of large dot-product values by the large dimension of \\(K\\), which can push the softmax function into regions with very small gradients.\n\n4.1.1 Time Complexity of Scaled Dot-Product Attention\nThe time complexity of the scaled dot-product attention mechanism can be analyzed as follows:\n\nQuery-Key Dot Product: The computation of the dot product between the query and key matrices has a time complexity of \\(O(n^2 d_k)\\), where \\(n\\) is the sequence length and \\(d_k\\) is the dimension of the keys.\nSoftmax Computation: The softmax function is applied to the dot product results, which has a time complexity of \\(O(n^2)\\).\nValue Weighting: The final step involves multiplying the softmax output with the value matrix, which has a time complexity of \\(O(n^2 d_v)\\), where \\(d_v\\) is the dimension of the values.\n\nOverall, the time complexity of the scaled dot-product attention is dominated by the query-key dot product and can be expressed as:\n\\[\nO(n^2 (d_k + d_v))\n\\tag{9}\\]\nAs we can see, the time complexity is quadratic in the sequence length, which can be a bottleneck for long sequences / contexts. Let’s see how we can improve it."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#grouped-query-attention-multi-query-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#grouped-query-attention-multi-query-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.2 Grouped Query Attention / Multi Query Attention",
    "text": "4.2 Grouped Query Attention / Multi Query Attention\n\n\n\n\n\n\nFigure 8: Overview of Grouped Query Attention & Multi Query Attention (Image Source: (Ainslie et al. 2023))\n\n\n\nProposed by the (Ainslie et al. 2023), Grouped Query Attention (GQA) and Multi Query Attention (MQA) are designed to reduce the computational burden of the attention mechanism by grouping queries and sharing keys and values across multiple queries."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#sliding-window-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#sliding-window-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.3 Sliding window attention",
    "text": "4.3 Sliding window attention\n\n\n\n\n\n\nFigure 9\n\n\n\n(Beltagy, Peters, and Cohan 2020)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#sparse-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#sparse-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.4 Sparse Attention",
    "text": "4.4 Sparse Attention\n\n\n\n\n\n\nFigure 10: The illustration of Sparse Attention. (Image Source: Generating Long Sequences with Sparse Transformers)\n\n\n\n(Child et al. 2019)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-latent-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-latent-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.5 Multi Latent Attention",
    "text": "4.5 Multi Latent Attention\n\n\n\n\n\n\nFigure 11: Compare between MHA, Grouped Query Attention, Multi Query Attention and Multi Latent Attention.\n\n\n\nMulti Latent Attention (MLA), proposed in (DeepSeek-AI et al. 2024) is a proposed extension to the attention mechanism that aims to capture more complex relationships within the input data by introducing multiple latent spaces. Each latent space can learn different aspects of the data, allowing for a more nuanced understanding of the input.\n\n\n\n\n\n\nFigure 12: The detail of Multi Latent Attention (MLA)."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#flash-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#flash-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.6 Flash Attention",
    "text": "4.6 Flash Attention\nSo far, we see different attention mechanisms that aim to improve the efficiency and effectiveness of the standard attention mechanism. However, all aforementioned methods improve the attention mechanism by approximating the attention calculation. On the other hand, Flash Attention, proposed in (Dao 2023), takes a different approach by optimizing the attention computation itself.\n\n\n\n\n\n\nFigure 13: The illustration of Flash Attention\n\n\n\n\n4.6.1 Flash Attention V1 vs. V2 vs. V3"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#native-sparse-attention",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#native-sparse-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.7 Native Sparse Attention",
    "text": "4.7 Native Sparse Attention\n\n\n\n\n\n\nFigure 14: Illustration of Native Sparse Attention (Image Source: Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention)\n\n\n\nProposed in (Yuan et al. 2025), this is a novel approach to sparse attention that aligns with hardware capabilities and allows for efficient training of sparse attention mechanisms."
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#attention-sink",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#attention-sink",
    "title": "LLM Part1: Architecture",
    "section": "4.8 Attention Sink",
    "text": "4.8 Attention Sink"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#swish",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#swish",
    "title": "LLM Part1: Architecture",
    "section": "5.1 Swish",
    "text": "5.1 Swish"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#gated-linear-unit-glu",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#gated-linear-unit-glu",
    "title": "LLM Part1: Architecture",
    "section": "5.2 Gated Linear Unit (GLU)",
    "text": "5.2 Gated Linear Unit (GLU)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-layer-perceptron-mlp",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#multi-layer-perceptron-mlp",
    "title": "LLM Part1: Architecture",
    "section": "6.1 Multi Layer Perceptron (MLP)",
    "text": "6.1 Multi Layer Perceptron (MLP)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#gated-linear-unit-glu-1",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#gated-linear-unit-glu-1",
    "title": "LLM Part1: Architecture",
    "section": "6.2 Gated Linear Unit (GLU)",
    "text": "6.2 Gated Linear Unit (GLU)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#mixture-of-experts-moe",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#mixture-of-experts-moe",
    "title": "LLM Part1: Architecture",
    "section": "6.3 Mixture of Experts (MoE)",
    "text": "6.3 Mixture of Experts (MoE)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#weight-initialization",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#weight-initialization",
    "title": "LLM Part1: Architecture",
    "section": "7.1 Weight Initialization",
    "text": "7.1 Weight Initialization"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#layer-initialization",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#layer-initialization",
    "title": "LLM Part1: Architecture",
    "section": "7.2 Layer Initialization",
    "text": "7.2 Layer Initialization"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#llama",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#llama",
    "title": "LLM Part1: Architecture",
    "section": "8.1 LLaMA",
    "text": "8.1 LLaMA"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#qwen",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#qwen",
    "title": "LLM Part1: Architecture",
    "section": "8.2 Qwen",
    "text": "8.2 Qwen"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#deepseek",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#deepseek",
    "title": "LLM Part1: Architecture",
    "section": "8.3 DeepSeek",
    "text": "8.3 DeepSeek"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#gpt-oss",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#gpt-oss",
    "title": "LLM Part1: Architecture",
    "section": "8.4 GPT-Oss",
    "text": "8.4 GPT-Oss"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#diffusion-language-models",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#diffusion-language-models",
    "title": "LLM Part1: Architecture",
    "section": "9.1 Diffusion Language Models",
    "text": "9.1 Diffusion Language Models\n\n\n\n\n\n\nFigure 15: Illustration of Diffusion Language Model. (Video Source: Inception Lab)\n\n\n\nLLaDA (Nie et al. 2025) is a diffusion-based language model that leverages the principles of diffusion models to generate text. By modeling the text generation process as a diffusion process, LLaDA aims to improve the quality and diversity of generated text.\n\n\n\n\n\n\nFigure 16: Example of LLaDA generation process. Prompt: “Explain what artificial intelligence is.” (Image Source: LLaDA demo)\n\n\n\n\n\n\n\n\n\nFigure 17: The training process and sampling process of LLaDA. (Image Source: LLaDA)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Architecture/post.html#state-space-model-ssm",
    "href": "posts/Blogs/LLM/LLM-Architecture/post.html#state-space-model-ssm",
    "title": "LLM Part1: Architecture",
    "section": "9.2 State Space Model (SSM)",
    "text": "9.2 State Space Model (SSM)"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Alignment/post.html",
    "href": "posts/Blogs/LLM/LLM-Alignment/post.html",
    "title": "LLM Part3: Alignment",
    "section": "",
    "text": "On this page\n   \n  \n  1 Supervised Fine-Tuning (SFT)\n  2 Review of Reinforcement Learning\n  3 RLHF\n  \n\nIn the part 1 and part 2 of the LLM series, we covered the architecture and inference techniques for LLMs. In this part 3, we will focus on alignment techniques, which are crucial for ensuring that LLMs behave in ways that are consistent with human values and intentions. We will explore various methods for aligning LLMs, including reinforcement learning from human feedback (RLHF), and discuss their implications for the development and deployment of these models. We will first explore the simple Supervised Fine-Tuning (SFT) approach, which involves fine-tuning LLMs on curated datasets that reflect human values and preferences. Than we will explore different RLHF techniques, which involve training LLMs using feedback from human evaluators to improve their alignment with human intentions. We will explore algorithms from PPO, DPO to GRPO and\n\n1 Supervised Fine-Tuning (SFT)\n\n\n2 Review of Reinforcement Learning\nIn this part, we will first review the basics of reinforcement learning, including key concepts such as rewards, policy, loss function, actor-critic methods, and more. This will provide a solid foundation for understanding the RLHF techniques we will explore later.\n\n\n3 RLHF"
  },
  {
    "objectID": "posts/Blogs/LLM/LLM-Inference/post.html",
    "href": "posts/Blogs/LLM/LLM-Inference/post.html",
    "title": "LLM Part2: Inference",
    "section": "",
    "text": "In the part 1 of the LLM series, we covered the architecture of LLMs, including key components such as position encoding, attention mechanisms, and more. We also explored various normalization techniques and the training process for these models. In the part 2, we will explore different inference techniques, which is necessary for effectively utilizing LLMs in real-world applications. And we will also explore several practical examples and use cases to illustrate these techniques in action, such as: - vLLM"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/post.html",
    "href": "posts/Blogs/DiffusionModels/post.html",
    "title": "Diffusion Models",
    "section": "",
    "text": "1 Preliminaries\n  \n  1.1 Kullback-Leibler Divergence(KL Divergence)\n  1.2 Jensen’s Inequality\n  1.3 Variational Lower Bound (ELBO)\n  1.4 Langevin Dynamics\n  \n  2 Diffusion Models\n  3 Case Study\n  \n  3.1 Text-to-Image Generation\n  3.2 Video Generation\n  3.3 Diffusion Policy\n  3.4 Diffusion Language Models\n  \n  4 Learning Resource\nInterested in the generative models. For example Nano Banana, DALL-E 2, Stable Diffusion, Midjourney, and so on. I have been reading some papers and blogs about diffusion models. Here is a summary of what I learned.\nFirst, let’s prepare some math knowledge."
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/post.html#kullback-leibler-divergencekl-divergence",
    "href": "posts/Blogs/DiffusionModels/post.html#kullback-leibler-divergencekl-divergence",
    "title": "Diffusion Models",
    "section": "1.1 Kullback-Leibler Divergence(KL Divergence)",
    "text": "1.1 Kullback-Leibler Divergence(KL Divergence)\nThe Kullback-Leibler divergence, also known as relative entropy, is a measure of how one probability distribution diverges from a second expected probability distribution. For two probability distributions \\(P\\) and \\(Q\\) defined on the same probability space, the KL divergence from \\(P\\) to \\(Q\\) is defined as:\n\\[\nD_{KL}(P || Q) = \\mathbb{E}_{x \\sim P}\\left[\\log\\left(\\frac{P(x)}{Q(x)}\\right)\\right]\n\\tag{1}\\]\nthere are some properties of KL divergence:\n\nNon-negativity: \\(D_{KL}(P || Q) \\geq 0\\), with equality if and only if \\(P = Q\\) almost everywhere.\nAsymmetry: \\(D_{KL}(P || Q) \\neq D_{KL}(Q || P)\\) in general."
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/post.html#jensens-inequality",
    "href": "posts/Blogs/DiffusionModels/post.html#jensens-inequality",
    "title": "Diffusion Models",
    "section": "1.2 Jensen’s Inequality",
    "text": "1.2 Jensen’s Inequality\nJensen’s Inequality is a fundamental result in convex analysis. It states that for a concave function \\(f\\) and a random variable \\(X\\),\n\\[\nf(\\mathbb{E}[X]) \\geq \\mathbb{E}[f(X)] .\n\\tag{2}\\]\nFor example, \\(f(x) = \\log x\\) is a concave function, so we have \\[\n\\log(\\mathbb{E}[X]) \\geq \\mathbb{E}[\\log(X)] .\n\\tag{3}\\]"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/post.html#variational-lower-bound-elbo",
    "href": "posts/Blogs/DiffusionModels/post.html#variational-lower-bound-elbo",
    "title": "Diffusion Models",
    "section": "1.3 Variational Lower Bound (ELBO)",
    "text": "1.3 Variational Lower Bound (ELBO)\nIn the context of probabilistic models, the Evidence Lower Bound (ELBO) is a lower bound on the log-likelihood of observed data. It is commonly used in variational inference to approximate complex posterior distributions. For example, in the variational autoencoder (VAE)(Kingma and Welling 2022) framework, the ELBO is given by:\n\\[\n\\text{ELBO}(x) = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) || p(z))\n\\tag{4}\\] where:\n\n\\(x\\) is the observed data,\n\\(z\\) is the latent variable,\n\\(p(x|z)\\) is the likelihood of the data given the latent variable,\n\\(p(z)\\) is the prior distribution over the latent variable,\n\\(q(z|x)\\) is the variational posterior distribution.\n\\(D_{KL}(q(z|x) || p(z))\\) is the KL divergence between the variational posterior and the prior.\n\nThe ELBO serves as a surrogate objective function that can be maximized to learn the parameters of the model and the variational distribution. Maximizing the ELBO is equivalent to minimizing the KL divergence between the variational posterior and the true posterior distribution, thereby improving the approximation of the posterior."
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/post.html#langevin-dynamics",
    "href": "posts/Blogs/DiffusionModels/post.html#langevin-dynamics",
    "title": "Diffusion Models",
    "section": "1.4 Langevin Dynamics",
    "text": "1.4 Langevin Dynamics\nLangevin dynamics is a sampling method that combines gradient information with stochastic noise to sample from a target distribution. It is particularly useful for sampling from complex, high-dimensional distributions where traditional methods may struggle. The Langevin dynamics update rule is given by: \\[\nx_{t+1} = x_t + \\frac{\\epsilon}{2} \\nabla \\log p(x_t) + \\sqrt{\\epsilon} \\eta_t\n\\tag{5}\\]"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/post.html#text-to-image-generation",
    "href": "posts/Blogs/DiffusionModels/post.html#text-to-image-generation",
    "title": "Diffusion Models",
    "section": "3.1 Text-to-Image Generation",
    "text": "3.1 Text-to-Image Generation"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/post.html#video-generation",
    "href": "posts/Blogs/DiffusionModels/post.html#video-generation",
    "title": "Diffusion Models",
    "section": "3.2 Video Generation",
    "text": "3.2 Video Generation\nMovie Gen"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/post.html#diffusion-policy",
    "href": "posts/Blogs/DiffusionModels/post.html#diffusion-policy",
    "title": "Diffusion Models",
    "section": "3.3 Diffusion Policy",
    "text": "3.3 Diffusion Policy"
  },
  {
    "objectID": "posts/Blogs/DiffusionModels/post.html#diffusion-language-models",
    "href": "posts/Blogs/DiffusionModels/post.html#diffusion-language-models",
    "title": "Diffusion Models",
    "section": "3.4 Diffusion Language Models",
    "text": "3.4 Diffusion Language Models"
  },
  {
    "objectID": "posts/notes.html",
    "href": "posts/notes.html",
    "title": "Learning Notes",
    "section": "",
    "text": "Note\n\n\n\n\nI have moved all my learning notes to this website. You can find them at Course Notes.\n\n\n\n\n\n\nAbout Page of the Course notes"
  },
  {
    "objectID": "posts/Projects/PaliGemma/pali-gemma.html",
    "href": "posts/Projects/PaliGemma/pali-gemma.html",
    "title": "PaliGemma Inference and Fine Tuning",
    "section": "",
    "text": "Here is the full process of the Pali-Gemma\n\n\n\n\n\n\n\nFigure 1: The Pali-Gemma model is a multi-modal large language model that integrates vision and language tasks. The full process includes data collection, model training, and fine-tuning for specific applications."
  },
  {
    "objectID": "posts/blogs.html",
    "href": "posts/blogs.html",
    "title": "👋🏻Welcome to Yuyang’s Blog",
    "section": "",
    "text": "Diffusion Models\n\n\n\nGenerative AI\n\nDiffusion Models\n\n\n\nThis blog introduces diffusion models, covering their core principles and perspectives. We begin with the basics of DDPM, then explore score-based, ODE, and SDE viewpoints, as well as flow and score matching. Finally, we highlight applications in image generation, text-to-image, video, and reinforcement learning.\n\n\n\n\n3 min\n\n540 words\n\n\n2025-09-11\n\n\n\n\n\n\n\nLLM Part1: Architecture\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn this blog, we will going through different transformer architectures that power LLMs. Such as position encoding, attention mechanisms, and more. In the end, we will explore some SOTA LLM architectures, and study how to they combine those techniques to achieve better performance.\n\n\n\n\n27 min\n\n5,390 words\n\n\n2025-09-03\n\n\n\n\n\n\n\nLLM Part3: Alignment\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn this blog, we will going through different alignment techniques for LLMs, including how to effectively align them with human values and intentions. We will explore techniques such as reinforcement learning from human feedback (RLHF), and more. By the end, you will have a solid understanding of how to align LLMs for your own applications.\n\n\n\n\n1 min\n\n178 words\n\n\n2025-08-25\n\n\n\n\n\n\n\nLLM Part2: Inference\n\n\n\nLarge Language Model\n\nInference\n\n\n\nIn this blog, we will going through the inference process of LLMs, including how to effectively use them for various tasks. We will explore techniques such as prompt engineering, few-shot learning, and more. By the end, you will have a solid understanding of how to leverage LLMs for your own applications.\n\n\n\n\n1 min\n\n79 words\n\n\n2025-08-25\n\n\n\n\n\n\n\nSpeed Up Training for Neural Networks\n\n\n\nTraining Tricks\n\n\n\nTraining large neural networks can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculating in the single GPUs, and parallelizing the training across multiple GPUs.\n\n\n\n\n\nYuyang Zhang\n\n14 min\n\n2,648 words\n\n\n2025-08-07\n\n\n\n\n\nNo matching items"
  }
]